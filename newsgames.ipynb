{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seabhra/ChatBot_NewsgamesIA/blob/main/newsgames.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run verificacao_simples.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "pGKLkvn76oHN",
        "outputId": "04d27828-66f2-4730-80aa-bf85c6d79827"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'resultados' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/content/verificacao_simples.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[0;31m# Fatores que afetam a pontuação\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m pontuacao_fatores = [\n\u001b[0;32m--> 622\u001b[0;31m     \u001b[0mresultados\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pontos_fonte\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m     \u001b[0mresultados\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pontos_consistencia\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m     \u001b[0mresultados\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pontos_evidencia\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'resultados' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile verificacao_simples.py\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import ssl\n",
        "import urllib3\n",
        "import json\n",
        "from urllib.parse import urlparse, urlencode\n",
        "import hashlib\n",
        "from collections import Counter\n",
        "import time\n",
        "import logging\n",
        "import base64\n",
        "import io\n",
        "import nltk\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"python-dotenv\"])\n",
        "    from dotenv import load_dotenv\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from textblob import TextBlob\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "from datetime import datetime\n",
        "\n",
        "# Configurar o logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(\"VerificadorNoticias\")\n",
        "\n",
        "# Desabilitar avisos SSL\n",
        "urllib3.disable_warnings()\n",
        "\n",
        "class VerificadorNoticias:\n",
        "    def __init__(self):\n",
        "        logger.info(\"Verificador de Notícias inicializado!\")\n",
        "\n",
        "        # Carregar variáveis do ambiente\n",
        "        load_dotenv()\n",
        "        self.fact_check_api_key = os.getenv(\"FACT_CHECK_API_KEY\")\n",
        "        self.fact_check_api_url = \"https://factchecktools.googleapis.com/v1alpha1/claims:search\"\n",
        "\n",
        "        # Carregar listas de fontes confiáveis e não confiáveis\n",
        "        self.fontes_confiaveis = [\n",
        "            'g1.globo.com', 'bbc.com', 'bbc.co.uk', 'reuters.com', 'apnews.com',\n",
        "            'nytimes.com', 'washingtonpost.com', 'estadao.com.br', 'folha.uol.com.br',\n",
        "            'cnn.com', 'valor.globo.com', 'info.abril.com.br', 'oglobo.globo.com',\n",
        "            'dw.com', 'npr.org', 'tecmundo.com.br', 'canaltech.com.br', 'olhardigital.com.br'\n",
        "        ]\n",
        "        self.fontes_suspeitas = [\n",
        "            'theonion.com', 'sensacionalista.com.br', 'breitbart.com', 'infowars.com',\n",
        "            'clickhole.com', 'nationalreport.net', 'worldnewsdailyreport.com',\n",
        "            'dailycurrant.com', 'empirenews.net'\n",
        "        ]\n",
        "\n",
        "        # Palavras para análise linguística\n",
        "        self.palavras_sensacionalistas = {\n",
        "            \"chocante\": 2, \"inacreditável\": 2, \"surpreendente\": 1, \"sensacional\": 2,\n",
        "            \"impressionante\": 1, \"alarmante\": 1.5, \"exclusivo\": 1, \"urgente\": 1.5,\n",
        "            \"shocking\": 2, \"unbelievable\": 2, \"amazing\": 1, \"surprising\": 1,\n",
        "            \"imperdível\": 1.5, \"incrível\": 1.5, \"absurdo\": 1.5, \"escandaloso\": 2,\n",
        "            \"bomba\": 1.5, \"inédito\": 1, \"revelado\": 1, \"segredo\": 1.5,\n",
        "            \"polêmico\": 1.5, \"controverso\": 1, \"escândalo\": 1.5, \"viral\": 1.5,\n",
        "            \"bizarro\": 1.5, \"aterrorizante\": 2, \"assustador\": 1.5, \"terrível\": 1.5,\n",
        "            \"estarrecedor\": 2, \"impossível\": 1.5\n",
        "        }\n",
        "\n",
        "        self.palavras_incerteza = [\n",
        "            \"talvez\", \"possivelmente\", \"provavelmente\", \"pode ser\", \"especula-se\",\n",
        "            \"acredita-se\", \"sugere\", \"alega\", \"supostamente\", \"rumores\",\n",
        "            \"maybe\", \"possibly\", \"probably\", \"might be\", \"it is speculated\",\n",
        "            \"it is believed\", \"suggests\", \"allegedly\", \"supposedly\", \"rumors\",\n",
        "            \"parece que\", \"há relatos\", \"não confirmado\", \"segundo fontes\",\n",
        "            \"indícios\", \"sinalizam\", \"aparentemente\", \"estaria\", \"poderia\",\n",
        "            \"teria\", \"estima-se\"\n",
        "        ]\n",
        "\n",
        "        self.expressoes_opinativas = [\n",
        "            \"eu acho\", \"na minha opinião\", \"parece que\", \"acredito que\",\n",
        "            \"i think\", \"in my opinion\", \"it seems that\", \"i believe\",\n",
        "            \"na visão de\", \"do meu ponto de vista\", \"considero\", \"entendo que\",\n",
        "            \"me parece\", \"creio que\", \"penso que\", \"sinto que\"\n",
        "        ]\n",
        "\n",
        "        # Tentar baixar recursos NLTK necessários para análise de texto\n",
        "        try:\n",
        "            nltk.download('punkt', quiet=True)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Não foi possível baixar recursos NLTK: {e}\")\n",
        "\n",
        "    def extrair_dominio(self, url):\n",
        "        \"\"\"Extrai o domínio base da URL.\"\"\"\n",
        "        parsed_url = urlparse(url)\n",
        "        return parsed_url.netloc\n",
        "\n",
        "    def verificar_url_simples(self, url):\n",
        "        \"\"\"Executa uma verificação simples em uma URL de notícia.\"\"\"\n",
        "        try:\n",
        "            # Extrair domínio\n",
        "            parsed_url = urlparse(url)\n",
        "            dominio = parsed_url.netloc\n",
        "\n",
        "            # Tentar acessar a URL\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers, timeout=10, verify=False)\n",
        "            response.raise_for_status()  # Levantar exceção para status HTTP de erro\n",
        "\n",
        "            # Verificar status HTTP\n",
        "            if response.status_code == 200:\n",
        "                logger.info(f\"URL acessada com sucesso: {url}\")\n",
        "                logger.info(f\"Domínio: {dominio}\")\n",
        "                logger.info(f\"Tamanho do conteúdo: {len(response.text)} caracteres.\")\n",
        "                return {\n",
        "                    \"status\": \"sucesso\",\n",
        "                    \"dominio\": dominio,\n",
        "                    \"tamanho_conteudo\": len(response.text),\n",
        "                    \"mensagem\": \"URL acessada com sucesso\"\n",
        "                }\n",
        "            else:\n",
        "                logger.warning(f\"Erro ao acessar a URL. Código HTTP: {response.status_code}\")\n",
        "                return {\n",
        "                    \"status\": \"erro\",\n",
        "                    \"mensagem\": f\"Erro ao acessar a URL. Código HTTP: {response.status_code}\"\n",
        "                }\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            logger.error(f\"Erro ao acessar a URL: {e}\")\n",
        "            return {\n",
        "                \"status\": \"erro\",\n",
        "                \"mensagem\": str(e)\n",
        "            }\n",
        "\n",
        "    def analisar_sentimento(self, texto):\n",
        "        \"\"\"Analisa o sentimento do texto usando TextBlob.\"\"\"\n",
        "        try:\n",
        "            sentiment = TextBlob(texto).sentiment\n",
        "            return {\n",
        "                \"polaridade\": sentiment.polarity,\n",
        "                \"subjetividade\": sentiment.subjectivity\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro na análise de sentimento: {e}\")\n",
        "            return {\n",
        "                \"polaridade\": 0,\n",
        "                \"subjetividade\": 0\n",
        "            }\n",
        "\n",
        "    def verificar_fonte(self, url):\n",
        "        \"\"\"Verifica o domínio da URL contra as listas de fontes.\"\"\"\n",
        "        dominio = self.extrair_dominio(url)\n",
        "        resultado = {\"dominio\": dominio}\n",
        "\n",
        "        # Verificar se o domínio está nas listas\n",
        "        if any(dominio.endswith(fonte) for fonte in self.fontes_confiaveis):\n",
        "            resultado[\"status_fonte\"] = \"confiável\"\n",
        "            resultado[\"pontos_fonte\"] = 20\n",
        "            return \"Fonte confiável\", 20, resultado\n",
        "        elif any(dominio.endswith(fonte) for fonte in self.fontes_suspeitas):\n",
        "            resultado[\"status_fonte\"] = \"suspeita\"\n",
        "            resultado[\"pontos_fonte\"] = -30\n",
        "            resultado[\"alerta_fonte\"] = \"Fonte conhecida por conteúdo satírico ou de baixa credibilidade\"\n",
        "            return \"Fonte suspeita\", -30, resultado\n",
        "        else:\n",
        "            resultado[\"status_fonte\"] = \"não categorizada\"\n",
        "            resultado[\"pontos_fonte\"] = 0\n",
        "            return \"Fonte não categorizada\", 0, resultado\n",
        "\n",
        "    def baixar_artigo(self, url):\n",
        "        \"\"\"Baixa o conteúdo do artigo a partir da URL.\"\"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers, verify=False, timeout=15)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Tenta extrair o título\n",
        "            titulo = soup.title.text.strip() if soup.title else \"Título não encontrado\"\n",
        "\n",
        "            # Tenta extrair o conteúdo do artigo (abordagem mais abrangente)\n",
        "            content = \"\"\n",
        "            # Procura por conteúdo em elementos comuns\n",
        "            for tag in ['article', 'main', '.content', '.post', '.entry', '#content', '.article-body', '.news-text']:\n",
        "                if tag.startswith('.') or tag.startswith('#'):\n",
        "                    elements = soup.select(tag)\n",
        "                else:\n",
        "                    elements = soup.find_all(tag)\n",
        "                if elements:\n",
        "                    for element in elements:\n",
        "                        content += element.get_text(separator=' ', strip=True) + \" \"\n",
        "                    break\n",
        "\n",
        "            # Se não encontrou conteúdo nos elementos específicos, pega todos os parágrafos\n",
        "            if not content:\n",
        "                paragraphs = soup.find_all('p')\n",
        "                content = ' '.join([p.get_text(strip=True) for p in paragraphs])\n",
        "\n",
        "            # Tenta encontrar a data\n",
        "            data = None\n",
        "            # Procura por metadados comuns\n",
        "            for meta in soup.find_all('meta'):\n",
        "                if meta.get('property') in ['article:published_time', 'og:published_time', 'publication_date'] or meta.get('name') in ['publication_date', 'date', 'pubdate']:\n",
        "                    data = meta.get('content')\n",
        "                    break\n",
        "\n",
        "            # Procura por elementos que possam conter autores\n",
        "            autores = []\n",
        "            for author_tag in ['author', 'byline', '.author', '.byline', '[rel=author]', '[name=author]']:\n",
        "                if author_tag.startswith('.') or author_tag.startswith('['):\n",
        "                    elements = soup.select(author_tag)\n",
        "                else:\n",
        "                    elements = soup.find_all(author_tag)\n",
        "\n",
        "                if elements:\n",
        "                    for element in elements:\n",
        "                        autor = element.get_text(strip=True)\n",
        "                        if autor and len(autor) < 100:  # Evitar pegar conteúdo grande por engano\n",
        "                            autores.append(autor)\n",
        "\n",
        "            # Remover duplicações nos autores\n",
        "            autores = list(dict.fromkeys(autores))\n",
        "\n",
        "            # Extrair imagens\n",
        "            imagens = []\n",
        "            for img in soup.find_all('img'):\n",
        "                src = img.get('src')\n",
        "                alt = img.get('alt', '')\n",
        "                if src and not src.startswith('data:'):\n",
        "                    # Garantir URL completa\n",
        "                    if src.startswith('/'):\n",
        "                        parsed_url = urlparse(url)\n",
        "                        base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
        "                        src = base_url + src\n",
        "                    imagens.append({\n",
        "                        'url': src,\n",
        "                        'alt': alt\n",
        "                    })\n",
        "\n",
        "            # Extrair palavras-chave\n",
        "            keywords = []\n",
        "            meta_keywords = soup.find('meta', attrs={'name': 'keywords'})\n",
        "            if meta_keywords:\n",
        "                keywords = meta_keywords.get('content', '').split(',')\n",
        "                keywords = [k.strip() for k in keywords]\n",
        "\n",
        "            return {\n",
        "                'titulo': titulo,\n",
        "                'conteudo': content,\n",
        "                'data': data,\n",
        "                'autores': autores,\n",
        "                'url': response.url,  # Use a URL final após redirecionamentos\n",
        "                'imagens': imagens,\n",
        "                'keywords': keywords,\n",
        "                'html': response.text  # Manter o HTML para possível análise adicional\n",
        "            }\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            logger.error(f\"Erro ao baixar artigo ({url}): {e}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro inesperado ao processar artigo ({url}): {e}\")\n",
        "            return None\n",
        "class VerificadorFactCheck:\n",
        "    def __init__(self):\n",
        "        \"\"\"Inicializa a classe com os atributos necessários.\"\"\"\n",
        "        self.fact_check_api_url = \"https://factchecktools.googleapis.com/v1alpha1/claims:search\"\n",
        "\n",
        "    def consultar_fact_check(self, query):\n",
        "        \"\"\"Consulta a API do Google Fact Check Tools.\"\"\"\n",
        "        # Restante do método\n",
        "        pass\n",
        "\n",
        "        # Carregar variáveis de ambiente\n",
        "        load_dotenv()\n",
        "        api_key = os.getenv(\"FACT_CHECK_API_KEY\")\n",
        "\n",
        "        if not api_key or api_key.strip() == \"\":\n",
        "            raise ValueError(\"Chave de API não encontrada no arquivo .env. Certifique-se de definir FACT_CHECK_API_KEY corretamente.\")\n",
        "\n",
        "        # Verificar se a URL base da API está configurada\n",
        "        if not self.fact_check_api_url.strip():\n",
        "            logger.warning(\"URL base para a API de Fact Check não configurada.\")\n",
        "            return None\n",
        "\n",
        "        # Construir a URL da API\n",
        "        url_api = f\"{self.fact_check_api_url}?key={api_key}&query={query}\"\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url_api)\n",
        "            if response.status_code == 200:\n",
        "                return response.json()\n",
        "            else:\n",
        "                logger.warning(f\"Falha ao acessar API de Fact Check. Status: {response.status_code}\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro na API de Fact Check: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "\n",
        "    def analisar_linguagem(self, texto):\n",
        "        \"\"\"Analisa o conteúdo em busca de sensacionalismo, incerteza e opinatividade.\"\"\"\n",
        "        if not texto or len(texto) < 10:\n",
        "            return {\n",
        "                \"sensacionalismo\": 0,\n",
        "                \"incerteza\": 0,\n",
        "                \"opinativo\": 0,\n",
        "                \"exclamacoes\": 0\n",
        "            }\n",
        "\n",
        "        resultado = {}\n",
        "\n",
        "        # Análise de sensacionalismo\n",
        "        indice_sensacionalismo = sum(\n",
        "            len(re.findall(r'\\b' + re.escape(palavra) + r'\\b', texto.lower())) * peso\n",
        "            for palavra, peso in self.palavras_sensacionalistas.items()\n",
        "        )\n",
        "        resultado[\"sensacionalismo\"] = indice_sensacionalismo\n",
        "\n",
        "        # Normalizar por tamanho do texto (por 1000 palavras)\n",
        "        palavras_totais = len(texto.split())\n",
        "        if palavras_totais > 0:\n",
        "            indice_sensacionalismo_norm = (indice_sensacionalismo * 1000) / palavras_totais\n",
        "            resultado[\"sensacionalismo_normalizado\"] = indice_sensacionalismo_norm\n",
        "\n",
        "            # Adicionar alerta se o índice for alto\n",
        "            if indice_sensacionalismo_norm > 10:\n",
        "                resultado[\"alerta_sensacionalismo\"] = \"Alto índice de linguagem sensacionalista\"\n",
        "            elif indice_sensacionalismo_norm > 5:\n",
        "                resultado[\"alerta_sensacionalismo\"] = \"Moderado índice de linguagem sensacionalista\"\n",
        "\n",
        "        # Análise de incerteza\n",
        "        indice_incerteza = sum(\n",
        "            len(re.findall(r'\\b' + re.escape(palavra) + r'\\b', texto.lower()))\n",
        "            for palavra in self.palavras_incerteza\n",
        "        )\n",
        "        resultado[\"incerteza\"] = indice_incerteza\n",
        "\n",
        "        if palavras_totais > 0:\n",
        "            indice_incerteza_norm = (indice_incerteza * 1000) / palavras_totais\n",
        "            resultado[\"incerteza_normalizada\"] = indice_incerteza_norm\n",
        "\n",
        "            if indice_incerteza_norm > 8:\n",
        "                resultado[\"alerta_incerteza\"] = \"Alto índice de expressões de incerteza\"\n",
        "\n",
        "        # Análise de opinatividade\n",
        "        indice_opiniao = sum(\n",
        "            len(re.findall(r'\\b' + re.escape(expressao) + r'\\b', texto.lower()))\n",
        "            for expressao in self.expressoes_opinativas\n",
        "        )\n",
        "        resultado[\"opinativo\"] = indice_opiniao\n",
        "\n",
        "        if palavras_totais > 0:\n",
        "            indice_opiniao_norm = (indice_opiniao * 1000) / palavras_totais\n",
        "            resultado[\"opinativo_normalizado\"] = indice_opiniao_norm\n",
        "\n",
        "            if indice_opiniao_norm > 5:\n",
        "                resultado[\"alerta_opiniao\"] = \"Alto índice de expressões opinativas\"\n",
        "\n",
        "        # Contagem de pontos de exclamação (indicador de sensacionalismo)\n",
        "        exclamacoes = texto.count('!')\n",
        "        resultado[\"exclamacoes\"] = exclamacoes\n",
        "\n",
        "        if palavras_totais > 0:\n",
        "            exclamacoes_norm = (exclamacoes * 1000) / palavras_totais\n",
        "            if exclamacoes_norm > 10:\n",
        "                resultado[\"alerta_exclamacoes\"] = \"Uso excessivo de pontos de exclamação\"\n",
        "\n",
        "        # Análise de sentimento usando TextBlob\n",
        "        try:\n",
        "            analise = TextBlob(texto)\n",
        "            resultado[\"sentimento_polaridade\"] = analise.sentiment.polarity  # -1 a 1 (negativo a positivo)\n",
        "            resultado[\"sentimento_subjetividade\"] = analise.sentiment.subjectivity  # 0 a 1 (objetivo a subjetivo)\n",
        "\n",
        "            # Categorização baseada na polaridade\n",
        "            if resultado[\"sentimento_polaridade\"] < -0.5:\n",
        "                resultado[\"sentimento_categoria\"] = \"muito negativo\"\n",
        "            elif resultado[\"sentimento_polaridade\"] < -0.1:\n",
        "                resultado[\"sentimento_categoria\"] = \"negativo\"\n",
        "            elif resultado[\"sentimento_polaridade\"] <= 0.1:\n",
        "                resultado[\"sentimento_categoria\"] = \"neutro\"\n",
        "            elif resultado[\"sentimento_polaridade\"] <= 0.5:\n",
        "                resultado[\"sentimento_categoria\"] = \"positivo\"\n",
        "            else:\n",
        "                resultado[\"sentimento_categoria\"] = \"muito positivo\"\n",
        "\n",
        "            # Alerta para alto nível de subjetividade em notícias que deveriam ser objetivas\n",
        "            if resultado[\"sentimento_subjetividade\"] > 0.6:\n",
        "                resultado[\"alerta_subjetividade\"] = \"Conteúdo altamente subjetivo\"\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Erro na análise de sentimento: {e}\")\n",
        "\n",
        "        return resultado\n",
        "\n",
        "    def verificar_imagens(self, imagens):\n",
        "        \"\"\"Analisa as imagens associadas ao artigo.\"\"\"\n",
        "        resultados = {}\n",
        "        if not imagens:\n",
        "            resultados[\"alerta_imagens\"] = \"Nenhuma imagem encontrada no artigo\"\n",
        "            return resultados\n",
        "\n",
        "        resultados[\"num_imagens\"] = len(imagens)\n",
        "\n",
        "        # Verificar descrições das imagens (alt text)\n",
        "        imagens_sem_descricao = sum(1 for img in imagens if not img.get('alt'))\n",
        "        resultados[\"imagens_sem_descricao\"] = imagens_sem_descricao\n",
        "\n",
        "        if imagens_sem_descricao == len(imagens) and len(imagens) > 0:\n",
        "            resultados[\"alerta_imagens_descricao\"] = \"Nenhuma das imagens possui descrição adequada\"\n",
        "\n",
        "        # Calcular hash básico das URLs de imagens para potencial comparação futura\n",
        "        hashes_urls = [hashlib.md5(img['url'].encode()).hexdigest()[:8] for img in imagens]\n",
        "        resultados[\"hashes_imagens\"] = hashes_urls\n",
        "\n",
        "        # Analisar metadados das primeiras 5 imagens (limitar para evitar muito processamento)\n",
        "        try:\n",
        "            with ThreadPoolExecutor(max_workers=3) as executor:\n",
        "                metadados = list(executor.map(self.extrair_metadados_imagem, [img['url'] for img in imagens[:5]]))\n",
        "                resultados[\"metadados_imagens\"] = [m for m in metadados if m]  # Filtrar None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro ao analisar metadados das imagens: {e}\")\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def extrair_metadados_imagem(self, url_imagem):\n",
        "        \"\"\"Extrai metadados de uma imagem a partir da URL.\"\"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            response = requests.get(url_imagem, headers=headers, stream=True, timeout=5, verify=False)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Abrir a imagem com PIL\n",
        "            img = Image.open(io.BytesIO(response.content))\n",
        "\n",
        "            # Coletar metadados básicos\n",
        "            metadados = {\n",
        "                \"formato\": img.format,\n",
        "                \"tamanho\": img.size,\n",
        "                \"modo\": img.mode,\n",
        "                \"url\": url_imagem,\n",
        "                \"hash\": hashlib.md5(response.content).hexdigest()\n",
        "            }\n",
        "\n",
        "            # Tentar extrair EXIF se disponível\n",
        "            if hasattr(img, '_getexif') and img._getexif():\n",
        "                exif = img._getexif()\n",
        "                if exif:\n",
        "                    exif_data = {}\n",
        "                    for tag_id, value in exif.items():\n",
        "                        # Converter para string para garantir serialização JSON\n",
        "                        exif_data[str(tag_id)] = str(value)\n",
        "                    metadados[\"exif\"] = exif_data\n",
        "\n",
        "            return metadados\n",
        "        except (requests.RequestException, UnidentifiedImageError, IOError) as e:\n",
        "            logger.warning(f\"Erro ao extrair metadados da imagem {url_imagem}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def buscar_fact_check(self, texto, titulo=None):\n",
        "        \"\"\"Busca por verificações de fatos relacionadas ao conteúdo.\"\"\"\n",
        "        resultados = {}\n",
        "\n",
        "        try:\n",
        "            # Preparar termos de busca\n",
        "            query = titulo if titulo else texto[:100]\n",
        "\n",
        "            # Consultar API de fact check\n",
        "            data = self.consultar_fact_check(query)\n",
        "\n",
        "            if data:\n",
        "                resultados[\"fact_checks\"] = data.get('claims', [])\n",
        "                resultados[\"num_fact_checks\"] = len(resultados[\"fact_checks\"])\n",
        "\n",
        "                # Analisar resultados\n",
        "                if resultados[\"num_fact_checks\"] > 0:\n",
        "                    # Verificar ratings médios\n",
        "                    ratings = []\n",
        "                    for claim in resultados[\"fact_checks\"]:\n",
        "                        for review in claim.get('claimReview', []):\n",
        "                            if 'textualRating' in review:\n",
        "                                ratings.append(review['textualRating'].lower())\n",
        "\n",
        "                    # Contar frequência de cada rating\n",
        "                    if ratings:\n",
        "                        counter = Counter(ratings)\n",
        "                        resultados[\"ratings_frequencia\"] = counter\n",
        "\n",
        "                        # Detectar termos comuns de falsidade\n",
        "                        termos_falso = ['false', 'fake', 'falso', 'mentira', 'enganoso', 'misleading']\n",
        "                        count_falso = sum(counter.get(termo, 0) for termo in termos_falso)\n",
        "\n",
        "                        if count_falso > 0:\n",
        "                            resultados[\"alerta_fact_check\"] = \"Conteúdo marcado como falso ou enganoso por verificadores de fatos\"\n",
        "                            resultados[\"pontos_fact_check\"] = -25\n",
        "                        else:\n",
        "                            resultados[\"pontos_fact_check\"] = 0\n",
        "            else:\n",
        "                resultados[\"fact_checks\"] = []\n",
        "                resultados[\"num_fact_checks\"] = 0\n",
        "                resultados[\"pontos_fact_check\"] = 0\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro ao buscar fact checks: {e}\")\n",
        "            resultados[\"erro_fact_check\"] = str(e)\n",
        "            resultados[\"pontos_fact_check\"] = 0\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def buscar_noticias_similares(self, titulo, keywords, conteudo=None):\n",
        "        \"\"\"Busca por notícias similares na web usando palavras-chave do artigo.\"\"\"\n",
        "        resultados = {}\n",
        "        try:\n",
        "            # Preparar termos de busca\n",
        "            termos_busca = titulo\n",
        "            if keywords:\n",
        "                termos_busca += \" \" + \" \".join(keywords[:3])  # Limitar a 3 keywords\n",
        "\n",
        "            # Simular resultados para demonstração\n",
        "            # Em uma implementação real, você usaria uma API como Google News ou similar\n",
        "            resultados[\"artigos_similares\"] = 5  # Simulando 5 artigos similares\n",
        "            resultados[\"similaridade\"] = \"média\"  # baixa, média, alta\n",
        "\n",
        "            # Simular consistência entre fontes\n",
        "            resultados[\"consistencia_entre_fontes\"] = \"alta\"  # baixa, média, alta\n",
        "            resultados[\"pontos_consistencia\"] = 10  # Simulando pontos por alta consistência\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro ao buscar notícias similares: {e}\")\n",
        "            resultados[\"erro_busca\"] = str(e)\n",
        "            resultados[\"pontos_consistencia\"] = 0\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def converter_nivel_para_texto(self, nivel):\n",
        "        \"\"\"Converte um nível numérico de veracidade para texto.\"\"\"\n",
        "        nivel_texto = {\n",
        "            1: \"CONFIRMADO\",\n",
        "            2: \"PROVAVELMENTE VERDADEIRO\",\n",
        "            3: \"INCONCLUSIVO\",\n",
        "            4: \"PROVAVELMENTE FALSO\",\n",
        "            5: \"FALSO\"\n",
        "        }.get(nivel, \"INCONCLUSIVO\")\n",
        "        return nivel_texto\n",
        "\n",
        "    def analisar_credibilidade(self, url):\n",
        "        \"\"\"Análise completa de credibilidade de uma URL de notícia.\"\"\"\n",
        "        # 1. Baixar o artigo\n",
        "        artigo = self.baixar_artigo(url)\n",
        "        if not artigo or not artigo.get('conteudo'):\n",
        "            logger.error(f\"Não foi possível baixar o artigo: {url}\")\n",
        "            return {\"erro\": \"Não foi possível baixar o artigo para análise\"}\n",
        "\n",
        "        resultados = {}\n",
        "        conteudo = artigo['conteudo']\n",
        "\n",
        "        # 2. Verificação de fonte/domínio\n",
        "        status_fonte, pontos_fonte, info_fonte = self.verificar_fonte(url)\n",
        "        resultados.update(info_fonte)\n",
        "\n",
        "        # 3. Copiar informações básicas\n",
        "        resultados[\"titulo\"] = artigo['titulo']\n",
        "        resultados[\"data_publicacao\"] = artigo['data']\n",
        "        resultados[\"autores\"] = artigo['autores']\n",
        "        resultados[\"url\"] = artigo['url']\n",
        "\n",
        "        # 4. Análise básica do conteúdo\n",
        "        palavras = len(conteudo.split())\n",
        "        resultados[\"num_palavras\"] = palavras\n",
        "        if palavras < 100:\n",
        "            resultados[\"alerta_tamanho\"] = \"Texto muito curto, pode não ser um artigo completo\"\n",
        "\n",
        "        # 5. Análise detalhada da linguagem\n",
        "        analise_linguagem = self.analisar_linguagem(conteudo)\n",
        "        resultados.update(analise_linguagem)\n",
        "\n",
        "        # 6. Verificação de citações\n",
        "        citacoes = re.findall(r'\"([^\"]+)\"', conteudo)\n",
        "        resultados[\"num_citacoes\"] = len(citacoes)\n",
        "        if len(citacoes) == 0:\n",
        "            resultados[\"alerta_fontes\"] = \"Nenhuma citação direta encontrada\"\n",
        "\n",
        "        # 7. Verificação de URLs externos (referências)\n",
        "        urls = re.findall(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', conteudo)\n",
        "        resultados[\"num_urls\"] = len(urls)\n",
        "\n",
        "        # 8. Análise de imagens\n",
        "        resultados_imagens = self.verificar_imagens(artigo.get('imagens', []))\n",
        "        resultados.update(resultados_imagens)\n",
        "\n",
        "        # 9. Verificação cruzada com outras notícias\n",
        "        resultados_similares = self.buscar_noticias_similares(\n",
        "            artigo['titulo'],\n",
        "            artigo.get('keywords', []),\n",
        "            conteudo\n",
        "        )\n",
        "        resultados.update(resultados_similares)\n",
        "\n",
        "        # 10. Verificação em serviços de fact-checking\n",
        "        resultados_fact_check = self.buscar_fact_check(conteudo, artigo['titulo'])\n",
        "        resultados.update(resultados_fact_check)\n",
        "\n",
        "        # Cálculo mais sofisticado usando vários fatores\n",
        "# 11. Determinar um nível de veracidade\n",
        "# Cálculo mais sofisticado usando vários fatores\n",
        "base_pontuacao = 70  # Base inicial neutra\n",
        "\n",
        "# Fatores que afetam a pontuação\n",
        "pontuacao_fatores = [\n",
        "    resultados.get(\"pontos_fonte\", 0),\n",
        "    resultados.get(\"pontos_consistencia\", 0),\n",
        "    resultados.get(\"pontos_evidencia\", 0),\n",
        "    resultados.get(\"pontos_conhecimento_previo\", 0),\n",
        "    resultados.get(\"pontos_contexto\", 0),\n",
        "    resultados.get(\"pontos_bias\", 0)\n",
        "]\n",
        "\n",
        "# Aplicar pesos aos fatores\n",
        "pesos = [1.5, 1.2, 2.0, 1.0, 1.0, 0.8]\n",
        "pontuacao_ponderada = sum(p * f for p, f in zip(pesos, pontuacao_fatores))\n",
        "\n",
        "# Calcular pontuação final\n",
        "pontuacao_final = max(0, min(100, base_pontuacao + pontuacao_ponderada))\n",
        "\n",
        "# Determinar nível de veracidade baseado na pontuação final\n",
        "if pontuacao_final >= 90:\n",
        "    nivel_veracidade = \"Altamente verificado\"\n",
        "elif pontuacao_final >= 75:\n",
        "    nivel_veracidade = \"Provavelmente verdadeiro\"\n",
        "elif pontuacao_final >= 60:\n",
        "    nivel_veracidade = \"Parcialmente verificado\"\n",
        "elif pontuacao_final >= 40:\n",
        "    nivel_veracidade = \"Inconclusivo\"\n",
        "elif pontuacao_final >= 25:\n",
        "    nivel_veracidade = \"Provavelmente falso\"\n",
        "else:\n",
        "    nivel_veracidade = \"Falso\"\n",
        "\n",
        "resultados[\"pontuacao_veracidade\"] = pontuacao_final\n",
        "resultados[\"nivel_veracidade\"] = nivel_veracidade\n",
        "\n",
        "# 12. Sumarizar os resultados\n",
        "resumo = {\n",
        "    \"nivel_veracidade\": nivel_veracidade,\n",
        "    \"pontuacao\": pontuacao_final,\n",
        "    \"analise_detalhada\": resultados,\n",
        "    \"pontos_chave\": {\n",
        "        \"fontes\": resultados.get(\"qualidade_fonte\", \"Não analisado\"),\n",
        "        \"consistencia\": resultados.get(\"consistencia_narrativa\", \"Não analisado\"),\n",
        "        \"evidencias\": resultados.get(\"qualidade_evidencia\", \"Não analisado\"),\n",
        "        \"contexto\": resultados.get(\"analise_contexto\", \"Não analisado\")\n",
        "    },\n",
        "    \"recomendacao\": \"\"\n",
        "}\n",
        "\n",
        "# Adicionar uma recomendação baseada no nível de veracidade\n",
        "def adicionar_recomendacao(pontuacao_final):\n",
        "    \"\"\"Adiciona uma recomendação ao resumo com base no nível de veracidade.\"\"\"\n",
        "    resumo = {}\n",
        "    if pontuacao_final >= 75:\n",
        "        resumo[\"recomendacao\"] = \"Informação confiável para uso e disseminação.\"\n",
        "    elif pontuacao_final >= 50:\n",
        "        resumo[\"recomendacao\"] = \"Verificar informações adicionais antes de aceitar completamente.\"\n",
        "    else:\n",
        "        resumo[\"recomendacao\"] = \"Não recomendado para uso ou compartilhamento sem verificação adicional.\"\n",
        "\n",
        "    return resumo\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-W8i0vkO6dIG",
        "outputId": "cfa3cbb7-231b-46dc-df71-bfc525ab0886"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting verificacao_simples.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run verificacao_simples.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "jRiRHLrd5vfn",
        "outputId": "328dae70-13fc-4c54-fd25-fcda5bad52cf"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unindent does not match any outer indentation level (verificacao_simples.py, line 274)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/content/verificacao_simples.py\"\u001b[0;36m, line \u001b[0;32m274\u001b[0m\n\u001b[0;31m    class VerificadorFactCheck:\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile verificacao_simples.py\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import ssl\n",
        "import urllib3\n",
        "import json\n",
        "from urllib.parse import urlparse, urlencode\n",
        "import hashlib\n",
        "from collections import Counter\n",
        "import time\n",
        "import logging\n",
        "import base64\n",
        "import io\n",
        "import nltk\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"python-dotenv\"])\n",
        "    from dotenv import load_dotenv\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from textblob import TextBlob\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "from datetime import datetime\n",
        "\n",
        "# Configurar o logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(\"VerificadorNoticias\")\n",
        "\n",
        "# Desabilitar avisos SSL\n",
        "urllib3.disable_warnings()\n",
        "\n",
        "class VerificadorNoticias:\n",
        "    def __init__(self):\n",
        "        logger.info(\"Verificador de Notícias inicializado!\")\n",
        "\n",
        "        # Carregar variáveis do ambiente\n",
        "        load_dotenv()\n",
        "        self.fact_check_api_key = os.getenv(\"FACT_CHECK_API_KEY\")\n",
        "        self.fact_check_api_url = \"https://factchecktools.googleapis.com/v1alpha1/claims:search\"\n",
        "\n",
        "        # Carregar listas de fontes confiáveis e não confiáveis\n",
        "        self.fontes_confiaveis = [\n",
        "            'g1.globo.com', 'bbc.com', 'bbc.co.uk', 'reuters.com', 'apnews.com',\n",
        "            'nytimes.com', 'washingtonpost.com', 'estadao.com.br', 'folha.uol.com.br',\n",
        "            'cnn.com', 'valor.globo.com', 'info.abril.com.br', 'oglobo.globo.com',\n",
        "            'dw.com', 'npr.org', 'tecmundo.com.br', 'canaltech.com.br', 'olhardigital.com.br'\n",
        "        ]\n",
        "        self.fontes_suspeitas = [\n",
        "            'theonion.com', 'sensacionalista.com.br', 'breitbart.com', 'infowars.com',\n",
        "            'clickhole.com', 'nationalreport.net', 'worldnewsdailyreport.com',\n",
        "            'dailycurrant.com', 'empirenews.net'\n",
        "        ]\n",
        "\n",
        "        # Palavras para análise linguística\n",
        "        self.palavras_sensacionalistas = {\n",
        "            \"chocante\": 2, \"inacreditável\": 2, \"surpreendente\": 1, \"sensacional\": 2,\n",
        "            \"impressionante\": 1, \"alarmante\": 1.5, \"exclusivo\": 1, \"urgente\": 1.5,\n",
        "            \"shocking\": 2, \"unbelievable\": 2, \"amazing\": 1, \"surprising\": 1,\n",
        "            \"imperdível\": 1.5, \"incrível\": 1.5, \"absurdo\": 1.5, \"escandaloso\": 2,\n",
        "            \"bomba\": 1.5, \"inédito\": 1, \"revelado\": 1, \"segredo\": 1.5,\n",
        "            \"polêmico\": 1.5, \"controverso\": 1, \"escândalo\": 1.5, \"viral\": 1.5,\n",
        "            \"bizarro\": 1.5, \"aterrorizante\": 2, \"assustador\": 1.5, \"terrível\": 1.5,\n",
        "            \"estarrecedor\": 2, \"impossível\": 1.5\n",
        "        }\n",
        "\n",
        "        self.palavras_incerteza = [\n",
        "            \"talvez\", \"possivelmente\", \"provavelmente\", \"pode ser\", \"especula-se\",\n",
        "            \"acredita-se\", \"sugere\", \"alega\", \"supostamente\", \"rumores\",\n",
        "            \"maybe\", \"possibly\", \"probably\", \"might be\", \"it is speculated\",\n",
        "            \"it is believed\", \"suggests\", \"allegedly\", \"supposedly\", \"rumors\",\n",
        "            \"parece que\", \"há relatos\", \"não confirmado\", \"segundo fontes\",\n",
        "            \"indícios\", \"sinalizam\", \"aparentemente\", \"estaria\", \"poderia\",\n",
        "            \"teria\", \"estima-se\"\n",
        "        ]\n",
        "\n",
        "        self.expressoes_opinativas = [\n",
        "            \"eu acho\", \"na minha opinião\", \"parece que\", \"acredito que\",\n",
        "            \"i think\", \"in my opinion\", \"it seems that\", \"i believe\",\n",
        "            \"na visão de\", \"do meu ponto de vista\", \"considero\", \"entendo que\",\n",
        "            \"me parece\", \"creio que\", \"penso que\", \"sinto que\"\n",
        "        ]\n",
        "\n",
        "        # Tentar baixar recursos NLTK necessários para análise de texto\n",
        "        try:\n",
        "            nltk.download('punkt', quiet=True)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Não foi possível baixar recursos NLTK: {e}\")\n",
        "\n",
        "    def extrair_dominio(self, url):\n",
        "        \"\"\"Extrai o domínio base da URL.\"\"\"\n",
        "        parsed_url = urlparse(url)\n",
        "        return parsed_url.netloc\n",
        "\n",
        "    def verificar_url_simples(self, url):\n",
        "        \"\"\"Executa uma verificação simples em uma URL de notícia.\"\"\"\n",
        "        try:\n",
        "            # Extrair domínio\n",
        "            parsed_url = urlparse(url)\n",
        "            dominio = parsed_url.netloc\n",
        "\n",
        "            # Tentar acessar a URL\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers, timeout=10, verify=False)\n",
        "            response.raise_for_status()  # Levantar exceção para status HTTP de erro\n",
        "\n",
        "            # Verificar status HTTP\n",
        "            if response.status_code == 200:\n",
        "                logger.info(f\"URL acessada com sucesso: {url}\")\n",
        "                logger.info(f\"Domínio: {dominio}\")\n",
        "                logger.info(f\"Tamanho do conteúdo: {len(response.text)} caracteres.\")\n",
        "                return {\n",
        "                    \"status\": \"sucesso\",\n",
        "                    \"dominio\": dominio,\n",
        "                    \"tamanho_conteudo\": len(response.text),\n",
        "                    \"mensagem\": \"URL acessada com sucesso\"\n",
        "                }\n",
        "            else:\n",
        "                logger.warning(f\"Erro ao acessar a URL. Código HTTP: {response.status_code}\")\n",
        "                return {\n",
        "                    \"status\": \"erro\",\n",
        "                    \"mensagem\": f\"Erro ao acessar a URL. Código HTTP: {response.status_code}\"\n",
        "                }\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            logger.error(f\"Erro ao acessar a URL: {e}\")\n",
        "            return {\n",
        "                \"status\": \"erro\",\n",
        "                \"mensagem\": str(e)\n",
        "            }\n",
        "\n",
        "    def analisar_sentimento(self, texto):\n",
        "        \"\"\"Analisa o sentimento do texto usando TextBlob.\"\"\"\n",
        "        try:\n",
        "            sentiment = TextBlob(texto).sentiment\n",
        "            return {\n",
        "                \"polaridade\": sentiment.polarity,\n",
        "                \"subjetividade\": sentiment.subjectivity\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro na análise de sentimento: {e}\")\n",
        "            return {\n",
        "                \"polaridade\": 0,\n",
        "                \"subjetividade\": 0\n",
        "            }\n",
        "\n",
        "    def verificar_fonte(self, url):\n",
        "        \"\"\"Verifica o domínio da URL contra as listas de fontes.\"\"\"\n",
        "        dominio = self.extrair_dominio(url)\n",
        "        resultado = {\"dominio\": dominio}\n",
        "\n",
        "        # Verificar se o domínio está nas listas\n",
        "        if any(dominio.endswith(fonte) for fonte in self.fontes_confiaveis):\n",
        "            resultado[\"status_fonte\"] = \"confiável\"\n",
        "            resultado[\"pontos_fonte\"] = 20\n",
        "            return \"Fonte confiável\", 20, resultado\n",
        "        elif any(dominio.endswith(fonte) for fonte in self.fontes_suspeitas):\n",
        "            resultado[\"status_fonte\"] = \"suspeita\"\n",
        "            resultado[\"pontos_fonte\"] = -30\n",
        "            resultado[\"alerta_fonte\"] = \"Fonte conhecida por conteúdo satírico ou de baixa credibilidade\"\n",
        "            return \"Fonte suspeita\", -30, resultado\n",
        "        else:\n",
        "            resultado[\"status_fonte\"] = \"não categorizada\"\n",
        "            resultado[\"pontos_fonte\"] = 0\n",
        "            return \"Fonte não categorizada\", 0, resultado\n",
        "\n",
        "    def baixar_artigo(self, url):\n",
        "        \"\"\"Baixa o conteúdo do artigo a partir da URL.\"\"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers, verify=False, timeout=15)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Tenta extrair o título\n",
        "            titulo = soup.title.text.strip() if soup.title else \"Título não encontrado\"\n",
        "\n",
        "            # Tenta extrair o conteúdo do artigo (abordagem mais abrangente)\n",
        "            content = \"\"\n",
        "            # Procura por conteúdo em elementos comuns\n",
        "            for tag in ['article', 'main', '.content', '.post', '.entry', '#content', '.article-body', '.news-text']:\n",
        "                if tag.startswith('.') or tag.startswith('#'):\n",
        "                    elements = soup.select(tag)\n",
        "                else:\n",
        "                    elements = soup.find_all(tag)\n",
        "                if elements:\n",
        "                    for element in elements:\n",
        "                        content += element.get_text(separator=' ', strip=True) + \" \"\n",
        "                    break\n",
        "\n",
        "            # Se não encontrou conteúdo nos elementos específicos, pega todos os parágrafos\n",
        "            if not content:\n",
        "                paragraphs = soup.find_all('p')\n",
        "                content = ' '.join([p.get_text(strip=True) for p in paragraphs])\n",
        "\n",
        "            # Tenta encontrar a data\n",
        "            data = None\n",
        "            # Procura por metadados comuns\n",
        "            for meta in soup.find_all('meta'):\n",
        "                if meta.get('property') in ['article:published_time', 'og:published_time', 'publication_date'] or meta.get('name') in ['publication_date', 'date', 'pubdate']:\n",
        "                    data = meta.get('content')\n",
        "                    break\n",
        "\n",
        "            # Procura por elementos que possam conter autores\n",
        "            autores = []\n",
        "            for author_tag in ['author', 'byline', '.author', '.byline', '[rel=author]', '[name=author]']:\n",
        "                if author_tag.startswith('.') or author_tag.startswith('['):\n",
        "                    elements = soup.select(author_tag)\n",
        "                else:\n",
        "                    elements = soup.find_all(author_tag)\n",
        "\n",
        "                if elements:\n",
        "                    for element in elements:\n",
        "                        autor = element.get_text(strip=True)\n",
        "                        if autor and len(autor) < 100:  # Evitar pegar conteúdo grande por engano\n",
        "                            autores.append(autor)\n",
        "\n",
        "            # Remover duplicações nos autores\n",
        "            autores = list(dict.fromkeys(autores))\n",
        "\n",
        "            # Extrair imagens\n",
        "            imagens = []\n",
        "            for img in soup.find_all('img'):\n",
        "                src = img.get('src')\n",
        "                alt = img.get('alt', '')\n",
        "                if src and not src.startswith('data:'):\n",
        "                    # Garantir URL completa\n",
        "                    if src.startswith('/'):\n",
        "                        parsed_url = urlparse(url)\n",
        "                        base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
        "                        src = base_url + src\n",
        "                    imagens.append({\n",
        "                        'url': src,\n",
        "                        'alt': alt\n",
        "                    })\n",
        "\n",
        "            # Extrair palavras-chave\n",
        "            keywords = []\n",
        "            meta_keywords = soup.find('meta', attrs={'name': 'keywords'})\n",
        "            if meta_keywords:\n",
        "                keywords = meta_keywords.get('content', '').split(',')\n",
        "                keywords = [k.strip() for k in keywords]\n",
        "\n",
        "            return {\n",
        "                'titulo': titulo,\n",
        "                'conteudo': content,\n",
        "                'data': data,\n",
        "                'autores': autores,\n",
        "                'url': response.url,  # Use a URL final após redirecionamentos\n",
        "                'imagens': imagens,\n",
        "                'keywords': keywords,\n",
        "                'html': response.text  # Manter o HTML para possível análise adicional\n",
        "            }\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            logger.error(f\"Erro ao baixar artigo ({url}): {e}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro inesperado ao processar artigo ({url}): {e}\")\n",
        "            return None\n",
        "\n",
        "   class VerificadorFactCheck:\n",
        "    def __init__(self):\n",
        "        self.fact_check_api_url = \"https://factchecktools.googleapis.com/v1alpha1/claims:search\"\n",
        "\n",
        "    def consultar_fact_check(self, query):\n",
        "        \"\"\"Consulta a API do Google Fact Check Tools.\"\"\"\n",
        "\n",
        "        # Carregar variáveis de ambiente\n",
        "        load_dotenv()\n",
        "        api_key = os.getenv(\"FACT_CHECK_API_KEY\")\n",
        "\n",
        "        if not api_key or api_key.strip() == \"\":\n",
        "            raise ValueError(\"Chave de API não encontrada no arquivo .env. Certifique-se de definir FACT_CHECK_API_KEY corretamente.\")\n",
        "\n",
        "        # Verificar se a URL base da API está configurada\n",
        "        if not self.fact_check_api_url.strip():\n",
        "            logger.warning(\"URL base para a API de Fact Check não configurada.\")\n",
        "            return None\n",
        "\n",
        "        # Construir a URL da API\n",
        "        url_api = f\"{self.fact_check_api_url}?key={api_key}&query={query}\"\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url_api)\n",
        "            if response.status_code == 200:\n",
        "                return response.json()\n",
        "            else:\n",
        "                logger.warning(f\"Falha ao acessar API de Fact Check. Status: {response.status_code}\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro na API de Fact Check: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "\n",
        "    def analisar_linguagem(self, texto):\n",
        "        \"\"\"Analisa o conteúdo em busca de sensacionalismo, incerteza e opinatividade.\"\"\"\n",
        "        if not texto or len(texto) < 10:\n",
        "            return {\n",
        "                \"sensacionalismo\": 0,\n",
        "                \"incerteza\": 0,\n",
        "                \"opinativo\": 0,\n",
        "                \"exclamacoes\": 0\n",
        "            }\n",
        "\n",
        "        resultado = {}\n",
        "\n",
        "        # Análise de sensacionalismo\n",
        "        indice_sensacionalismo = sum(\n",
        "            len(re.findall(r'\\b' + re.escape(palavra) + r'\\b', texto.lower())) * peso\n",
        "            for palavra, peso in self.palavras_sensacionalistas.items()\n",
        "        )\n",
        "        resultado[\"sensacionalismo\"] = indice_sensacionalismo\n",
        "\n",
        "        # Normalizar por tamanho do texto (por 1000 palavras)\n",
        "        palavras_totais = len(texto.split())\n",
        "        if palavras_totais > 0:\n",
        "            indice_sensacionalismo_norm = (indice_sensacionalismo * 1000) / palavras_totais\n",
        "            resultado[\"sensacionalismo_normalizado\"] = indice_sensacionalismo_norm\n",
        "\n",
        "            # Adicionar alerta se o índice for alto\n",
        "            if indice_sensacionalismo_norm > 10:\n",
        "                resultado[\"alerta_sensacionalismo\"] = \"Alto índice de linguagem sensacionalista\"\n",
        "            elif indice_sensacionalismo_norm > 5:\n",
        "                resultado[\"alerta_sensacionalismo\"] = \"Moderado índice de linguagem sensacionalista\"\n",
        "\n",
        "        # Análise de incerteza\n",
        "        indice_incerteza = sum(\n",
        "            len(re.findall(r'\\b' + re.escape(palavra) + r'\\b', texto.lower()))\n",
        "            for palavra in self.palavras_incerteza\n",
        "        )\n",
        "        resultado[\"incerteza\"] = indice_incerteza\n",
        "\n",
        "        if palavras_totais > 0:\n",
        "            indice_incerteza_norm = (indice_incerteza * 1000) / palavras_totais\n",
        "            resultado[\"incerteza_normalizada\"] = indice_incerteza_norm\n",
        "\n",
        "            if indice_incerteza_norm > 8:\n",
        "                resultado[\"alerta_incerteza\"] = \"Alto índice de expressões de incerteza\"\n",
        "\n",
        "        # Análise de opinatividade\n",
        "        indice_opiniao = sum(\n",
        "            len(re.findall(r'\\b' + re.escape(expressao) + r'\\b', texto.lower()))\n",
        "            for expressao in self.expressoes_opinativas\n",
        "        )\n",
        "        resultado[\"opinativo\"] = indice_opiniao\n",
        "\n",
        "        if palavras_totais > 0:\n",
        "            indice_opiniao_norm = (indice_opiniao * 1000) / palavras_totais\n",
        "            resultado[\"opinativo_normalizado\"] = indice_opiniao_norm\n",
        "\n",
        "            if indice_opiniao_norm > 5:\n",
        "                resultado[\"alerta_opiniao\"] = \"Alto índice de expressões opinativas\"\n",
        "\n",
        "        # Contagem de pontos de exclamação (indicador de sensacionalismo)\n",
        "        exclamacoes = texto.count('!')\n",
        "        resultado[\"exclamacoes\"] = exclamacoes\n",
        "\n",
        "        if palavras_totais > 0:\n",
        "            exclamacoes_norm = (exclamacoes * 1000) / palavras_totais\n",
        "            if exclamacoes_norm > 10:\n",
        "                resultado[\"alerta_exclamacoes\"] = \"Uso excessivo de pontos de exclamação\"\n",
        "\n",
        "        # Análise de sentimento usando TextBlob\n",
        "        try:\n",
        "            analise = TextBlob(texto)\n",
        "            resultado[\"sentimento_polaridade\"] = analise.sentiment.polarity  # -1 a 1 (negativo a positivo)\n",
        "            resultado[\"sentimento_subjetividade\"] = analise.sentiment.subjectivity  # 0 a 1 (objetivo a subjetivo)\n",
        "\n",
        "            # Categorização baseada na polaridade\n",
        "            if resultado[\"sentimento_polaridade\"] < -0.5:\n",
        "                resultado[\"sentimento_categoria\"] = \"muito negativo\"\n",
        "            elif resultado[\"sentimento_polaridade\"] < -0.1:\n",
        "                resultado[\"sentimento_categoria\"] = \"negativo\"\n",
        "            elif resultado[\"sentimento_polaridade\"] <= 0.1:\n",
        "                resultado[\"sentimento_categoria\"] = \"neutro\"\n",
        "            elif resultado[\"sentimento_polaridade\"] <= 0.5:\n",
        "                resultado[\"sentimento_categoria\"] = \"positivo\"\n",
        "            else:\n",
        "                resultado[\"sentimento_categoria\"] = \"muito positivo\"\n",
        "\n",
        "            # Alerta para alto nível de subjetividade em notícias que deveriam ser objetivas\n",
        "            if resultado[\"sentimento_subjetividade\"] > 0.6:\n",
        "                resultado[\"alerta_subjetividade\"] = \"Conteúdo altamente subjetivo\"\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Erro na análise de sentimento: {e}\")\n",
        "\n",
        "        return resultado\n",
        "\n",
        "    def verificar_imagens(self, imagens):\n",
        "        \"\"\"Analisa as imagens associadas ao artigo.\"\"\"\n",
        "        resultados = {}\n",
        "        if not imagens:\n",
        "            resultados[\"alerta_imagens\"] = \"Nenhuma imagem encontrada no artigo\"\n",
        "            return resultados\n",
        "\n",
        "        resultados[\"num_imagens\"] = len(imagens)\n",
        "\n",
        "        # Verificar descrições das imagens (alt text)\n",
        "        imagens_sem_descricao = sum(1 for img in imagens if not img.get('alt'))\n",
        "        resultados[\"imagens_sem_descricao\"] = imagens_sem_descricao\n",
        "\n",
        "        if imagens_sem_descricao == len(imagens) and len(imagens) > 0:\n",
        "            resultados[\"alerta_imagens_descricao\"] = \"Nenhuma das imagens possui descrição adequada\"\n",
        "\n",
        "        # Calcular hash básico das URLs de imagens para potencial comparação futura\n",
        "        hashes_urls = [hashlib.md5(img['url'].encode()).hexdigest()[:8] for img in imagens]\n",
        "        resultados[\"hashes_imagens\"] = hashes_urls\n",
        "\n",
        "        # Analisar metadados das primeiras 5 imagens (limitar para evitar muito processamento)\n",
        "        try:\n",
        "            with ThreadPoolExecutor(max_workers=3) as executor:\n",
        "                metadados = list(executor.map(self.extrair_metadados_imagem, [img['url'] for img in imagens[:5]]))\n",
        "                resultados[\"metadados_imagens\"] = [m for m in metadados if m]  # Filtrar None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro ao analisar metadados das imagens: {e}\")\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def extrair_metadados_imagem(self, url_imagem):\n",
        "        \"\"\"Extrai metadados de uma imagem a partir da URL.\"\"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            response = requests.get(url_imagem, headers=headers, stream=True, timeout=5, verify=False)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Abrir a imagem com PIL\n",
        "            img = Image.open(io.BytesIO(response.content))\n",
        "\n",
        "            # Coletar metadados básicos\n",
        "            metadados = {\n",
        "                \"formato\": img.format,\n",
        "                \"tamanho\": img.size,\n",
        "                \"modo\": img.mode,\n",
        "                \"url\": url_imagem,\n",
        "                \"hash\": hashlib.md5(response.content).hexdigest()\n",
        "            }\n",
        "\n",
        "            # Tentar extrair EXIF se disponível\n",
        "            if hasattr(img, '_getexif') and img._getexif():\n",
        "                exif = img._getexif()\n",
        "                if exif:\n",
        "                    exif_data = {}\n",
        "                    for tag_id, value in exif.items():\n",
        "                        # Converter para string para garantir serialização JSON\n",
        "                        exif_data[str(tag_id)] = str(value)\n",
        "                    metadados[\"exif\"] = exif_data\n",
        "\n",
        "            return metadados\n",
        "        except (requests.RequestException, UnidentifiedImageError, IOError) as e:\n",
        "            logger.warning(f\"Erro ao extrair metadados da imagem {url_imagem}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def buscar_fact_check(self, texto, titulo=None):\n",
        "        \"\"\"Busca por verificações de fatos relacionadas ao conteúdo.\"\"\"\n",
        "        resultados = {}\n",
        "\n",
        "        try:\n",
        "            # Preparar termos de busca\n",
        "            query = titulo if titulo else texto[:100]\n",
        "\n",
        "            # Consultar API de fact check\n",
        "            data = self.consultar_fact_check(query)\n",
        "\n",
        "            if data:\n",
        "                resultados[\"fact_checks\"] = data.get('claims', [])\n",
        "                resultados[\"num_fact_checks\"] = len(resultados[\"fact_checks\"])\n",
        "\n",
        "                # Analisar resultados\n",
        "                if resultados[\"num_fact_checks\"] > 0:\n",
        "                    # Verificar ratings médios\n",
        "                    ratings = []\n",
        "                    for claim in resultados[\"fact_checks\"]:\n",
        "                        for review in claim.get('claimReview', []):\n",
        "                            if 'textualRating' in review:\n",
        "                                ratings.append(review['textualRating'].lower())\n",
        "\n",
        "                    # Contar frequência de cada rating\n",
        "                    if ratings:\n",
        "                        counter = Counter(ratings)\n",
        "                        resultados[\"ratings_frequencia\"] = counter\n",
        "\n",
        "                        # Detectar termos comuns de falsidade\n",
        "                        termos_falso = ['false', 'fake', 'falso', 'mentira', 'enganoso', 'misleading']\n",
        "                        count_falso = sum(counter.get(termo, 0) for termo in termos_falso)\n",
        "\n",
        "                        if count_falso > 0:\n",
        "                            resultados[\"alerta_fact_check\"] = \"Conteúdo marcado como falso ou enganoso por verificadores de fatos\"\n",
        "                            resultados[\"pontos_fact_check\"] = -25\n",
        "                        else:\n",
        "                            resultados[\"pontos_fact_check\"] = 0\n",
        "            else:\n",
        "                resultados[\"fact_checks\"] = []\n",
        "                resultados[\"num_fact_checks\"] = 0\n",
        "                resultados[\"pontos_fact_check\"] = 0\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro ao buscar fact checks: {e}\")\n",
        "            resultados[\"erro_fact_check\"] = str(e)\n",
        "            resultados[\"pontos_fact_check\"] = 0\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def buscar_noticias_similares(self, titulo, keywords, conteudo=None):\n",
        "        \"\"\"Busca por notícias similares na web usando palavras-chave do artigo.\"\"\"\n",
        "        resultados = {}\n",
        "        try:\n",
        "            # Preparar termos de busca\n",
        "            termos_busca = titulo\n",
        "            if keywords:\n",
        "                termos_busca += \" \" + \" \".join(keywords[:3])  # Limitar a 3 keywords\n",
        "\n",
        "            # Simular resultados para demonstração\n",
        "            # Em uma implementação real, você usaria uma API como Google News ou similar\n",
        "            resultados[\"artigos_similares\"] = 5  # Simulando 5 artigos similares\n",
        "            resultados[\"similaridade\"] = \"média\"  # baixa, média, alta\n",
        "\n",
        "            # Simular consistência entre fontes\n",
        "            resultados[\"consistencia_entre_fontes\"] = \"alta\"  # baixa, média, alta\n",
        "            resultados[\"pontos_consistencia\"] = 10  # Simulando pontos por alta consistência\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro ao buscar notícias similares: {e}\")\n",
        "            resultados[\"erro_busca\"] = str(e)\n",
        "            resultados[\"pontos_consistencia\"] = 0\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def converter_nivel_para_texto(self, nivel):\n",
        "        \"\"\"Converte um nível numérico de veracidade para texto.\"\"\"\n",
        "        nivel_texto = {\n",
        "            1: \"CONFIRMADO\",\n",
        "            2: \"PROVAVELMENTE VERDADEIRO\",\n",
        "            3: \"INCONCLUSIVO\",\n",
        "            4: \"PROVAVELMENTE FALSO\",\n",
        "            5: \"FALSO\"\n",
        "        }.get(nivel, \"INCONCLUSIVO\")\n",
        "        return nivel_texto\n",
        "\n",
        "    def analisar_credibilidade(self, url):\n",
        "        \"\"\"Análise completa de credibilidade de uma URL de notícia.\"\"\"\n",
        "        # 1. Baixar o artigo\n",
        "        artigo = self.baixar_artigo(url)\n",
        "        if not artigo or not artigo.get('conteudo'):\n",
        "            logger.error(f\"Não foi possível baixar o artigo: {url}\")\n",
        "            return {\"erro\": \"Não foi possível baixar o artigo para análise\"}\n",
        "\n",
        "        resultados = {}\n",
        "        conteudo = artigo['conteudo']\n",
        "\n",
        "        # 2. Verificação de fonte/domínio\n",
        "        status_fonte, pontos_fonte, info_fonte = self.verificar_fonte(url)\n",
        "        resultados.update(info_fonte)\n",
        "\n",
        "        # 3. Copiar informações básicas\n",
        "        resultados[\"titulo\"] = artigo['titulo']\n",
        "        resultados[\"data_publicacao\"] = artigo['data']\n",
        "        resultados[\"autores\"] = artigo['autores']\n",
        "        resultados[\"url\"] = artigo['url']\n",
        "\n",
        "        # 4. Análise básica do conteúdo\n",
        "        palavras = len(conteudo.split())\n",
        "        resultados[\"num_palavras\"] = palavras\n",
        "        if palavras < 100:\n",
        "            resultados[\"alerta_tamanho\"] = \"Texto muito curto, pode não ser um artigo completo\"\n",
        "\n",
        "        # 5. Análise detalhada da linguagem\n",
        "        analise_linguagem = self.analisar_linguagem(conteudo)\n",
        "        resultados.update(analise_linguagem)\n",
        "\n",
        "        # 6. Verificação de citações\n",
        "        citacoes = re.findall(r'\"([^\"]+)\"', conteudo)\n",
        "        resultados[\"num_citacoes\"] = len(citacoes)\n",
        "        if len(citacoes) == 0:\n",
        "            resultados[\"alerta_fontes\"] = \"Nenhuma citação direta encontrada\"\n",
        "\n",
        "        # 7. Verificação de URLs externos (referências)\n",
        "        urls = re.findall(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', conteudo)\n",
        "        resultados[\"num_urls\"] = len(urls)\n",
        "\n",
        "        # 8. Análise de imagens\n",
        "        resultados_imagens = self.verificar_imagens(artigo.get('imagens', []))\n",
        "        resultados.update(resultados_imagens)\n",
        "\n",
        "        # 9. Verificação cruzada com outras notícias\n",
        "        resultados_similares = self.buscar_noticias_similares(\n",
        "            artigo['titulo'],\n",
        "            artigo.get('keywords', []),\n",
        "            conteudo\n",
        "        )\n",
        "        resultados.update(resultados_similares)\n",
        "\n",
        "        # 10. Verificação em serviços de fact-checking\n",
        "        resultados_fact_check = self.buscar_fact_check(conteudo, artigo['titulo'])\n",
        "        resultados.update(resultados_fact_check)\n",
        "\n",
        "        # Cálculo mais sofisticado usando vários fatores\n",
        "# 11. Determinar um nível de veracidade\n",
        "# Cálculo mais sofisticado usando vários fatores\n",
        "base_pontuacao = 70  # Base inicial neutra\n",
        "\n",
        "# Fatores que afetam a pontuação\n",
        "pontuacao_fatores = [\n",
        "    resultados.get(\"pontos_fonte\", 0),\n",
        "    resultados.get(\"pontos_consistencia\", 0),\n",
        "    resultados.get(\"pontos_evidencia\", 0),\n",
        "    resultados.get(\"pontos_conhecimento_previo\", 0),\n",
        "    resultados.get(\"pontos_contexto\", 0),\n",
        "    resultados.get(\"pontos_bias\", 0)\n",
        "]\n",
        "\n",
        "# Aplicar pesos aos fatores\n",
        "pesos = [1.5, 1.2, 2.0, 1.0, 1.0, 0.8]\n",
        "pontuacao_ponderada = sum(p * f for p, f in zip(pesos, pontuacao_fatores))\n",
        "\n",
        "# Calcular pontuação final\n",
        "pontuacao_final = max(0, min(100, base_pontuacao + pontuacao_ponderada))\n",
        "\n",
        "# Determinar nível de veracidade baseado na pontuação final\n",
        "if pontuacao_final >= 90:\n",
        "    nivel_veracidade = \"Altamente verificado\"\n",
        "elif pontuacao_final >= 75:\n",
        "    nivel_veracidade = \"Provavelmente verdadeiro\"\n",
        "elif pontuacao_final >= 60:\n",
        "    nivel_veracidade = \"Parcialmente verificado\"\n",
        "elif pontuacao_final >= 40:\n",
        "    nivel_veracidade = \"Inconclusivo\"\n",
        "elif pontuacao_final >= 25:\n",
        "    nivel_veracidade = \"Provavelmente falso\"\n",
        "else:\n",
        "    nivel_veracidade = \"Falso\"\n",
        "\n",
        "resultados[\"pontuacao_veracidade\"] = pontuacao_final\n",
        "resultados[\"nivel_veracidade\"] = nivel_veracidade\n",
        "\n",
        "# 12. Sumarizar os resultados\n",
        "resumo = {\n",
        "    \"nivel_veracidade\": nivel_veracidade,\n",
        "    \"pontuacao\": pontuacao_final,\n",
        "    \"analise_detalhada\": resultados,\n",
        "    \"pontos_chave\": {\n",
        "        \"fontes\": resultados.get(\"qualidade_fonte\", \"Não analisado\"),\n",
        "        \"consistencia\": resultados.get(\"consistencia_narrativa\", \"Não analisado\"),\n",
        "        \"evidencias\": resultados.get(\"qualidade_evidencia\", \"Não analisado\"),\n",
        "        \"contexto\": resultados.get(\"analise_contexto\", \"Não analisado\")\n",
        "    },\n",
        "    \"recomendacao\": \"\"\n",
        "}\n",
        "\n",
        "# Adicionar uma recomendação baseada no nível de veracidade\n",
        "def adicionar_recomendacao(pontuacao_final):\n",
        "    \"\"\"Adiciona uma recomendação ao resumo com base no nível de veracidade.\"\"\"\n",
        "    resumo = {}\n",
        "    if pontuacao_final >= 75:\n",
        "        resumo[\"recomendacao\"] = \"Informação confiável para uso e disseminação.\"\n",
        "    elif pontuacao_final >= 50:\n",
        "        resumo[\"recomendacao\"] = \"Verificar informações adicionais antes de aceitar completamente.\"\n",
        "    else:\n",
        "        resumo[\"recomendacao\"] = \"Não recomendado para uso ou compartilhamento sem verificação adicional.\"\n",
        "\n",
        "    return resumo\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SRjwzbc5ggC",
        "outputId": "e7b2cc3e-66c3-40f7-9214-f344103c7434"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting verificacao_simples.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run verificacao_simples.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "NjM7ygTv5EAK",
        "outputId": "58f4ecbb-7597-4f97-958a-32b731c3f284"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unindent does not match any outer indentation level (verificacao_simples.py, line 273)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/content/verificacao_simples.py\"\u001b[0;36m, line \u001b[0;32m273\u001b[0m\n\u001b[0;31m    def consultar_fact_check(self, query):\u001b[0m\n\u001b[0m                                          ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile verificacao_simples.py\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import ssl\n",
        "import urllib3\n",
        "import json\n",
        "from urllib.parse import urlparse, urlencode\n",
        "import hashlib\n",
        "from collections import Counter\n",
        "import time\n",
        "import logging\n",
        "import base64\n",
        "import io\n",
        "import nltk\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"python-dotenv\"])\n",
        "    from dotenv import load_dotenv\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from textblob import TextBlob\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "from datetime import datetime\n",
        "\n",
        "# Configurar o logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(\"VerificadorNoticias\")\n",
        "\n",
        "# Desabilitar avisos SSL\n",
        "urllib3.disable_warnings()\n",
        "\n",
        "class VerificadorNoticias:\n",
        "    def __init__(self):\n",
        "        logger.info(\"Verificador de Notícias inicializado!\")\n",
        "\n",
        "        # Carregar variáveis do ambiente\n",
        "        load_dotenv()\n",
        "        self.fact_check_api_key = os.getenv(\"FACT_CHECK_API_KEY\")\n",
        "        self.fact_check_api_url = \"https://factchecktools.googleapis.com/v1alpha1/claims:search\"\n",
        "\n",
        "        # Carregar listas de fontes confiáveis e não confiáveis\n",
        "        self.fontes_confiaveis = [\n",
        "            'g1.globo.com', 'bbc.com', 'bbc.co.uk', 'reuters.com', 'apnews.com',\n",
        "            'nytimes.com', 'washingtonpost.com', 'estadao.com.br', 'folha.uol.com.br',\n",
        "            'cnn.com', 'valor.globo.com', 'info.abril.com.br', 'oglobo.globo.com',\n",
        "            'dw.com', 'npr.org', 'tecmundo.com.br', 'canaltech.com.br', 'olhardigital.com.br'\n",
        "        ]\n",
        "        self.fontes_suspeitas = [\n",
        "            'theonion.com', 'sensacionalista.com.br', 'breitbart.com', 'infowars.com',\n",
        "            'clickhole.com', 'nationalreport.net', 'worldnewsdailyreport.com',\n",
        "            'dailycurrant.com', 'empirenews.net'\n",
        "        ]\n",
        "\n",
        "        # Palavras para análise linguística\n",
        "        self.palavras_sensacionalistas = {\n",
        "            \"chocante\": 2, \"inacreditável\": 2, \"surpreendente\": 1, \"sensacional\": 2,\n",
        "            \"impressionante\": 1, \"alarmante\": 1.5, \"exclusivo\": 1, \"urgente\": 1.5,\n",
        "            \"shocking\": 2, \"unbelievable\": 2, \"amazing\": 1, \"surprising\": 1,\n",
        "            \"imperdível\": 1.5, \"incrível\": 1.5, \"absurdo\": 1.5, \"escandaloso\": 2,\n",
        "            \"bomba\": 1.5, \"inédito\": 1, \"revelado\": 1, \"segredo\": 1.5,\n",
        "            \"polêmico\": 1.5, \"controverso\": 1, \"escândalo\": 1.5, \"viral\": 1.5,\n",
        "            \"bizarro\": 1.5, \"aterrorizante\": 2, \"assustador\": 1.5, \"terrível\": 1.5,\n",
        "            \"estarrecedor\": 2, \"impossível\": 1.5\n",
        "        }\n",
        "\n",
        "        self.palavras_incerteza = [\n",
        "            \"talvez\", \"possivelmente\", \"provavelmente\", \"pode ser\", \"especula-se\",\n",
        "            \"acredita-se\", \"sugere\", \"alega\", \"supostamente\", \"rumores\",\n",
        "            \"maybe\", \"possibly\", \"probably\", \"might be\", \"it is speculated\",\n",
        "            \"it is believed\", \"suggests\", \"allegedly\", \"supposedly\", \"rumors\",\n",
        "            \"parece que\", \"há relatos\", \"não confirmado\", \"segundo fontes\",\n",
        "            \"indícios\", \"sinalizam\", \"aparentemente\", \"estaria\", \"poderia\",\n",
        "            \"teria\", \"estima-se\"\n",
        "        ]\n",
        "\n",
        "        self.expressoes_opinativas = [\n",
        "            \"eu acho\", \"na minha opinião\", \"parece que\", \"acredito que\",\n",
        "            \"i think\", \"in my opinion\", \"it seems that\", \"i believe\",\n",
        "            \"na visão de\", \"do meu ponto de vista\", \"considero\", \"entendo que\",\n",
        "            \"me parece\", \"creio que\", \"penso que\", \"sinto que\"\n",
        "        ]\n",
        "\n",
        "        # Tentar baixar recursos NLTK necessários para análise de texto\n",
        "        try:\n",
        "            nltk.download('punkt', quiet=True)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Não foi possível baixar recursos NLTK: {e}\")\n",
        "\n",
        "    def extrair_dominio(self, url):\n",
        "        \"\"\"Extrai o domínio base da URL.\"\"\"\n",
        "        parsed_url = urlparse(url)\n",
        "        return parsed_url.netloc\n",
        "\n",
        "    def verificar_url_simples(self, url):\n",
        "        \"\"\"Executa uma verificação simples em uma URL de notícia.\"\"\"\n",
        "        try:\n",
        "            # Extrair domínio\n",
        "            parsed_url = urlparse(url)\n",
        "            dominio = parsed_url.netloc\n",
        "\n",
        "            # Tentar acessar a URL\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers, timeout=10, verify=False)\n",
        "            response.raise_for_status()  # Levantar exceção para status HTTP de erro\n",
        "\n",
        "            # Verificar status HTTP\n",
        "            if response.status_code == 200:\n",
        "                logger.info(f\"URL acessada com sucesso: {url}\")\n",
        "                logger.info(f\"Domínio: {dominio}\")\n",
        "                logger.info(f\"Tamanho do conteúdo: {len(response.text)} caracteres.\")\n",
        "                return {\n",
        "                    \"status\": \"sucesso\",\n",
        "                    \"dominio\": dominio,\n",
        "                    \"tamanho_conteudo\": len(response.text),\n",
        "                    \"mensagem\": \"URL acessada com sucesso\"\n",
        "                }\n",
        "            else:\n",
        "                logger.warning(f\"Erro ao acessar a URL. Código HTTP: {response.status_code}\")\n",
        "                return {\n",
        "                    \"status\": \"erro\",\n",
        "                    \"mensagem\": f\"Erro ao acessar a URL. Código HTTP: {response.status_code}\"\n",
        "                }\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            logger.error(f\"Erro ao acessar a URL: {e}\")\n",
        "            return {\n",
        "                \"status\": \"erro\",\n",
        "                \"mensagem\": str(e)\n",
        "            }\n",
        "\n",
        "    def analisar_sentimento(self, texto):\n",
        "        \"\"\"Analisa o sentimento do texto usando TextBlob.\"\"\"\n",
        "        try:\n",
        "            sentiment = TextBlob(texto).sentiment\n",
        "            return {\n",
        "                \"polaridade\": sentiment.polarity,\n",
        "                \"subjetividade\": sentiment.subjectivity\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro na análise de sentimento: {e}\")\n",
        "            return {\n",
        "                \"polaridade\": 0,\n",
        "                \"subjetividade\": 0\n",
        "            }\n",
        "\n",
        "    def verificar_fonte(self, url):\n",
        "        \"\"\"Verifica o domínio da URL contra as listas de fontes.\"\"\"\n",
        "        dominio = self.extrair_dominio(url)\n",
        "        resultado = {\"dominio\": dominio}\n",
        "\n",
        "        # Verificar se o domínio está nas listas\n",
        "        if any(dominio.endswith(fonte) for fonte in self.fontes_confiaveis):\n",
        "            resultado[\"status_fonte\"] = \"confiável\"\n",
        "            resultado[\"pontos_fonte\"] = 20\n",
        "            return \"Fonte confiável\", 20, resultado\n",
        "        elif any(dominio.endswith(fonte) for fonte in self.fontes_suspeitas):\n",
        "            resultado[\"status_fonte\"] = \"suspeita\"\n",
        "            resultado[\"pontos_fonte\"] = -30\n",
        "            resultado[\"alerta_fonte\"] = \"Fonte conhecida por conteúdo satírico ou de baixa credibilidade\"\n",
        "            return \"Fonte suspeita\", -30, resultado\n",
        "        else:\n",
        "            resultado[\"status_fonte\"] = \"não categorizada\"\n",
        "            resultado[\"pontos_fonte\"] = 0\n",
        "            return \"Fonte não categorizada\", 0, resultado\n",
        "\n",
        "    def baixar_artigo(self, url):\n",
        "        \"\"\"Baixa o conteúdo do artigo a partir da URL.\"\"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers, verify=False, timeout=15)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Tenta extrair o título\n",
        "            titulo = soup.title.text.strip() if soup.title else \"Título não encontrado\"\n",
        "\n",
        "            # Tenta extrair o conteúdo do artigo (abordagem mais abrangente)\n",
        "            content = \"\"\n",
        "            # Procura por conteúdo em elementos comuns\n",
        "            for tag in ['article', 'main', '.content', '.post', '.entry', '#content', '.article-body', '.news-text']:\n",
        "                if tag.startswith('.') or tag.startswith('#'):\n",
        "                    elements = soup.select(tag)\n",
        "                else:\n",
        "                    elements = soup.find_all(tag)\n",
        "                if elements:\n",
        "                    for element in elements:\n",
        "                        content += element.get_text(separator=' ', strip=True) + \" \"\n",
        "                    break\n",
        "\n",
        "            # Se não encontrou conteúdo nos elementos específicos, pega todos os parágrafos\n",
        "            if not content:\n",
        "                paragraphs = soup.find_all('p')\n",
        "                content = ' '.join([p.get_text(strip=True) for p in paragraphs])\n",
        "\n",
        "            # Tenta encontrar a data\n",
        "            data = None\n",
        "            # Procura por metadados comuns\n",
        "            for meta in soup.find_all('meta'):\n",
        "                if meta.get('property') in ['article:published_time', 'og:published_time', 'publication_date'] or meta.get('name') in ['publication_date', 'date', 'pubdate']:\n",
        "                    data = meta.get('content')\n",
        "                    break\n",
        "\n",
        "            # Procura por elementos que possam conter autores\n",
        "            autores = []\n",
        "            for author_tag in ['author', 'byline', '.author', '.byline', '[rel=author]', '[name=author]']:\n",
        "                if author_tag.startswith('.') or author_tag.startswith('['):\n",
        "                    elements = soup.select(author_tag)\n",
        "                else:\n",
        "                    elements = soup.find_all(author_tag)\n",
        "\n",
        "                if elements:\n",
        "                    for element in elements:\n",
        "                        autor = element.get_text(strip=True)\n",
        "                        if autor and len(autor) < 100:  # Evitar pegar conteúdo grande por engano\n",
        "                            autores.append(autor)\n",
        "\n",
        "            # Remover duplicações nos autores\n",
        "            autores = list(dict.fromkeys(autores))\n",
        "\n",
        "            # Extrair imagens\n",
        "            imagens = []\n",
        "            for img in soup.find_all('img'):\n",
        "                src = img.get('src')\n",
        "                alt = img.get('alt', '')\n",
        "                if src and not src.startswith('data:'):\n",
        "                    # Garantir URL completa\n",
        "                    if src.startswith('/'):\n",
        "                        parsed_url = urlparse(url)\n",
        "                        base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
        "                        src = base_url + src\n",
        "                    imagens.append({\n",
        "                        'url': src,\n",
        "                        'alt': alt\n",
        "                    })\n",
        "\n",
        "            # Extrair palavras-chave\n",
        "            keywords = []\n",
        "            meta_keywords = soup.find('meta', attrs={'name': 'keywords'})\n",
        "            if meta_keywords:\n",
        "                keywords = meta_keywords.get('content', '').split(',')\n",
        "                keywords = [k.strip() for k in keywords]\n",
        "\n",
        "            return {\n",
        "                'titulo': titulo,\n",
        "                'conteudo': content,\n",
        "                'data': data,\n",
        "                'autores': autores,\n",
        "                'url': response.url,  # Use a URL final após redirecionamentos\n",
        "                'imagens': imagens,\n",
        "                'keywords': keywords,\n",
        "                'html': response.text  # Manter o HTML para possível análise adicional\n",
        "            }\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            logger.error(f\"Erro ao baixar artigo ({url}): {e}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro inesperado ao processar artigo ({url}): {e}\")\n",
        "            return None\n",
        "\n",
        "   def consultar_fact_check(self, query):\n",
        "    \"\"\"Consulta a API do Google Fact Check Tools.\"\"\"\n",
        "\n",
        "    # Carregar variáveis de ambiente\n",
        "    load_dotenv()\n",
        "    api_key = os.getenv(\"FACT_CHECK_API_KEY\")\n",
        "\n",
        "    if not api_key or api_key.strip() == \"\":\n",
        "        raise ValueError(\"Chave de API não encontrada no arquivo .env. Certifique-se de definir FACT_CHECK_API_KEY corretamente.\")\n",
        "\n",
        "    # Verificar se a URL base da API está configurada\n",
        "    if not hasattr(self, 'fact_check_api_url') or not self.fact_check_api_url.strip():\n",
        "        logger.warning(\"URL base para a API de Fact Check não configurada.\")\n",
        "        return None\n",
        "\n",
        "    # Construir a URL da API\n",
        "    url_api = f\"{self.fact_check_api_url}?key={api_key}&query={query}\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url_api)\n",
        "        if response.status_code == 200:\n",
        "            return response.json()\n",
        "        else:\n",
        "            logger.warning(f\"Falha ao acessar API de Fact Check. Status: {response.status_code}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Erro na API de Fact Check: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "    def analisar_linguagem(self, texto):\n",
        "        \"\"\"Analisa o conteúdo em busca de sensacionalismo, incerteza e opinatividade.\"\"\"\n",
        "        if not texto or len(texto) < 10:\n",
        "            return {\n",
        "                \"sensacionalismo\": 0,\n",
        "                \"incerteza\": 0,\n",
        "                \"opinativo\": 0,\n",
        "                \"exclamacoes\": 0\n",
        "            }\n",
        "\n",
        "        resultado = {}\n",
        "\n",
        "        # Análise de sensacionalismo\n",
        "        indice_sensacionalismo = sum(\n",
        "            len(re.findall(r'\\b' + re.escape(palavra) + r'\\b', texto.lower())) * peso\n",
        "            for palavra, peso in self.palavras_sensacionalistas.items()\n",
        "        )\n",
        "        resultado[\"sensacionalismo\"] = indice_sensacionalismo\n",
        "\n",
        "        # Normalizar por tamanho do texto (por 1000 palavras)\n",
        "        palavras_totais = len(texto.split())\n",
        "        if palavras_totais > 0:\n",
        "            indice_sensacionalismo_norm = (indice_sensacionalismo * 1000) / palavras_totais\n",
        "            resultado[\"sensacionalismo_normalizado\"] = indice_sensacionalismo_norm\n",
        "\n",
        "            # Adicionar alerta se o índice for alto\n",
        "            if indice_sensacionalismo_norm > 10:\n",
        "                resultado[\"alerta_sensacionalismo\"] = \"Alto índice de linguagem sensacionalista\"\n",
        "            elif indice_sensacionalismo_norm > 5:\n",
        "                resultado[\"alerta_sensacionalismo\"] = \"Moderado índice de linguagem sensacionalista\"\n",
        "\n",
        "        # Análise de incerteza\n",
        "        indice_incerteza = sum(\n",
        "            len(re.findall(r'\\b' + re.escape(palavra) + r'\\b', texto.lower()))\n",
        "            for palavra in self.palavras_incerteza\n",
        "        )\n",
        "        resultado[\"incerteza\"] = indice_incerteza\n",
        "\n",
        "        if palavras_totais > 0:\n",
        "            indice_incerteza_norm = (indice_incerteza * 1000) / palavras_totais\n",
        "            resultado[\"incerteza_normalizada\"] = indice_incerteza_norm\n",
        "\n",
        "            if indice_incerteza_norm > 8:\n",
        "                resultado[\"alerta_incerteza\"] = \"Alto índice de expressões de incerteza\"\n",
        "\n",
        "        # Análise de opinatividade\n",
        "        indice_opiniao = sum(\n",
        "            len(re.findall(r'\\b' + re.escape(expressao) + r'\\b', texto.lower()))\n",
        "            for expressao in self.expressoes_opinativas\n",
        "        )\n",
        "        resultado[\"opinativo\"] = indice_opiniao\n",
        "\n",
        "        if palavras_totais > 0:\n",
        "            indice_opiniao_norm = (indice_opiniao * 1000) / palavras_totais\n",
        "            resultado[\"opinativo_normalizado\"] = indice_opiniao_norm\n",
        "\n",
        "            if indice_opiniao_norm > 5:\n",
        "                resultado[\"alerta_opiniao\"] = \"Alto índice de expressões opinativas\"\n",
        "\n",
        "        # Contagem de pontos de exclamação (indicador de sensacionalismo)\n",
        "        exclamacoes = texto.count('!')\n",
        "        resultado[\"exclamacoes\"] = exclamacoes\n",
        "\n",
        "        if palavras_totais > 0:\n",
        "            exclamacoes_norm = (exclamacoes * 1000) / palavras_totais\n",
        "            if exclamacoes_norm > 10:\n",
        "                resultado[\"alerta_exclamacoes\"] = \"Uso excessivo de pontos de exclamação\"\n",
        "\n",
        "        # Análise de sentimento usando TextBlob\n",
        "        try:\n",
        "            analise = TextBlob(texto)\n",
        "            resultado[\"sentimento_polaridade\"] = analise.sentiment.polarity  # -1 a 1 (negativo a positivo)\n",
        "            resultado[\"sentimento_subjetividade\"] = analise.sentiment.subjectivity  # 0 a 1 (objetivo a subjetivo)\n",
        "\n",
        "            # Categorização baseada na polaridade\n",
        "            if resultado[\"sentimento_polaridade\"] < -0.5:\n",
        "                resultado[\"sentimento_categoria\"] = \"muito negativo\"\n",
        "            elif resultado[\"sentimento_polaridade\"] < -0.1:\n",
        "                resultado[\"sentimento_categoria\"] = \"negativo\"\n",
        "            elif resultado[\"sentimento_polaridade\"] <= 0.1:\n",
        "                resultado[\"sentimento_categoria\"] = \"neutro\"\n",
        "            elif resultado[\"sentimento_polaridade\"] <= 0.5:\n",
        "                resultado[\"sentimento_categoria\"] = \"positivo\"\n",
        "            else:\n",
        "                resultado[\"sentimento_categoria\"] = \"muito positivo\"\n",
        "\n",
        "            # Alerta para alto nível de subjetividade em notícias que deveriam ser objetivas\n",
        "            if resultado[\"sentimento_subjetividade\"] > 0.6:\n",
        "                resultado[\"alerta_subjetividade\"] = \"Conteúdo altamente subjetivo\"\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Erro na análise de sentimento: {e}\")\n",
        "\n",
        "        return resultado\n",
        "\n",
        "    def verificar_imagens(self, imagens):\n",
        "        \"\"\"Analisa as imagens associadas ao artigo.\"\"\"\n",
        "        resultados = {}\n",
        "        if not imagens:\n",
        "            resultados[\"alerta_imagens\"] = \"Nenhuma imagem encontrada no artigo\"\n",
        "            return resultados\n",
        "\n",
        "        resultados[\"num_imagens\"] = len(imagens)\n",
        "\n",
        "        # Verificar descrições das imagens (alt text)\n",
        "        imagens_sem_descricao = sum(1 for img in imagens if not img.get('alt'))\n",
        "        resultados[\"imagens_sem_descricao\"] = imagens_sem_descricao\n",
        "\n",
        "        if imagens_sem_descricao == len(imagens) and len(imagens) > 0:\n",
        "            resultados[\"alerta_imagens_descricao\"] = \"Nenhuma das imagens possui descrição adequada\"\n",
        "\n",
        "        # Calcular hash básico das URLs de imagens para potencial comparação futura\n",
        "        hashes_urls = [hashlib.md5(img['url'].encode()).hexdigest()[:8] for img in imagens]\n",
        "        resultados[\"hashes_imagens\"] = hashes_urls\n",
        "\n",
        "        # Analisar metadados das primeiras 5 imagens (limitar para evitar muito processamento)\n",
        "        try:\n",
        "            with ThreadPoolExecutor(max_workers=3) as executor:\n",
        "                metadados = list(executor.map(self.extrair_metadados_imagem, [img['url'] for img in imagens[:5]]))\n",
        "                resultados[\"metadados_imagens\"] = [m for m in metadados if m]  # Filtrar None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro ao analisar metadados das imagens: {e}\")\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def extrair_metadados_imagem(self, url_imagem):\n",
        "        \"\"\"Extrai metadados de uma imagem a partir da URL.\"\"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            response = requests.get(url_imagem, headers=headers, stream=True, timeout=5, verify=False)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Abrir a imagem com PIL\n",
        "            img = Image.open(io.BytesIO(response.content))\n",
        "\n",
        "            # Coletar metadados básicos\n",
        "            metadados = {\n",
        "                \"formato\": img.format,\n",
        "                \"tamanho\": img.size,\n",
        "                \"modo\": img.mode,\n",
        "                \"url\": url_imagem,\n",
        "                \"hash\": hashlib.md5(response.content).hexdigest()\n",
        "            }\n",
        "\n",
        "            # Tentar extrair EXIF se disponível\n",
        "            if hasattr(img, '_getexif') and img._getexif():\n",
        "                exif = img._getexif()\n",
        "                if exif:\n",
        "                    exif_data = {}\n",
        "                    for tag_id, value in exif.items():\n",
        "                        # Converter para string para garantir serialização JSON\n",
        "                        exif_data[str(tag_id)] = str(value)\n",
        "                    metadados[\"exif\"] = exif_data\n",
        "\n",
        "            return metadados\n",
        "        except (requests.RequestException, UnidentifiedImageError, IOError) as e:\n",
        "            logger.warning(f\"Erro ao extrair metadados da imagem {url_imagem}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def buscar_fact_check(self, texto, titulo=None):\n",
        "        \"\"\"Busca por verificações de fatos relacionadas ao conteúdo.\"\"\"\n",
        "        resultados = {}\n",
        "\n",
        "        try:\n",
        "            # Preparar termos de busca\n",
        "            query = titulo if titulo else texto[:100]\n",
        "\n",
        "            # Consultar API de fact check\n",
        "            data = self.consultar_fact_check(query)\n",
        "\n",
        "            if data:\n",
        "                resultados[\"fact_checks\"] = data.get('claims', [])\n",
        "                resultados[\"num_fact_checks\"] = len(resultados[\"fact_checks\"])\n",
        "\n",
        "                # Analisar resultados\n",
        "                if resultados[\"num_fact_checks\"] > 0:\n",
        "                    # Verificar ratings médios\n",
        "                    ratings = []\n",
        "                    for claim in resultados[\"fact_checks\"]:\n",
        "                        for review in claim.get('claimReview', []):\n",
        "                            if 'textualRating' in review:\n",
        "                                ratings.append(review['textualRating'].lower())\n",
        "\n",
        "                    # Contar frequência de cada rating\n",
        "                    if ratings:\n",
        "                        counter = Counter(ratings)\n",
        "                        resultados[\"ratings_frequencia\"] = counter\n",
        "\n",
        "                        # Detectar termos comuns de falsidade\n",
        "                        termos_falso = ['false', 'fake', 'falso', 'mentira', 'enganoso', 'misleading']\n",
        "                        count_falso = sum(counter.get(termo, 0) for termo in termos_falso)\n",
        "\n",
        "                        if count_falso > 0:\n",
        "                            resultados[\"alerta_fact_check\"] = \"Conteúdo marcado como falso ou enganoso por verificadores de fatos\"\n",
        "                            resultados[\"pontos_fact_check\"] = -25\n",
        "                        else:\n",
        "                            resultados[\"pontos_fact_check\"] = 0\n",
        "            else:\n",
        "                resultados[\"fact_checks\"] = []\n",
        "                resultados[\"num_fact_checks\"] = 0\n",
        "                resultados[\"pontos_fact_check\"] = 0\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro ao buscar fact checks: {e}\")\n",
        "            resultados[\"erro_fact_check\"] = str(e)\n",
        "            resultados[\"pontos_fact_check\"] = 0\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def buscar_noticias_similares(self, titulo, keywords, conteudo=None):\n",
        "        \"\"\"Busca por notícias similares na web usando palavras-chave do artigo.\"\"\"\n",
        "        resultados = {}\n",
        "        try:\n",
        "            # Preparar termos de busca\n",
        "            termos_busca = titulo\n",
        "            if keywords:\n",
        "                termos_busca += \" \" + \" \".join(keywords[:3])  # Limitar a 3 keywords\n",
        "\n",
        "            # Simular resultados para demonstração\n",
        "            # Em uma implementação real, você usaria uma API como Google News ou similar\n",
        "            resultados[\"artigos_similares\"] = 5  # Simulando 5 artigos similares\n",
        "            resultados[\"similaridade\"] = \"média\"  # baixa, média, alta\n",
        "\n",
        "            # Simular consistência entre fontes\n",
        "            resultados[\"consistencia_entre_fontes\"] = \"alta\"  # baixa, média, alta\n",
        "            resultados[\"pontos_consistencia\"] = 10  # Simulando pontos por alta consistência\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro ao buscar notícias similares: {e}\")\n",
        "            resultados[\"erro_busca\"] = str(e)\n",
        "            resultados[\"pontos_consistencia\"] = 0\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def converter_nivel_para_texto(self, nivel):\n",
        "        \"\"\"Converte um nível numérico de veracidade para texto.\"\"\"\n",
        "        nivel_texto = {\n",
        "            1: \"CONFIRMADO\",\n",
        "            2: \"PROVAVELMENTE VERDADEIRO\",\n",
        "            3: \"INCONCLUSIVO\",\n",
        "            4: \"PROVAVELMENTE FALSO\",\n",
        "            5: \"FALSO\"\n",
        "        }.get(nivel, \"INCONCLUSIVO\")\n",
        "        return nivel_texto\n",
        "\n",
        "    def analisar_credibilidade(self, url):\n",
        "        \"\"\"Análise completa de credibilidade de uma URL de notícia.\"\"\"\n",
        "        # 1. Baixar o artigo\n",
        "        artigo = self.baixar_artigo(url)\n",
        "        if not artigo or not artigo.get('conteudo'):\n",
        "            logger.error(f\"Não foi possível baixar o artigo: {url}\")\n",
        "            return {\"erro\": \"Não foi possível baixar o artigo para análise\"}\n",
        "\n",
        "        resultados = {}\n",
        "        conteudo = artigo['conteudo']\n",
        "\n",
        "        # 2. Verificação de fonte/domínio\n",
        "        status_fonte, pontos_fonte, info_fonte = self.verificar_fonte(url)\n",
        "        resultados.update(info_fonte)\n",
        "\n",
        "        # 3. Copiar informações básicas\n",
        "        resultados[\"titulo\"] = artigo['titulo']\n",
        "        resultados[\"data_publicacao\"] = artigo['data']\n",
        "        resultados[\"autores\"] = artigo['autores']\n",
        "        resultados[\"url\"] = artigo['url']\n",
        "\n",
        "        # 4. Análise básica do conteúdo\n",
        "        palavras = len(conteudo.split())\n",
        "        resultados[\"num_palavras\"] = palavras\n",
        "        if palavras < 100:\n",
        "            resultados[\"alerta_tamanho\"] = \"Texto muito curto, pode não ser um artigo completo\"\n",
        "\n",
        "        # 5. Análise detalhada da linguagem\n",
        "        analise_linguagem = self.analisar_linguagem(conteudo)\n",
        "        resultados.update(analise_linguagem)\n",
        "\n",
        "        # 6. Verificação de citações\n",
        "        citacoes = re.findall(r'\"([^\"]+)\"', conteudo)\n",
        "        resultados[\"num_citacoes\"] = len(citacoes)\n",
        "        if len(citacoes) == 0:\n",
        "            resultados[\"alerta_fontes\"] = \"Nenhuma citação direta encontrada\"\n",
        "\n",
        "        # 7. Verificação de URLs externos (referências)\n",
        "        urls = re.findall(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', conteudo)\n",
        "        resultados[\"num_urls\"] = len(urls)\n",
        "\n",
        "        # 8. Análise de imagens\n",
        "        resultados_imagens = self.verificar_imagens(artigo.get('imagens', []))\n",
        "        resultados.update(resultados_imagens)\n",
        "\n",
        "        # 9. Verificação cruzada com outras notícias\n",
        "        resultados_similares = self.buscar_noticias_similares(\n",
        "            artigo['titulo'],\n",
        "            artigo.get('keywords', []),\n",
        "            conteudo\n",
        "        )\n",
        "        resultados.update(resultados_similares)\n",
        "\n",
        "        # 10. Verificação em serviços de fact-checking\n",
        "        resultados_fact_check = self.buscar_fact_check(conteudo, artigo['titulo'])\n",
        "        resultados.update(resultados_fact_check)\n",
        "\n",
        "        # Cálculo mais sofisticado usando vários fatores\n",
        "# 11. Determinar um nível de veracidade\n",
        "# Cálculo mais sofisticado usando vários fatores\n",
        "base_pontuacao = 70  # Base inicial neutra\n",
        "\n",
        "# Fatores que afetam a pontuação\n",
        "pontuacao_fatores = [\n",
        "    resultados.get(\"pontos_fonte\", 0),\n",
        "    resultados.get(\"pontos_consistencia\", 0),\n",
        "    resultados.get(\"pontos_evidencia\", 0),\n",
        "    resultados.get(\"pontos_conhecimento_previo\", 0),\n",
        "    resultados.get(\"pontos_contexto\", 0),\n",
        "    resultados.get(\"pontos_bias\", 0)\n",
        "]\n",
        "\n",
        "# Aplicar pesos aos fatores\n",
        "pesos = [1.5, 1.2, 2.0, 1.0, 1.0, 0.8]\n",
        "pontuacao_ponderada = sum(p * f for p, f in zip(pesos, pontuacao_fatores))\n",
        "\n",
        "# Calcular pontuação final\n",
        "pontuacao_final = max(0, min(100, base_pontuacao + pontuacao_ponderada))\n",
        "\n",
        "# Determinar nível de veracidade baseado na pontuação final\n",
        "if pontuacao_final >= 90:\n",
        "    nivel_veracidade = \"Altamente verificado\"\n",
        "elif pontuacao_final >= 75:\n",
        "    nivel_veracidade = \"Provavelmente verdadeiro\"\n",
        "elif pontuacao_final >= 60:\n",
        "    nivel_veracidade = \"Parcialmente verificado\"\n",
        "elif pontuacao_final >= 40:\n",
        "    nivel_veracidade = \"Inconclusivo\"\n",
        "elif pontuacao_final >= 25:\n",
        "    nivel_veracidade = \"Provavelmente falso\"\n",
        "else:\n",
        "    nivel_veracidade = \"Falso\"\n",
        "\n",
        "resultados[\"pontuacao_veracidade\"] = pontuacao_final\n",
        "resultados[\"nivel_veracidade\"] = nivel_veracidade\n",
        "\n",
        "# 12. Sumarizar os resultados\n",
        "resumo = {\n",
        "    \"nivel_veracidade\": nivel_veracidade,\n",
        "    \"pontuacao\": pontuacao_final,\n",
        "    \"analise_detalhada\": resultados,\n",
        "    \"pontos_chave\": {\n",
        "        \"fontes\": resultados.get(\"qualidade_fonte\", \"Não analisado\"),\n",
        "        \"consistencia\": resultados.get(\"consistencia_narrativa\", \"Não analisado\"),\n",
        "        \"evidencias\": resultados.get(\"qualidade_evidencia\", \"Não analisado\"),\n",
        "        \"contexto\": resultados.get(\"analise_contexto\", \"Não analisado\")\n",
        "    },\n",
        "    \"recomendacao\": \"\"\n",
        "}\n",
        "\n",
        "# Adicionar uma recomendação baseada no nível de veracidade\n",
        "def adicionar_recomendacao(pontuacao_final):\n",
        "    \"\"\"Adiciona uma recomendação ao resumo com base no nível de veracidade.\"\"\"\n",
        "    resumo = {}\n",
        "    if pontuacao_final >= 75:\n",
        "        resumo[\"recomendacao\"] = \"Informação confiável para uso e disseminação.\"\n",
        "    elif pontuacao_final >= 50:\n",
        "        resumo[\"recomendacao\"] = \"Verificar informações adicionais antes de aceitar completamente.\"\n",
        "    else:\n",
        "        resumo[\"recomendacao\"] = \"Não recomendado para uso ou compartilhamento sem verificação adicional.\"\n",
        "\n",
        "    return resumo\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-K2Nmg-o46o3",
        "outputId": "50ae024b-f73a-40be-9caa-4e5944b51990"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting verificacao_simples.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run verificacao_simples.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "BmQR491n3fKa",
        "outputId": "e316aa97-3f06-4001-a7bc-a2ff740742d6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'resultados' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/content/verificacao_simples.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;31m# Fatores que afetam a pontuação\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m pontuacao_fatores = [\n\u001b[0;32m--> 603\u001b[0;31m     \u001b[0mresultados\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pontos_fonte\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m     \u001b[0mresultados\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pontos_consistencia\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0mresultados\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pontos_evidencia\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'resultados' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile verificacao_simples.py\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import ssl\n",
        "import urllib3\n",
        "import json\n",
        "from urllib.parse import urlparse, urlencode\n",
        "import hashlib\n",
        "from collections import Counter\n",
        "import time\n",
        "import logging\n",
        "import base64\n",
        "import io\n",
        "import nltk\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"python-dotenv\"])\n",
        "    from dotenv import load_dotenv\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from textblob import TextBlob\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "from datetime import datetime\n",
        "\n",
        "# Configurar o logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(\"VerificadorNoticias\")\n",
        "\n",
        "# Desabilitar avisos SSL\n",
        "urllib3.disable_warnings()\n",
        "\n",
        "class VerificadorNoticias:\n",
        "    def __init__(self):\n",
        "        logger.info(\"Verificador de Notícias inicializado!\")\n",
        "\n",
        "        # Carregar variáveis do ambiente\n",
        "        load_dotenv()\n",
        "        self.fact_check_api_key = os.getenv(\"FACT_CHECK_API_KEY\")\n",
        "        self.fact_check_api_url = \"https://factchecktools.googleapis.com/v1alpha1/claims:search\"\n",
        "\n",
        "        # Carregar listas de fontes confiáveis e não confiáveis\n",
        "        self.fontes_confiaveis = [\n",
        "            'g1.globo.com', 'bbc.com', 'bbc.co.uk', 'reuters.com', 'apnews.com',\n",
        "            'nytimes.com', 'washingtonpost.com', 'estadao.com.br', 'folha.uol.com.br',\n",
        "            'cnn.com', 'valor.globo.com', 'info.abril.com.br', 'oglobo.globo.com',\n",
        "            'dw.com', 'npr.org', 'tecmundo.com.br', 'canaltech.com.br', 'olhardigital.com.br'\n",
        "        ]\n",
        "        self.fontes_suspeitas = [\n",
        "            'theonion.com', 'sensacionalista.com.br', 'breitbart.com', 'infowars.com',\n",
        "            'clickhole.com', 'nationalreport.net', 'worldnewsdailyreport.com',\n",
        "            'dailycurrant.com', 'empirenews.net'\n",
        "        ]\n",
        "\n",
        "        # Palavras para análise linguística\n",
        "        self.palavras_sensacionalistas = {\n",
        "            \"chocante\": 2, \"inacreditável\": 2, \"surpreendente\": 1, \"sensacional\": 2,\n",
        "            \"impressionante\": 1, \"alarmante\": 1.5, \"exclusivo\": 1, \"urgente\": 1.5,\n",
        "            \"shocking\": 2, \"unbelievable\": 2, \"amazing\": 1, \"surprising\": 1,\n",
        "            \"imperdível\": 1.5, \"incrível\": 1.5, \"absurdo\": 1.5, \"escandaloso\": 2,\n",
        "            \"bomba\": 1.5, \"inédito\": 1, \"revelado\": 1, \"segredo\": 1.5,\n",
        "            \"polêmico\": 1.5, \"controverso\": 1, \"escândalo\": 1.5, \"viral\": 1.5,\n",
        "            \"bizarro\": 1.5, \"aterrorizante\": 2, \"assustador\": 1.5, \"terrível\": 1.5,\n",
        "            \"estarrecedor\": 2, \"impossível\": 1.5\n",
        "        }\n",
        "\n",
        "        self.palavras_incerteza = [\n",
        "            \"talvez\", \"possivelmente\", \"provavelmente\", \"pode ser\", \"especula-se\",\n",
        "            \"acredita-se\", \"sugere\", \"alega\", \"supostamente\", \"rumores\",\n",
        "            \"maybe\", \"possibly\", \"probably\", \"might be\", \"it is speculated\",\n",
        "            \"it is believed\", \"suggests\", \"allegedly\", \"supposedly\", \"rumors\",\n",
        "            \"parece que\", \"há relatos\", \"não confirmado\", \"segundo fontes\",\n",
        "            \"indícios\", \"sinalizam\", \"aparentemente\", \"estaria\", \"poderia\",\n",
        "            \"teria\", \"estima-se\"\n",
        "        ]\n",
        "\n",
        "        self.expressoes_opinativas = [\n",
        "            \"eu acho\", \"na minha opinião\", \"parece que\", \"acredito que\",\n",
        "            \"i think\", \"in my opinion\", \"it seems that\", \"i believe\",\n",
        "            \"na visão de\", \"do meu ponto de vista\", \"considero\", \"entendo que\",\n",
        "            \"me parece\", \"creio que\", \"penso que\", \"sinto que\"\n",
        "        ]\n",
        "\n",
        "        # Tentar baixar recursos NLTK necessários para análise de texto\n",
        "        try:\n",
        "            nltk.download('punkt', quiet=True)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Não foi possível baixar recursos NLTK: {e}\")\n",
        "\n",
        "    def extrair_dominio(self, url):\n",
        "        \"\"\"Extrai o domínio base da URL.\"\"\"\n",
        "        parsed_url = urlparse(url)\n",
        "        return parsed_url.netloc\n",
        "\n",
        "    def verificar_url_simples(self, url):\n",
        "        \"\"\"Executa uma verificação simples em uma URL de notícia.\"\"\"\n",
        "        try:\n",
        "            # Extrair domínio\n",
        "            parsed_url = urlparse(url)\n",
        "            dominio = parsed_url.netloc\n",
        "\n",
        "            # Tentar acessar a URL\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers, timeout=10, verify=False)\n",
        "            response.raise_for_status()  # Levantar exceção para status HTTP de erro\n",
        "\n",
        "            # Verificar status HTTP\n",
        "            if response.status_code == 200:\n",
        "                logger.info(f\"URL acessada com sucesso: {url}\")\n",
        "                logger.info(f\"Domínio: {dominio}\")\n",
        "                logger.info(f\"Tamanho do conteúdo: {len(response.text)} caracteres.\")\n",
        "                return {\n",
        "                    \"status\": \"sucesso\",\n",
        "                    \"dominio\": dominio,\n",
        "                    \"tamanho_conteudo\": len(response.text),\n",
        "                    \"mensagem\": \"URL acessada com sucesso\"\n",
        "                }\n",
        "            else:\n",
        "                logger.warning(f\"Erro ao acessar a URL. Código HTTP: {response.status_code}\")\n",
        "                return {\n",
        "                    \"status\": \"erro\",\n",
        "                    \"mensagem\": f\"Erro ao acessar a URL. Código HTTP: {response.status_code}\"\n",
        "                }\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            logger.error(f\"Erro ao acessar a URL: {e}\")\n",
        "            return {\n",
        "                \"status\": \"erro\",\n",
        "                \"mensagem\": str(e)\n",
        "            }\n",
        "\n",
        "    def analisar_sentimento(self, texto):\n",
        "        \"\"\"Analisa o sentimento do texto usando TextBlob.\"\"\"\n",
        "        try:\n",
        "            sentiment = TextBlob(texto).sentiment\n",
        "            return {\n",
        "                \"polaridade\": sentiment.polarity,\n",
        "                \"subjetividade\": sentiment.subjectivity\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro na análise de sentimento: {e}\")\n",
        "            return {\n",
        "                \"polaridade\": 0,\n",
        "                \"subjetividade\": 0\n",
        "            }\n",
        "\n",
        "    def verificar_fonte(self, url):\n",
        "        \"\"\"Verifica o domínio da URL contra as listas de fontes.\"\"\"\n",
        "        dominio = self.extrair_dominio(url)\n",
        "        resultado = {\"dominio\": dominio}\n",
        "\n",
        "        # Verificar se o domínio está nas listas\n",
        "        if any(dominio.endswith(fonte) for fonte in self.fontes_confiaveis):\n",
        "            resultado[\"status_fonte\"] = \"confiável\"\n",
        "            resultado[\"pontos_fonte\"] = 20\n",
        "            return \"Fonte confiável\", 20, resultado\n",
        "        elif any(dominio.endswith(fonte) for fonte in self.fontes_suspeitas):\n",
        "            resultado[\"status_fonte\"] = \"suspeita\"\n",
        "            resultado[\"pontos_fonte\"] = -30\n",
        "            resultado[\"alerta_fonte\"] = \"Fonte conhecida por conteúdo satírico ou de baixa credibilidade\"\n",
        "            return \"Fonte suspeita\", -30, resultado\n",
        "        else:\n",
        "            resultado[\"status_fonte\"] = \"não categorizada\"\n",
        "            resultado[\"pontos_fonte\"] = 0\n",
        "            return \"Fonte não categorizada\", 0, resultado\n",
        "\n",
        "    def baixar_artigo(self, url):\n",
        "        \"\"\"Baixa o conteúdo do artigo a partir da URL.\"\"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers, verify=False, timeout=15)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Tenta extrair o título\n",
        "            titulo = soup.title.text.strip() if soup.title else \"Título não encontrado\"\n",
        "\n",
        "            # Tenta extrair o conteúdo do artigo (abordagem mais abrangente)\n",
        "            content = \"\"\n",
        "            # Procura por conteúdo em elementos comuns\n",
        "            for tag in ['article', 'main', '.content', '.post', '.entry', '#content', '.article-body', '.news-text']:\n",
        "                if tag.startswith('.') or tag.startswith('#'):\n",
        "                    elements = soup.select(tag)\n",
        "                else:\n",
        "                    elements = soup.find_all(tag)\n",
        "                if elements:\n",
        "                    for element in elements:\n",
        "                        content += element.get_text(separator=' ', strip=True) + \" \"\n",
        "                    break\n",
        "\n",
        "            # Se não encontrou conteúdo nos elementos específicos, pega todos os parágrafos\n",
        "            if not content:\n",
        "                paragraphs = soup.find_all('p')\n",
        "                content = ' '.join([p.get_text(strip=True) for p in paragraphs])\n",
        "\n",
        "            # Tenta encontrar a data\n",
        "            data = None\n",
        "            # Procura por metadados comuns\n",
        "            for meta in soup.find_all('meta'):\n",
        "                if meta.get('property') in ['article:published_time', 'og:published_time', 'publication_date'] or meta.get('name') in ['publication_date', 'date', 'pubdate']:\n",
        "                    data = meta.get('content')\n",
        "                    break\n",
        "\n",
        "            # Procura por elementos que possam conter autores\n",
        "            autores = []\n",
        "            for author_tag in ['author', 'byline', '.author', '.byline', '[rel=author]', '[name=author]']:\n",
        "                if author_tag.startswith('.') or author_tag.startswith('['):\n",
        "                    elements = soup.select(author_tag)\n",
        "                else:\n",
        "                    elements = soup.find_all(author_tag)\n",
        "\n",
        "                if elements:\n",
        "                    for element in elements:\n",
        "                        autor = element.get_text(strip=True)\n",
        "                        if autor and len(autor) < 100:  # Evitar pegar conteúdo grande por engano\n",
        "                            autores.append(autor)\n",
        "\n",
        "            # Remover duplicações nos autores\n",
        "            autores = list(dict.fromkeys(autores))\n",
        "\n",
        "            # Extrair imagens\n",
        "            imagens = []\n",
        "            for img in soup.find_all('img'):\n",
        "                src = img.get('src')\n",
        "                alt = img.get('alt', '')\n",
        "                if src and not src.startswith('data:'):\n",
        "                    # Garantir URL completa\n",
        "                    if src.startswith('/'):\n",
        "                        parsed_url = urlparse(url)\n",
        "                        base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
        "                        src = base_url + src\n",
        "                    imagens.append({\n",
        "                        'url': src,\n",
        "                        'alt': alt\n",
        "                    })\n",
        "\n",
        "            # Extrair palavras-chave\n",
        "            keywords = []\n",
        "            meta_keywords = soup.find('meta', attrs={'name': 'keywords'})\n",
        "            if meta_keywords:\n",
        "                keywords = meta_keywords.get('content', '').split(',')\n",
        "                keywords = [k.strip() for k in keywords]\n",
        "\n",
        "            return {\n",
        "                'titulo': titulo,\n",
        "                'conteudo': content,\n",
        "                'data': data,\n",
        "                'autores': autores,\n",
        "                'url': response.url,  # Use a URL final após redirecionamentos\n",
        "                'imagens': imagens,\n",
        "                'keywords': keywords,\n",
        "                'html': response.text  # Manter o HTML para possível análise adicional\n",
        "            }\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            logger.error(f\"Erro ao baixar artigo ({url}): {e}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro inesperado ao processar artigo ({url}): {e}\")\n",
        "            return None\n",
        "\n",
        "    def consultar_fact_check(self, query):\n",
        "        \"\"\"Consulta a API do Google Fact Check Tools.\"\"\"\n",
        "        if not self.fact_check_api_key:\n",
        "            logger.warning(\"Chave de API para fact check não configurada\")\n",
        "            return None\n",
        "\n",
        "        url_api = f\"{self.fact_check_api_url}?key={self.fact_check_api_key}&query={query}\"\n",
        "        try:\n",
        "            response = requests.get(url_api)\n",
        "            if response.status_code == 200:\n",
        "                return response.json()\n",
        "            else:\n",
        "                logger.warning(f\"Falha ao acessar API de Fact Check. Status: {response.status_code}\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro na API de Fact Check: {e}\")\n",
        "            return None\n",
        "\n",
        "    def analisar_linguagem(self, texto):\n",
        "        \"\"\"Analisa o conteúdo em busca de sensacionalismo, incerteza e opinatividade.\"\"\"\n",
        "        if not texto or len(texto) < 10:\n",
        "            return {\n",
        "                \"sensacionalismo\": 0,\n",
        "                \"incerteza\": 0,\n",
        "                \"opinativo\": 0,\n",
        "                \"exclamacoes\": 0\n",
        "            }\n",
        "\n",
        "        resultado = {}\n",
        "\n",
        "        # Análise de sensacionalismo\n",
        "        indice_sensacionalismo = sum(\n",
        "            len(re.findall(r'\\b' + re.escape(palavra) + r'\\b', texto.lower())) * peso\n",
        "            for palavra, peso in self.palavras_sensacionalistas.items()\n",
        "        )\n",
        "        resultado[\"sensacionalismo\"] = indice_sensacionalismo\n",
        "\n",
        "        # Normalizar por tamanho do texto (por 1000 palavras)\n",
        "        palavras_totais = len(texto.split())\n",
        "        if palavras_totais > 0:\n",
        "            indice_sensacionalismo_norm = (indice_sensacionalismo * 1000) / palavras_totais\n",
        "            resultado[\"sensacionalismo_normalizado\"] = indice_sensacionalismo_norm\n",
        "\n",
        "            # Adicionar alerta se o índice for alto\n",
        "            if indice_sensacionalismo_norm > 10:\n",
        "                resultado[\"alerta_sensacionalismo\"] = \"Alto índice de linguagem sensacionalista\"\n",
        "            elif indice_sensacionalismo_norm > 5:\n",
        "                resultado[\"alerta_sensacionalismo\"] = \"Moderado índice de linguagem sensacionalista\"\n",
        "\n",
        "        # Análise de incerteza\n",
        "        indice_incerteza = sum(\n",
        "            len(re.findall(r'\\b' + re.escape(palavra) + r'\\b', texto.lower()))\n",
        "            for palavra in self.palavras_incerteza\n",
        "        )\n",
        "        resultado[\"incerteza\"] = indice_incerteza\n",
        "\n",
        "        if palavras_totais > 0:\n",
        "            indice_incerteza_norm = (indice_incerteza * 1000) / palavras_totais\n",
        "            resultado[\"incerteza_normalizada\"] = indice_incerteza_norm\n",
        "\n",
        "            if indice_incerteza_norm > 8:\n",
        "                resultado[\"alerta_incerteza\"] = \"Alto índice de expressões de incerteza\"\n",
        "\n",
        "        # Análise de opinatividade\n",
        "        indice_opiniao = sum(\n",
        "            len(re.findall(r'\\b' + re.escape(expressao) + r'\\b', texto.lower()))\n",
        "            for expressao in self.expressoes_opinativas\n",
        "        )\n",
        "        resultado[\"opinativo\"] = indice_opiniao\n",
        "\n",
        "        if palavras_totais > 0:\n",
        "            indice_opiniao_norm = (indice_opiniao * 1000) / palavras_totais\n",
        "            resultado[\"opinativo_normalizado\"] = indice_opiniao_norm\n",
        "\n",
        "            if indice_opiniao_norm > 5:\n",
        "                resultado[\"alerta_opiniao\"] = \"Alto índice de expressões opinativas\"\n",
        "\n",
        "        # Contagem de pontos de exclamação (indicador de sensacionalismo)\n",
        "        exclamacoes = texto.count('!')\n",
        "        resultado[\"exclamacoes\"] = exclamacoes\n",
        "\n",
        "        if palavras_totais > 0:\n",
        "            exclamacoes_norm = (exclamacoes * 1000) / palavras_totais\n",
        "            if exclamacoes_norm > 10:\n",
        "                resultado[\"alerta_exclamacoes\"] = \"Uso excessivo de pontos de exclamação\"\n",
        "\n",
        "        # Análise de sentimento usando TextBlob\n",
        "        try:\n",
        "            analise = TextBlob(texto)\n",
        "            resultado[\"sentimento_polaridade\"] = analise.sentiment.polarity  # -1 a 1 (negativo a positivo)\n",
        "            resultado[\"sentimento_subjetividade\"] = analise.sentiment.subjectivity  # 0 a 1 (objetivo a subjetivo)\n",
        "\n",
        "            # Categorização baseada na polaridade\n",
        "            if resultado[\"sentimento_polaridade\"] < -0.5:\n",
        "                resultado[\"sentimento_categoria\"] = \"muito negativo\"\n",
        "            elif resultado[\"sentimento_polaridade\"] < -0.1:\n",
        "                resultado[\"sentimento_categoria\"] = \"negativo\"\n",
        "            elif resultado[\"sentimento_polaridade\"] <= 0.1:\n",
        "                resultado[\"sentimento_categoria\"] = \"neutro\"\n",
        "            elif resultado[\"sentimento_polaridade\"] <= 0.5:\n",
        "                resultado[\"sentimento_categoria\"] = \"positivo\"\n",
        "            else:\n",
        "                resultado[\"sentimento_categoria\"] = \"muito positivo\"\n",
        "\n",
        "            # Alerta para alto nível de subjetividade em notícias que deveriam ser objetivas\n",
        "            if resultado[\"sentimento_subjetividade\"] > 0.6:\n",
        "                resultado[\"alerta_subjetividade\"] = \"Conteúdo altamente subjetivo\"\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Erro na análise de sentimento: {e}\")\n",
        "\n",
        "        return resultado\n",
        "\n",
        "    def verificar_imagens(self, imagens):\n",
        "        \"\"\"Analisa as imagens associadas ao artigo.\"\"\"\n",
        "        resultados = {}\n",
        "        if not imagens:\n",
        "            resultados[\"alerta_imagens\"] = \"Nenhuma imagem encontrada no artigo\"\n",
        "            return resultados\n",
        "\n",
        "        resultados[\"num_imagens\"] = len(imagens)\n",
        "\n",
        "        # Verificar descrições das imagens (alt text)\n",
        "        imagens_sem_descricao = sum(1 for img in imagens if not img.get('alt'))\n",
        "        resultados[\"imagens_sem_descricao\"] = imagens_sem_descricao\n",
        "\n",
        "        if imagens_sem_descricao == len(imagens) and len(imagens) > 0:\n",
        "            resultados[\"alerta_imagens_descricao\"] = \"Nenhuma das imagens possui descrição adequada\"\n",
        "\n",
        "        # Calcular hash básico das URLs de imagens para potencial comparação futura\n",
        "        hashes_urls = [hashlib.md5(img['url'].encode()).hexdigest()[:8] for img in imagens]\n",
        "        resultados[\"hashes_imagens\"] = hashes_urls\n",
        "\n",
        "        # Analisar metadados das primeiras 5 imagens (limitar para evitar muito processamento)\n",
        "        try:\n",
        "            with ThreadPoolExecutor(max_workers=3) as executor:\n",
        "                metadados = list(executor.map(self.extrair_metadados_imagem, [img['url'] for img in imagens[:5]]))\n",
        "                resultados[\"metadados_imagens\"] = [m for m in metadados if m]  # Filtrar None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro ao analisar metadados das imagens: {e}\")\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def extrair_metadados_imagem(self, url_imagem):\n",
        "        \"\"\"Extrai metadados de uma imagem a partir da URL.\"\"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            response = requests.get(url_imagem, headers=headers, stream=True, timeout=5, verify=False)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Abrir a imagem com PIL\n",
        "            img = Image.open(io.BytesIO(response.content))\n",
        "\n",
        "            # Coletar metadados básicos\n",
        "            metadados = {\n",
        "                \"formato\": img.format,\n",
        "                \"tamanho\": img.size,\n",
        "                \"modo\": img.mode,\n",
        "                \"url\": url_imagem,\n",
        "                \"hash\": hashlib.md5(response.content).hexdigest()\n",
        "            }\n",
        "\n",
        "            # Tentar extrair EXIF se disponível\n",
        "            if hasattr(img, '_getexif') and img._getexif():\n",
        "                exif = img._getexif()\n",
        "                if exif:\n",
        "                    exif_data = {}\n",
        "                    for tag_id, value in exif.items():\n",
        "                        # Converter para string para garantir serialização JSON\n",
        "                        exif_data[str(tag_id)] = str(value)\n",
        "                    metadados[\"exif\"] = exif_data\n",
        "\n",
        "            return metadados\n",
        "        except (requests.RequestException, UnidentifiedImageError, IOError) as e:\n",
        "            logger.warning(f\"Erro ao extrair metadados da imagem {url_imagem}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def buscar_fact_check(self, texto, titulo=None):\n",
        "        \"\"\"Busca por verificações de fatos relacionadas ao conteúdo.\"\"\"\n",
        "        resultados = {}\n",
        "\n",
        "        try:\n",
        "            # Preparar termos de busca\n",
        "            query = titulo if titulo else texto[:100]\n",
        "\n",
        "            # Consultar API de fact check\n",
        "            data = self.consultar_fact_check(query)\n",
        "\n",
        "            if data:\n",
        "                resultados[\"fact_checks\"] = data.get('claims', [])\n",
        "                resultados[\"num_fact_checks\"] = len(resultados[\"fact_checks\"])\n",
        "\n",
        "                # Analisar resultados\n",
        "                if resultados[\"num_fact_checks\"] > 0:\n",
        "                    # Verificar ratings médios\n",
        "                    ratings = []\n",
        "                    for claim in resultados[\"fact_checks\"]:\n",
        "                        for review in claim.get('claimReview', []):\n",
        "                            if 'textualRating' in review:\n",
        "                                ratings.append(review['textualRating'].lower())\n",
        "\n",
        "                    # Contar frequência de cada rating\n",
        "                    if ratings:\n",
        "                        counter = Counter(ratings)\n",
        "                        resultados[\"ratings_frequencia\"] = counter\n",
        "\n",
        "                        # Detectar termos comuns de falsidade\n",
        "                        termos_falso = ['false', 'fake', 'falso', 'mentira', 'enganoso', 'misleading']\n",
        "                        count_falso = sum(counter.get(termo, 0) for termo in termos_falso)\n",
        "\n",
        "                        if count_falso > 0:\n",
        "                            resultados[\"alerta_fact_check\"] = \"Conteúdo marcado como falso ou enganoso por verificadores de fatos\"\n",
        "                            resultados[\"pontos_fact_check\"] = -25\n",
        "                        else:\n",
        "                            resultados[\"pontos_fact_check\"] = 0\n",
        "            else:\n",
        "                resultados[\"fact_checks\"] = []\n",
        "                resultados[\"num_fact_checks\"] = 0\n",
        "                resultados[\"pontos_fact_check\"] = 0\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro ao buscar fact checks: {e}\")\n",
        "            resultados[\"erro_fact_check\"] = str(e)\n",
        "            resultados[\"pontos_fact_check\"] = 0\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def buscar_noticias_similares(self, titulo, keywords, conteudo=None):\n",
        "        \"\"\"Busca por notícias similares na web usando palavras-chave do artigo.\"\"\"\n",
        "        resultados = {}\n",
        "        try:\n",
        "            # Preparar termos de busca\n",
        "            termos_busca = titulo\n",
        "            if keywords:\n",
        "                termos_busca += \" \" + \" \".join(keywords[:3])  # Limitar a 3 keywords\n",
        "\n",
        "            # Simular resultados para demonstração\n",
        "            # Em uma implementação real, você usaria uma API como Google News ou similar\n",
        "            resultados[\"artigos_similares\"] = 5  # Simulando 5 artigos similares\n",
        "            resultados[\"similaridade\"] = \"média\"  # baixa, média, alta\n",
        "\n",
        "            # Simular consistência entre fontes\n",
        "            resultados[\"consistencia_entre_fontes\"] = \"alta\"  # baixa, média, alta\n",
        "            resultados[\"pontos_consistencia\"] = 10  # Simulando pontos por alta consistência\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro ao buscar notícias similares: {e}\")\n",
        "            resultados[\"erro_busca\"] = str(e)\n",
        "            resultados[\"pontos_consistencia\"] = 0\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def converter_nivel_para_texto(self, nivel):\n",
        "        \"\"\"Converte um nível numérico de veracidade para texto.\"\"\"\n",
        "        nivel_texto = {\n",
        "            1: \"CONFIRMADO\",\n",
        "            2: \"PROVAVELMENTE VERDADEIRO\",\n",
        "            3: \"INCONCLUSIVO\",\n",
        "            4: \"PROVAVELMENTE FALSO\",\n",
        "            5: \"FALSO\"\n",
        "        }.get(nivel, \"INCONCLUSIVO\")\n",
        "        return nivel_texto\n",
        "\n",
        "    def analisar_credibilidade(self, url):\n",
        "        \"\"\"Análise completa de credibilidade de uma URL de notícia.\"\"\"\n",
        "        # 1. Baixar o artigo\n",
        "        artigo = self.baixar_artigo(url)\n",
        "        if not artigo or not artigo.get('conteudo'):\n",
        "            logger.error(f\"Não foi possível baixar o artigo: {url}\")\n",
        "            return {\"erro\": \"Não foi possível baixar o artigo para análise\"}\n",
        "\n",
        "        resultados = {}\n",
        "        conteudo = artigo['conteudo']\n",
        "\n",
        "        # 2. Verificação de fonte/domínio\n",
        "        status_fonte, pontos_fonte, info_fonte = self.verificar_fonte(url)\n",
        "        resultados.update(info_fonte)\n",
        "\n",
        "        # 3. Copiar informações básicas\n",
        "        resultados[\"titulo\"] = artigo['titulo']\n",
        "        resultados[\"data_publicacao\"] = artigo['data']\n",
        "        resultados[\"autores\"] = artigo['autores']\n",
        "        resultados[\"url\"] = artigo['url']\n",
        "\n",
        "        # 4. Análise básica do conteúdo\n",
        "        palavras = len(conteudo.split())\n",
        "        resultados[\"num_palavras\"] = palavras\n",
        "        if palavras < 100:\n",
        "            resultados[\"alerta_tamanho\"] = \"Texto muito curto, pode não ser um artigo completo\"\n",
        "\n",
        "        # 5. Análise detalhada da linguagem\n",
        "        analise_linguagem = self.analisar_linguagem(conteudo)\n",
        "        resultados.update(analise_linguagem)\n",
        "\n",
        "        # 6. Verificação de citações\n",
        "        citacoes = re.findall(r'\"([^\"]+)\"', conteudo)\n",
        "        resultados[\"num_citacoes\"] = len(citacoes)\n",
        "        if len(citacoes) == 0:\n",
        "            resultados[\"alerta_fontes\"] = \"Nenhuma citação direta encontrada\"\n",
        "\n",
        "        # 7. Verificação de URLs externos (referências)\n",
        "        urls = re.findall(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', conteudo)\n",
        "        resultados[\"num_urls\"] = len(urls)\n",
        "\n",
        "        # 8. Análise de imagens\n",
        "        resultados_imagens = self.verificar_imagens(artigo.get('imagens', []))\n",
        "        resultados.update(resultados_imagens)\n",
        "\n",
        "        # 9. Verificação cruzada com outras notícias\n",
        "        resultados_similares = self.buscar_noticias_similares(\n",
        "            artigo['titulo'],\n",
        "            artigo.get('keywords', []),\n",
        "            conteudo\n",
        "        )\n",
        "        resultados.update(resultados_similares)\n",
        "\n",
        "        # 10. Verificação em serviços de fact-checking\n",
        "        resultados_fact_check = self.buscar_fact_check(conteudo, artigo['titulo'])\n",
        "        resultados.update(resultados_fact_check)\n",
        "\n",
        "        # Cálculo mais sofisticado usando vários fatores\n",
        "# 11. Determinar um nível de veracidade\n",
        "# Cálculo mais sofisticado usando vários fatores\n",
        "base_pontuacao = 70  # Base inicial neutra\n",
        "\n",
        "# Fatores que afetam a pontuação\n",
        "pontuacao_fatores = [\n",
        "    resultados.get(\"pontos_fonte\", 0),\n",
        "    resultados.get(\"pontos_consistencia\", 0),\n",
        "    resultados.get(\"pontos_evidencia\", 0),\n",
        "    resultados.get(\"pontos_conhecimento_previo\", 0),\n",
        "    resultados.get(\"pontos_contexto\", 0),\n",
        "    resultados.get(\"pontos_bias\", 0)\n",
        "]\n",
        "\n",
        "# Aplicar pesos aos fatores\n",
        "pesos = [1.5, 1.2, 2.0, 1.0, 1.0, 0.8]\n",
        "pontuacao_ponderada = sum(p * f for p, f in zip(pesos, pontuacao_fatores))\n",
        "\n",
        "# Calcular pontuação final\n",
        "pontuacao_final = max(0, min(100, base_pontuacao + pontuacao_ponderada))\n",
        "\n",
        "# Determinar nível de veracidade baseado na pontuação final\n",
        "if pontuacao_final >= 90:\n",
        "    nivel_veracidade = \"Altamente verificado\"\n",
        "elif pontuacao_final >= 75:\n",
        "    nivel_veracidade = \"Provavelmente verdadeiro\"\n",
        "elif pontuacao_final >= 60:\n",
        "    nivel_veracidade = \"Parcialmente verificado\"\n",
        "elif pontuacao_final >= 40:\n",
        "    nivel_veracidade = \"Inconclusivo\"\n",
        "elif pontuacao_final >= 25:\n",
        "    nivel_veracidade = \"Provavelmente falso\"\n",
        "else:\n",
        "    nivel_veracidade = \"Falso\"\n",
        "\n",
        "resultados[\"pontuacao_veracidade\"] = pontuacao_final\n",
        "resultados[\"nivel_veracidade\"] = nivel_veracidade\n",
        "\n",
        "# 12. Sumarizar os resultados\n",
        "resumo = {\n",
        "    \"nivel_veracidade\": nivel_veracidade,\n",
        "    \"pontuacao\": pontuacao_final,\n",
        "    \"analise_detalhada\": resultados,\n",
        "    \"pontos_chave\": {\n",
        "        \"fontes\": resultados.get(\"qualidade_fonte\", \"Não analisado\"),\n",
        "        \"consistencia\": resultados.get(\"consistencia_narrativa\", \"Não analisado\"),\n",
        "        \"evidencias\": resultados.get(\"qualidade_evidencia\", \"Não analisado\"),\n",
        "        \"contexto\": resultados.get(\"analise_contexto\", \"Não analisado\")\n",
        "    },\n",
        "    \"recomendacao\": \"\"\n",
        "}\n",
        "\n",
        "# Adicionar uma recomendação baseada no nível de veracidade\n",
        "def adicionar_recomendacao(pontuacao_final):\n",
        "    \"\"\"Adiciona uma recomendação ao resumo com base no nível de veracidade.\"\"\"\n",
        "    resumo = {}\n",
        "    if pontuacao_final >= 75:\n",
        "        resumo[\"recomendacao\"] = \"Informação confiável para uso e disseminação.\"\n",
        "    elif pontuacao_final >= 50:\n",
        "        resumo[\"recomendacao\"] = \"Verificar informações adicionais antes de aceitar completamente.\"\n",
        "    else:\n",
        "        resumo[\"recomendacao\"] = \"Não recomendado para uso ou compartilhamento sem verificação adicional.\"\n",
        "\n",
        "    return resumo\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGFQUyHf3X7X",
        "outputId": "ca4f5883-ec10-4ea0-e4fd-89ae7454e911"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting verificacao_simples.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile verificacao_simples.py\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import ssl\n",
        "import urllib3\n",
        "import json\n",
        "from urllib.parse import urlparse, urlencode\n",
        "import hashlib\n",
        "from collections import Counter\n",
        "import time\n",
        "import logging\n",
        "import base64\n",
        "import io\n",
        "import nltk\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from textblob import TextBlob\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "from datetime import datetime\n",
        "\n",
        "# Configurar o logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(\"VerificadorNoticias\")\n",
        "\n",
        "# Desabilitar avisos SSL\n",
        "urllib3.disable_warnings()\n",
        "\n",
        "class VerificadorNoticias:\n",
        "    def __init__(self):\n",
        "        logger.info(\"Verificador de Notícias inicializado!\")\n",
        "\n",
        "        # Carregar variáveis do ambiente\n",
        "        load_dotenv()\n",
        "        self.fact_check_api_key = os.getenv(\"FACT_CHECK_API_KEY\")\n",
        "        self.fact_check_api_url = \"https://factchecktools.googleapis.com/v1alpha1/claims:search\"\n",
        "\n",
        "        # Carregar listas de fontes confiáveis e não confiáveis\n",
        "        self.fontes_confiaveis = [\n",
        "            'g1.globo.com', 'bbc.com', 'bbc.co.uk', 'reuters.com', 'apnews.com',\n",
        "            'nytimes.com', 'washingtonpost.com', 'estadao.com.br', 'folha.uol.com.br',\n",
        "            'cnn.com', 'valor.globo.com', 'info.abril.com.br', 'oglobo.globo.com',\n",
        "            'dw.com', 'npr.org', 'tecmundo.com.br', 'canaltech.com.br', 'olhardigital.com.br'\n",
        "        ]\n",
        "        self.fontes_suspeitas = [\n",
        "            'theonion.com', 'sensacionalista.com.br', 'breitbart.com', 'infowars.com',\n",
        "            'clickhole.com', 'nationalreport.net', 'worldnewsdailyreport.com',\n",
        "            'dailycurrant.com', 'empirenews.net'\n",
        "        ]\n",
        "\n",
        "        # Palavras para análise linguística\n",
        "        self.palavras_sensacionalistas = {\n",
        "            \"chocante\": 2, \"inacreditável\": 2, \"surpreendente\": 1, \"sensacional\": 2,\n",
        "            \"impressionante\": 1, \"alarmante\": 1.5, \"exclusivo\": 1, \"urgente\": 1.5,\n",
        "            \"shocking\": 2, \"unbelievable\": 2, \"amazing\": 1, \"surprising\": 1,\n",
        "            \"imperdível\": 1.5, \"incrível\": 1.5, \"absurdo\": 1.5, \"escandaloso\": 2,\n",
        "            \"bomba\": 1.5, \"inédito\": 1, \"revelado\": 1, \"segredo\": 1.5,\n",
        "            \"polêmico\": 1.5, \"controverso\": 1, \"escândalo\": 1.5, \"viral\": 1.5,\n",
        "            \"bizarro\": 1.5, \"aterrorizante\": 2, \"assustador\": 1.5, \"terrível\": 1.5,\n",
        "            \"estarrecedor\": 2, \"impossível\": 1.5\n",
        "        }\n",
        "\n",
        "        self.palavras_incerteza = [\n",
        "            \"talvez\", \"possivelmente\", \"provavelmente\", \"pode ser\", \"especula-se\",\n",
        "            \"acredita-se\", \"sugere\", \"alega\", \"supostamente\", \"rumores\",\n",
        "            \"maybe\", \"possibly\", \"probably\", \"might be\", \"it is speculated\",\n",
        "            \"it is believed\", \"suggests\", \"allegedly\", \"supposedly\", \"rumors\",\n",
        "            \"parece que\", \"há relatos\", \"não confirmado\", \"segundo fontes\",\n",
        "            \"indícios\", \"sinalizam\", \"aparentemente\", \"estaria\", \"poderia\",\n",
        "            \"teria\", \"estima-se\"\n",
        "        ]\n",
        "\n",
        "        self.expressoes_opinativas = [\n",
        "            \"eu acho\", \"na minha opinião\", \"parece que\", \"acredito que\",\n",
        "            \"i think\", \"in my opinion\", \"it seems that\", \"i believe\",\n",
        "            \"na visão de\", \"do meu ponto de vista\", \"considero\", \"entendo que\",\n",
        "            \"me parece\", \"creio que\", \"penso que\", \"sinto que\"\n",
        "        ]\n",
        "\n",
        "        # Tentar baixar recursos NLTK necessários para análise de texto\n",
        "        try:\n",
        "            nltk.download('punkt', quiet=True)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Não foi possível baixar recursos NLTK: {e}\")\n",
        "\n",
        "    def extrair_dominio(self, url):\n",
        "        \"\"\"Extrai o domínio base da URL.\"\"\"\n",
        "        parsed_url = urlparse(url)\n",
        "        return parsed_url.netloc\n",
        "\n",
        "    def verificar_url_simples(self, url):\n",
        "        \"\"\"Executa uma verificação simples em uma URL de notícia.\"\"\"\n",
        "        try:\n",
        "            # Extrair domínio\n",
        "            parsed_url = urlparse(url)\n",
        "            dominio = parsed_url.netloc\n",
        "\n",
        "            # Tentar acessar a URL\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers, timeout=10, verify=False)\n",
        "            response.raise_for_status()  # Levantar exceção para status HTTP de erro\n",
        "\n",
        "            # Verificar status HTTP\n",
        "            if response.status_code == 200:\n",
        "                logger.info(f\"URL acessada com sucesso: {url}\")\n",
        "                logger.info(f\"Domínio: {dominio}\")\n",
        "                logger.info(f\"Tamanho do conteúdo: {len(response.text)} caracteres.\")\n",
        "                return {\n",
        "                    \"status\": \"sucesso\",\n",
        "                    \"dominio\": dominio,\n",
        "                    \"tamanho_conteudo\": len(response.text),\n",
        "                    \"mensagem\": \"URL acessada com sucesso\"\n",
        "                }\n",
        "            else:\n",
        "                logger.warning(f\"Erro ao acessar a URL. Código HTTP: {response.status_code}\")\n",
        "                return {\n",
        "                    \"status\": \"erro\",\n",
        "                    \"mensagem\": f\"Erro ao acessar a URL. Código HTTP: {response.status_code}\"\n",
        "                }\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            logger.error(f\"Erro ao acessar a URL: {e}\")\n",
        "            return {\n",
        "                \"status\": \"erro\",\n",
        "                \"mensagem\": str(e)\n",
        "            }\n",
        "\n",
        "    def analisar_sentimento(self, texto):\n",
        "        \"\"\"Analisa o sentimento do texto usando TextBlob.\"\"\"\n",
        "        try:\n",
        "            sentiment = TextBlob(texto).sentiment\n",
        "            return {\n",
        "                \"polaridade\": sentiment.polarity,\n",
        "                \"subjetividade\": sentiment.subjectivity\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro na análise de sentimento: {e}\")\n",
        "            return {\n",
        "                \"polaridade\": 0,\n",
        "                \"subjetividade\": 0\n",
        "            }\n",
        "\n",
        "    def verificar_fonte(self, url):\n",
        "        \"\"\"Verifica o domínio da URL contra as listas de fontes.\"\"\"\n",
        "        dominio = self.extrair_dominio(url)\n",
        "        resultado = {\"dominio\": dominio}\n",
        "\n",
        "        # Verificar se o domínio está nas listas\n",
        "        if any(dominio.endswith(fonte) for fonte in self.fontes_confiaveis):\n",
        "            resultado[\"status_fonte\"] = \"confiável\"\n",
        "            resultado[\"pontos_fonte\"] = 20\n",
        "            return \"Fonte confiável\", 20, resultado\n",
        "        elif any(dominio.endswith(fonte) for fonte in self.fontes_suspeitas):\n",
        "            resultado[\"status_fonte\"] = \"suspeita\"\n",
        "            resultado[\"pontos_fonte\"] = -30\n",
        "            resultado[\"alerta_fonte\"] = \"Fonte conhecida por conteúdo satírico ou de baixa credibilidade\"\n",
        "            return \"Fonte suspeita\", -30, resultado\n",
        "        else:\n",
        "            resultado[\"status_fonte\"] = \"não categorizada\"\n",
        "            resultado[\"pontos_fonte\"] = 0\n",
        "            return \"Fonte não categorizada\", 0, resultado\n",
        "\n",
        "    def baixar_artigo(self, url):\n",
        "        \"\"\"Baixa o conteúdo do artigo a partir da URL.\"\"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers, verify=False, timeout=15)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Tenta extrair o título\n",
        "            titulo = soup.title.text.strip() if soup.title else \"Título não encontrado\"\n",
        "\n",
        "            # Tenta extrair o conteúdo do artigo (abordagem mais abrangente)\n",
        "            content = \"\"\n",
        "            # Procura por conteúdo em elementos comuns\n",
        "            for tag in ['article', 'main', '.content', '.post', '.entry', '#content', '.article-body', '.news-text']:\n",
        "                if tag.startswith('.') or tag.startswith('#'):\n",
        "                    elements = soup.select(tag)\n",
        "                else:\n",
        "                    elements = soup.find_all(tag)\n",
        "                if elements:\n",
        "                    for element in elements:\n",
        "                        content += element.get_text(separator=' ', strip=True) + \" \"\n",
        "                    break\n",
        "\n",
        "            # Se não encontrou conteúdo nos elementos específicos, pega todos os parágrafos\n",
        "            if not content:\n",
        "                paragraphs = soup.find_all('p')\n",
        "                content = ' '.join([p.get_text(strip=True) for p in paragraphs])\n",
        "\n",
        "            # Tenta encontrar a data\n",
        "            data = None\n",
        "            # Procura por metadados comuns\n",
        "            for meta in soup.find_all('meta'):\n",
        "                if meta.get('property') in ['article:published_time', 'og:published_time', 'publication_date'] or meta.get('name') in ['publication_date', 'date', 'pubdate']:\n",
        "                    data = meta.get('content')\n",
        "                    break\n",
        "\n",
        "            # Procura por elementos que possam conter autores\n",
        "            autores = []\n",
        "            for author_tag in ['author', 'byline', '.author', '.byline', '[rel=author]', '[name=author]']:\n",
        "                if author_tag.startswith('.') or author_tag.startswith('['):\n",
        "                    elements = soup.select(author_tag)\n",
        "                else:\n",
        "                    elements = soup.find_all(author_tag)\n",
        "\n",
        "                if elements:\n",
        "                    for element in elements:\n",
        "                        autor = element.get_text(strip=True)\n",
        "                        if autor and len(autor) < 100:  # Evitar pegar conteúdo grande por engano\n",
        "                            autores.append(autor)\n",
        "\n",
        "            # Remover duplicações nos autores\n",
        "            autores = list(dict.fromkeys(autores))\n",
        "\n",
        "            # Extrair imagens\n",
        "            imagens = []\n",
        "            for img in soup.find_all('img'):\n",
        "                src = img.get('src')\n",
        "                alt = img.get('alt', '')\n",
        "                if src and not src.startswith('data:'):\n",
        "                    # Garantir URL completa\n",
        "                    if src.startswith('/'):\n",
        "                        parsed_url = urlparse(url)\n",
        "                        base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
        "                        src = base_url + src\n",
        "                    imagens.append({\n",
        "                        'url': src,\n",
        "                        'alt': alt\n",
        "                    })\n",
        "\n",
        "            # Extrair palavras-chave\n",
        "            keywords = []\n",
        "            meta_keywords = soup.find('meta', attrs={'name': 'keywords'})\n",
        "            if meta_keywords:\n",
        "                keywords = meta_keywords.get('content', '').split(',')\n",
        "                keywords = [k.strip() for k in keywords]\n",
        "\n",
        "            return {\n",
        "                'titulo': titulo,\n",
        "                'conteudo': content,\n",
        "                'data': data,\n",
        "                'autores': autores,\n",
        "                'url': response.url,  # Use a URL final após redirecionamentos\n",
        "                'imagens': imagens,\n",
        "                'keywords': keywords,\n",
        "                'html': response.text  # Manter o HTML para possível análise adicional\n",
        "            }\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            logger.error(f\"Erro ao baixar artigo ({url}): {e}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro inesperado ao processar artigo ({url}): {e}\")\n",
        "            return None\n",
        "\n",
        "    def consultar_fact_check(self, query):\n",
        "        \"\"\"Consulta a API do Google Fact Check Tools.\"\"\"\n",
        "        if not self.fact_check_api_key:\n",
        "            logger.warning(\"Chave de API para fact check não configurada\")\n",
        "            return None\n",
        "\n",
        "        url_api = f\"{self.fact_check_api_url}?key={self.fact_check_api_key}&query={query}\"\n",
        "        try:\n",
        "            response = requests.get(url_api)\n",
        "            if response.status_code == 200:\n",
        "                return response.json()\n",
        "            else:\n",
        "                logger.warning(f\"Falha ao acessar API de Fact Check. Status: {response.status_code}\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro na API de Fact Check: {e}\")\n",
        "            return None\n",
        "\n",
        "    def analisar_linguagem(self, texto):\n",
        "        \"\"\"Analisa o conteúdo em busca de sensacionalismo, incerteza e opinatividade.\"\"\"\n",
        "        if not texto or len(texto) < 10:\n",
        "            return {\n",
        "                \"sensacionalismo\": 0,\n",
        "                \"incerteza\": 0,\n",
        "                \"opinativo\": 0,\n",
        "                \"exclamacoes\": 0\n",
        "            }\n",
        "\n",
        "        resultado = {}\n",
        "\n",
        "        # Análise de sensacionalismo\n",
        "        indice_sensacionalismo = sum(\n",
        "            len(re.findall(r'\\b' + re.escape(palavra) + r'\\b', texto.lower())) * peso\n",
        "            for palavra, peso in self.palavras_sensacionalistas.items()\n",
        "        )\n",
        "        resultado[\"sensacionalismo\"] = indice_sensacionalismo\n",
        "\n",
        "        # Normalizar por tamanho do texto (por 1000 palavras)\n",
        "        palavras_totais = len(texto.split())\n",
        "        if palavras_totais > 0:\n",
        "            indice_sensacionalismo_norm = (indice_sensacionalismo * 1000) / palavras_totais\n",
        "            resultado[\"sensacionalismo_normalizado\"] = indice_sensacionalismo_norm\n",
        "\n",
        "            # Adicionar alerta se o índice for alto\n",
        "            if indice_sensacionalismo_norm > 10:\n",
        "                resultado[\"alerta_sensacionalismo\"] = \"Alto índice de linguagem sensacionalista\"\n",
        "            elif indice_sensacionalismo_norm > 5:\n",
        "                resultado[\"alerta_sensacionalismo\"] = \"Moderado índice de linguagem sensacionalista\"\n",
        "\n",
        "        # Análise de incerteza\n",
        "        indice_incerteza = sum(\n",
        "            len(re.findall(r'\\b' + re.escape(palavra) + r'\\b', texto.lower()))\n",
        "            for palavra in self.palavras_incerteza\n",
        "        )\n",
        "        resultado[\"incerteza\"] = indice_incerteza\n",
        "\n",
        "        if palavras_totais > 0:\n",
        "            indice_incerteza_norm = (indice_incerteza * 1000) / palavras_totais\n",
        "            resultado[\"incerteza_normalizada\"] = indice_incerteza_norm\n",
        "\n",
        "            if indice_incerteza_norm > 8:\n",
        "                resultado[\"alerta_incerteza\"] = \"Alto índice de expressões de incerteza\"\n",
        "\n",
        "        # Análise de opinatividade\n",
        "        indice_opiniao = sum(\n",
        "            len(re.findall(r'\\b' + re.escape(expressao) + r'\\b', texto.lower()))\n",
        "            for expressao in self.expressoes_opinativas\n",
        "        )\n",
        "        resultado[\"opinativo\"] = indice_opiniao\n",
        "\n",
        "        if palavras_totais > 0:\n",
        "            indice_opiniao_norm = (indice_opiniao * 1000) / palavras_totais\n",
        "            resultado[\"opinativo_normalizado\"] = indice_opiniao_norm\n",
        "\n",
        "            if indice_opiniao_norm > 5:\n",
        "                resultado[\"alerta_opiniao\"] = \"Alto índice de expressões opinativas\"\n",
        "\n",
        "        # Contagem de pontos de exclamação (indicador de sensacionalismo)\n",
        "        exclamacoes = texto.count('!')\n",
        "        resultado[\"exclamacoes\"] = exclamacoes\n",
        "\n",
        "        if palavras_totais > 0:\n",
        "            exclamacoes_norm = (exclamacoes * 1000) / palavras_totais\n",
        "            if exclamacoes_norm > 10:\n",
        "                resultado[\"alerta_exclamacoes\"] = \"Uso excessivo de pontos de exclamação\"\n",
        "\n",
        "        # Análise de sentimento usando TextBlob\n",
        "        try:\n",
        "            analise = TextBlob(texto)\n",
        "            resultado[\"sentimento_polaridade\"] = analise.sentiment.polarity  # -1 a 1 (negativo a positivo)\n",
        "            resultado[\"sentimento_subjetividade\"] = analise.sentiment.subjectivity  # 0 a 1 (objetivo a subjetivo)\n",
        "\n",
        "            # Categorização baseada na polaridade\n",
        "            if resultado[\"sentimento_polaridade\"] < -0.5:\n",
        "                resultado[\"sentimento_categoria\"] = \"muito negativo\"\n",
        "            elif resultado[\"sentimento_polaridade\"] < -0.1:\n",
        "                resultado[\"sentimento_categoria\"] = \"negativo\"\n",
        "            elif resultado[\"sentimento_polaridade\"] <= 0.1:\n",
        "                resultado[\"sentimento_categoria\"] = \"neutro\"\n",
        "            elif resultado[\"sentimento_polaridade\"] <= 0.5:\n",
        "                resultado[\"sentimento_categoria\"] = \"positivo\"\n",
        "            else:\n",
        "                resultado[\"sentimento_categoria\"] = \"muito positivo\"\n",
        "\n",
        "            # Alerta para alto nível de subjetividade em notícias que deveriam ser objetivas\n",
        "            if resultado[\"sentimento_subjetividade\"] > 0.6:\n",
        "                resultado[\"alerta_subjetividade\"] = \"Conteúdo altamente subjetivo\"\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Erro na análise de sentimento: {e}\")\n",
        "\n",
        "        return resultado\n",
        "\n",
        "    def verificar_imagens(self, imagens):\n",
        "        \"\"\"Analisa as imagens associadas ao artigo.\"\"\"\n",
        "        resultados = {}\n",
        "        if not imagens:\n",
        "            resultados[\"alerta_imagens\"] = \"Nenhuma imagem encontrada no artigo\"\n",
        "            return resultados\n",
        "\n",
        "        resultados[\"num_imagens\"] = len(imagens)\n",
        "\n",
        "        # Verificar descrições das imagens (alt text)\n",
        "        imagens_sem_descricao = sum(1 for img in imagens if not img.get('alt'))\n",
        "        resultados[\"imagens_sem_descricao\"] = imagens_sem_descricao\n",
        "\n",
        "        if imagens_sem_descricao == len(imagens) and len(imagens) > 0:\n",
        "            resultados[\"alerta_imagens_descricao\"] = \"Nenhuma das imagens possui descrição adequada\"\n",
        "\n",
        "        # Calcular hash básico das URLs de imagens para potencial comparação futura\n",
        "        hashes_urls = [hashlib.md5(img['url'].encode()).hexdigest()[:8] for img in imagens]\n",
        "        resultados[\"hashes_imagens\"] = hashes_urls\n",
        "\n",
        "        # Analisar metadados das primeiras 5 imagens (limitar para evitar muito processamento)\n",
        "        try:\n",
        "            with ThreadPoolExecutor(max_workers=3) as executor:\n",
        "                metadados = list(executor.map(self.extrair_metadados_imagem, [img['url'] for img in imagens[:5]]))\n",
        "                resultados[\"metadados_imagens\"] = [m for m in metadados if m]  # Filtrar None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro ao analisar metadados das imagens: {e}\")\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def extrair_metadados_imagem(self, url_imagem):\n",
        "        \"\"\"Extrai metadados de uma imagem a partir da URL.\"\"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            response = requests.get(url_imagem, headers=headers, stream=True, timeout=5, verify=False)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Abrir a imagem com PIL\n",
        "            img = Image.open(io.BytesIO(response.content))\n",
        "\n",
        "            # Coletar metadados básicos\n",
        "            metadados = {\n",
        "                \"formato\": img.format,\n",
        "                \"tamanho\": img.size,\n",
        "                \"modo\": img.mode,\n",
        "                \"url\": url_imagem,\n",
        "                \"hash\": hashlib.md5(response.content).hexdigest()\n",
        "            }\n",
        "\n",
        "            # Tentar extrair EXIF se disponível\n",
        "            if hasattr(img, '_getexif') and img._getexif():\n",
        "                exif = img._getexif()\n",
        "                if exif:\n",
        "                    exif_data = {}\n",
        "                    for tag_id, value in exif.items():\n",
        "                        # Converter para string para garantir serialização JSON\n",
        "                        exif_data[str(tag_id)] = str(value)\n",
        "                    metadados[\"exif\"] = exif_data\n",
        "\n",
        "            return metadados\n",
        "        except (requests.RequestException, UnidentifiedImageError, IOError) as e:\n",
        "            logger.warning(f\"Erro ao extrair metadados da imagem {url_imagem}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def buscar_fact_check(self, texto, titulo=None):\n",
        "        \"\"\"Busca por verificações de fatos relacionadas ao conteúdo.\"\"\"\n",
        "        resultados = {}\n",
        "\n",
        "        try:\n",
        "            # Preparar termos de busca\n",
        "            query = titulo if titulo else texto[:100]\n",
        "\n",
        "            # Consultar API de fact check\n",
        "            data = self.consultar_fact_check(query)\n",
        "\n",
        "            if data:\n",
        "                resultados[\"fact_checks\"] = data.get('claims', [])\n",
        "                resultados[\"num_fact_checks\"] = len(resultados[\"fact_checks\"])\n",
        "\n",
        "                # Analisar resultados\n",
        "                if resultados[\"num_fact_checks\"] > 0:\n",
        "                    # Verificar ratings médios\n",
        "                    ratings = []\n",
        "                    for claim in resultados[\"fact_checks\"]:\n",
        "                        for review in claim.get('claimReview', []):\n",
        "                            if 'textualRating' in review:\n",
        "                                ratings.append(review['textualRating'].lower())\n",
        "\n",
        "                    # Contar frequência de cada rating\n",
        "                    if ratings:\n",
        "                        counter = Counter(ratings)\n",
        "                        resultados[\"ratings_frequencia\"] = counter\n",
        "\n",
        "                        # Detectar termos comuns de falsidade\n",
        "                        termos_falso = ['false', 'fake', 'falso', 'mentira', 'enganoso', 'misleading']\n",
        "                        count_falso = sum(counter.get(termo, 0) for termo in termos_falso)\n",
        "\n",
        "                        if count_falso > 0:\n",
        "                            resultados[\"alerta_fact_check\"] = \"Conteúdo marcado como falso ou enganoso por verificadores de fatos\"\n",
        "                            resultados[\"pontos_fact_check\"] = -25\n",
        "                        else:\n",
        "                            resultados[\"pontos_fact_check\"] = 0\n",
        "            else:\n",
        "                resultados[\"fact_checks\"] = []\n",
        "                resultados[\"num_fact_checks\"] = 0\n",
        "                resultados[\"pontos_fact_check\"] = 0\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro ao buscar fact checks: {e}\")\n",
        "            resultados[\"erro_fact_check\"] = str(e)\n",
        "            resultados[\"pontos_fact_check\"] = 0\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def buscar_noticias_similares(self, titulo, keywords, conteudo=None):\n",
        "        \"\"\"Busca por notícias similares na web usando palavras-chave do artigo.\"\"\"\n",
        "        resultados = {}\n",
        "        try:\n",
        "            # Preparar termos de busca\n",
        "            termos_busca = titulo\n",
        "            if keywords:\n",
        "                termos_busca += \" \" + \" \".join(keywords[:3])  # Limitar a 3 keywords\n",
        "\n",
        "            # Simular resultados para demonstração\n",
        "            # Em uma implementação real, você usaria uma API como Google News ou similar\n",
        "            resultados[\"artigos_similares\"] = 5  # Simulando 5 artigos similares\n",
        "            resultados[\"similaridade\"] = \"média\"  # baixa, média, alta\n",
        "\n",
        "            # Simular consistência entre fontes\n",
        "            resultados[\"consistencia_entre_fontes\"] = \"alta\"  # baixa, média, alta\n",
        "            resultados[\"pontos_consistencia\"] = 10  # Simulando pontos por alta consistência\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro ao buscar notícias similares: {e}\")\n",
        "            resultados[\"erro_busca\"] = str(e)\n",
        "            resultados[\"pontos_consistencia\"] = 0\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def converter_nivel_para_texto(self, nivel):\n",
        "        \"\"\"Converte um nível numérico de veracidade para texto.\"\"\"\n",
        "        nivel_texto = {\n",
        "            1: \"CONFIRMADO\",\n",
        "            2: \"PROVAVELMENTE VERDADEIRO\",\n",
        "            3: \"INCONCLUSIVO\",\n",
        "            4: \"PROVAVELMENTE FALSO\",\n",
        "            5: \"FALSO\"\n",
        "        }.get(nivel, \"INCONCLUSIVO\")\n",
        "        return nivel_texto\n",
        "\n",
        "    def analisar_credibilidade(self, url):\n",
        "        \"\"\"Análise completa de credibilidade de uma URL de notícia.\"\"\"\n",
        "        # 1. Baixar o artigo\n",
        "        artigo = self.baixar_artigo(url)\n",
        "        if not artigo or not artigo.get('conteudo'):\n",
        "            logger.error(f\"Não foi possível baixar o artigo: {url}\")\n",
        "            return {\"erro\": \"Não foi possível baixar o artigo para análise\"}\n",
        "\n",
        "        resultados = {}\n",
        "        conteudo = artigo['conteudo']\n",
        "\n",
        "        # 2. Verificação de fonte/domínio\n",
        "        status_fonte, pontos_fonte, info_fonte = self.verificar_fonte(url)\n",
        "        resultados.update(info_fonte)\n",
        "\n",
        "        # 3. Copiar informações básicas\n",
        "        resultados[\"titulo\"] = artigo['titulo']\n",
        "        resultados[\"data_publicacao\"] = artigo['data']\n",
        "        resultados[\"autores\"] = artigo['autores']\n",
        "        resultados[\"url\"] = artigo['url']\n",
        "\n",
        "        # 4. Análise básica do conteúdo\n",
        "        palavras = len(conteudo.split())\n",
        "        resultados[\"num_palavras\"] = palavras\n",
        "        if palavras < 100:\n",
        "            resultados[\"alerta_tamanho\"] = \"Texto muito curto, pode não ser um artigo completo\"\n",
        "\n",
        "        # 5. Análise detalhada da linguagem\n",
        "        analise_linguagem = self.analisar_linguagem(conteudo)\n",
        "        resultados.update(analise_linguagem)\n",
        "\n",
        "        # 6. Verificação de citações\n",
        "        citacoes = re.findall(r'\"([^\"]+)\"', conteudo)\n",
        "        resultados[\"num_citacoes\"] = len(citacoes)\n",
        "        if len(citacoes) == 0:\n",
        "            resultados[\"alerta_fontes\"] = \"Nenhuma citação direta encontrada\"\n",
        "\n",
        "        # 7. Verificação de URLs externos (referências)\n",
        "        urls = re.findall(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', conteudo)\n",
        "        resultados[\"num_urls\"] = len(urls)\n",
        "\n",
        "        # 8. Análise de imagens\n",
        "        resultados_imagens = self.verificar_imagens(artigo.get('imagens', []))\n",
        "        resultados.update(resultados_imagens)\n",
        "\n",
        "        # 9. Verificação cruzada com outras notícias\n",
        "        resultados_similares = self.buscar_noticias_similares(\n",
        "            artigo['titulo'],\n",
        "            artigo.get('keywords', []),\n",
        "            conteudo\n",
        "        )\n",
        "        resultados.update(resultados_similares)\n",
        "\n",
        "        # 10. Verificação em serviços de fact-checking\n",
        "        resultados_fact_check = self.buscar_fact_check(conteudo, artigo['titulo'])\n",
        "        resultados.update(resultados_fact_check)\n",
        "\n",
        "        # Cálculo mais sofisticado usando vários fatores\n",
        "# 11. Determinar um nível de veracidade\n",
        "# Cálculo mais sofisticado usando vários fatores\n",
        "base_pontuacao = 70  # Base inicial neutra\n",
        "\n",
        "# Fatores que afetam a pontuação\n",
        "pontuacao_fatores = [\n",
        "    resultados.get(\"pontos_fonte\", 0),\n",
        "    resultados.get(\"pontos_consistencia\", 0),\n",
        "    resultados.get(\"pontos_evidencia\", 0),\n",
        "    resultados.get(\"pontos_conhecimento_previo\", 0),\n",
        "    resultados.get(\"pontos_contexto\", 0),\n",
        "    resultados.get(\"pontos_bias\", 0)\n",
        "]\n",
        "\n",
        "# Aplicar pesos aos fatores\n",
        "pesos = [1.5, 1.2, 2.0, 1.0, 1.0, 0.8]\n",
        "pontuacao_ponderada = sum(p * f for p, f in zip(pesos, pontuacao_fatores))\n",
        "\n",
        "# Calcular pontuação final\n",
        "pontuacao_final = max(0, min(100, base_pontuacao + pontuacao_ponderada))\n",
        "\n",
        "# Determinar nível de veracidade baseado na pontuação final\n",
        "if pontuacao_final >= 90:\n",
        "    nivel_veracidade = \"Altamente verificado\"\n",
        "elif pontuacao_final >= 75:\n",
        "    nivel_veracidade = \"Provavelmente verdadeiro\"\n",
        "elif pontuacao_final >= 60:\n",
        "    nivel_veracidade = \"Parcialmente verificado\"\n",
        "elif pontuacao_final >= 40:\n",
        "    nivel_veracidade = \"Inconclusivo\"\n",
        "elif pontuacao_final >= 25:\n",
        "    nivel_veracidade = \"Provavelmente falso\"\n",
        "else:\n",
        "    nivel_veracidade = \"Falso\"\n",
        "\n",
        "resultados[\"pontuacao_veracidade\"] = pontuacao_final\n",
        "resultados[\"nivel_veracidade\"] = nivel_veracidade\n",
        "\n",
        "# 12. Sumarizar os resultados\n",
        "resumo = {\n",
        "    \"nivel_veracidade\": nivel_veracidade,\n",
        "    \"pontuacao\": pontuacao_final,\n",
        "    \"analise_detalhada\": resultados,\n",
        "    \"pontos_chave\": {\n",
        "        \"fontes\": resultados.get(\"qualidade_fonte\", \"Não analisado\"),\n",
        "        \"consistencia\": resultados.get(\"consistencia_narrativa\", \"Não analisado\"),\n",
        "        \"evidencias\": resultados.get(\"qualidade_evidencia\", \"Não analisado\"),\n",
        "        \"contexto\": resultados.get(\"analise_contexto\", \"Não analisado\")\n",
        "    },\n",
        "    \"recomendacao\": \"\"\n",
        "}\n",
        "\n",
        "# Adicionar uma recomendação baseada no nível de veracidade\n",
        "def adicionar_recomendacao(pontuacao_final):\n",
        "    \"\"\"Adiciona uma recomendação ao resumo com base no nível de veracidade.\"\"\"\n",
        "    resumo = {}\n",
        "    if pontuacao_final >= 75:\n",
        "        resumo[\"recomendacao\"] = \"Informação confiável para uso e disseminação.\"\n",
        "    elif pontuacao_final >= 50:\n",
        "        resumo[\"recomendacao\"] = \"Verificar informações adicionais antes de aceitar completamente.\"\n",
        "    else:\n",
        "        resumo[\"recomendacao\"] = \"Não recomendado para uso ou compartilhamento sem verificação adicional.\"\n",
        "\n",
        "    return resumo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eO5uYqWy16VF",
        "outputId": "ff8a9173-6fe4-47fd-ff4a-3ab46f5077d6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting verificacao_simples.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run verificacao_simples.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "nQT3O8AC1Wgn",
        "outputId": "8d87623f-e486-4115-d64b-ea17eefd26f3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "'return' outside function (verificacao_simples.py, line 644)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/content/verificacao_simples.py\"\u001b[0;36m, line \u001b[0;32m644\u001b[0m\n\u001b[0;31m    return resumo\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile verificacao_simples.py\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import ssl\n",
        "import urllib3\n",
        "import json\n",
        "from urllib.parse import urlparse, urlencode\n",
        "import hashlib\n",
        "from collections import Counter\n",
        "import time\n",
        "import logging\n",
        "import base64\n",
        "import io\n",
        "import nltk\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from textblob import TextBlob\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "from datetime import datetime\n",
        "\n",
        "# Configurar o logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(\"VerificadorNoticias\")\n",
        "\n",
        "# Desabilitar avisos SSL\n",
        "urllib3.disable_warnings()\n",
        "\n",
        "class VerificadorNoticias:\n",
        "    def __init__(self):\n",
        "        logger.info(\"Verificador de Notícias inicializado!\")\n",
        "\n",
        "        # Carregar variáveis do ambiente\n",
        "        load_dotenv()\n",
        "        self.fact_check_api_key = os.getenv(\"FACT_CHECK_API_KEY\")\n",
        "        self.fact_check_api_url = \"https://factchecktools.googleapis.com/v1alpha1/claims:search\"\n",
        "\n",
        "        # Carregar listas de fontes confiáveis e não confiáveis\n",
        "        self.fontes_confiaveis = [\n",
        "            'g1.globo.com', 'bbc.com', 'bbc.co.uk', 'reuters.com', 'apnews.com',\n",
        "            'nytimes.com', 'washingtonpost.com', 'estadao.com.br', 'folha.uol.com.br',\n",
        "            'cnn.com', 'valor.globo.com', 'info.abril.com.br', 'oglobo.globo.com',\n",
        "            'dw.com', 'npr.org', 'tecmundo.com.br', 'canaltech.com.br', 'olhardigital.com.br'\n",
        "        ]\n",
        "        self.fontes_suspeitas = [\n",
        "            'theonion.com', 'sensacionalista.com.br', 'breitbart.com', 'infowars.com',\n",
        "            'clickhole.com', 'nationalreport.net', 'worldnewsdailyreport.com',\n",
        "            'dailycurrant.com', 'empirenews.net'\n",
        "        ]\n",
        "\n",
        "        # Palavras para análise linguística\n",
        "        self.palavras_sensacionalistas = {\n",
        "            \"chocante\": 2, \"inacreditável\": 2, \"surpreendente\": 1, \"sensacional\": 2,\n",
        "            \"impressionante\": 1, \"alarmante\": 1.5, \"exclusivo\": 1, \"urgente\": 1.5,\n",
        "            \"shocking\": 2, \"unbelievable\": 2, \"amazing\": 1, \"surprising\": 1,\n",
        "            \"imperdível\": 1.5, \"incrível\": 1.5, \"absurdo\": 1.5, \"escandaloso\": 2,\n",
        "            \"bomba\": 1.5, \"inédito\": 1, \"revelado\": 1, \"segredo\": 1.5,\n",
        "            \"polêmico\": 1.5, \"controverso\": 1, \"escândalo\": 1.5, \"viral\": 1.5,\n",
        "            \"bizarro\": 1.5, \"aterrorizante\": 2, \"assustador\": 1.5, \"terrível\": 1.5,\n",
        "            \"estarrecedor\": 2, \"impossível\": 1.5\n",
        "        }\n",
        "\n",
        "        self.palavras_incerteza = [\n",
        "            \"talvez\", \"possivelmente\", \"provavelmente\", \"pode ser\", \"especula-se\",\n",
        "            \"acredita-se\", \"sugere\", \"alega\", \"supostamente\", \"rumores\",\n",
        "            \"maybe\", \"possibly\", \"probably\", \"might be\", \"it is speculated\",\n",
        "            \"it is believed\", \"suggests\", \"allegedly\", \"supposedly\", \"rumors\",\n",
        "            \"parece que\", \"há relatos\", \"não confirmado\", \"segundo fontes\",\n",
        "            \"indícios\", \"sinalizam\", \"aparentemente\", \"estaria\", \"poderia\",\n",
        "            \"teria\", \"estima-se\"\n",
        "        ]\n",
        "\n",
        "        self.expressoes_opinativas = [\n",
        "            \"eu acho\", \"na minha opinião\", \"parece que\", \"acredito que\",\n",
        "            \"i think\", \"in my opinion\", \"it seems that\", \"i believe\",\n",
        "            \"na visão de\", \"do meu ponto de vista\", \"considero\", \"entendo que\",\n",
        "            \"me parece\", \"creio que\", \"penso que\", \"sinto que\"\n",
        "        ]\n",
        "\n",
        "        # Tentar baixar recursos NLTK necessários para análise de texto\n",
        "        try:\n",
        "            nltk.download('punkt', quiet=True)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Não foi possível baixar recursos NLTK: {e}\")\n",
        "\n",
        "    def extrair_dominio(self, url):\n",
        "        \"\"\"Extrai o domínio base da URL.\"\"\"\n",
        "        parsed_url = urlparse(url)\n",
        "        return parsed_url.netloc\n",
        "\n",
        "    def verificar_url_simples(self, url):\n",
        "        \"\"\"Executa uma verificação simples em uma URL de notícia.\"\"\"\n",
        "        try:\n",
        "            # Extrair domínio\n",
        "            parsed_url = urlparse(url)\n",
        "            dominio = parsed_url.netloc\n",
        "\n",
        "            # Tentar acessar a URL\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers, timeout=10, verify=False)\n",
        "            response.raise_for_status()  # Levantar exceção para status HTTP de erro\n",
        "\n",
        "            # Verificar status HTTP\n",
        "            if response.status_code == 200:\n",
        "                logger.info(f\"URL acessada com sucesso: {url}\")\n",
        "                logger.info(f\"Domínio: {dominio}\")\n",
        "                logger.info(f\"Tamanho do conteúdo: {len(response.text)} caracteres.\")\n",
        "                return {\n",
        "                    \"status\": \"sucesso\",\n",
        "                    \"dominio\": dominio,\n",
        "                    \"tamanho_conteudo\": len(response.text),\n",
        "                    \"mensagem\": \"URL acessada com sucesso\"\n",
        "                }\n",
        "            else:\n",
        "                logger.warning(f\"Erro ao acessar a URL. Código HTTP: {response.status_code}\")\n",
        "                return {\n",
        "                    \"status\": \"erro\",\n",
        "                    \"mensagem\": f\"Erro ao acessar a URL. Código HTTP: {response.status_code}\"\n",
        "                }\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            logger.error(f\"Erro ao acessar a URL: {e}\")\n",
        "            return {\n",
        "                \"status\": \"erro\",\n",
        "                \"mensagem\": str(e)\n",
        "            }\n",
        "\n",
        "    def analisar_sentimento(self, texto):\n",
        "        \"\"\"Analisa o sentimento do texto usando TextBlob.\"\"\"\n",
        "        try:\n",
        "            sentiment = TextBlob(texto).sentiment\n",
        "            return {\n",
        "                \"polaridade\": sentiment.polarity,\n",
        "                \"subjetividade\": sentiment.subjectivity\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro na análise de sentimento: {e}\")\n",
        "            return {\n",
        "                \"polaridade\": 0,\n",
        "                \"subjetividade\": 0\n",
        "            }\n",
        "\n",
        "    def verificar_fonte(self, url):\n",
        "        \"\"\"Verifica o domínio da URL contra as listas de fontes.\"\"\"\n",
        "        dominio = self.extrair_dominio(url)\n",
        "        resultado = {\"dominio\": dominio}\n",
        "\n",
        "        # Verificar se o domínio está nas listas\n",
        "        if any(dominio.endswith(fonte) for fonte in self.fontes_confiaveis):\n",
        "            resultado[\"status_fonte\"] = \"confiável\"\n",
        "            resultado[\"pontos_fonte\"] = 20\n",
        "            return \"Fonte confiável\", 20, resultado\n",
        "        elif any(dominio.endswith(fonte) for fonte in self.fontes_suspeitas):\n",
        "            resultado[\"status_fonte\"] = \"suspeita\"\n",
        "            resultado[\"pontos_fonte\"] = -30\n",
        "            resultado[\"alerta_fonte\"] = \"Fonte conhecida por conteúdo satírico ou de baixa credibilidade\"\n",
        "            return \"Fonte suspeita\", -30, resultado\n",
        "        else:\n",
        "            resultado[\"status_fonte\"] = \"não categorizada\"\n",
        "            resultado[\"pontos_fonte\"] = 0\n",
        "            return \"Fonte não categorizada\", 0, resultado\n",
        "\n",
        "    def baixar_artigo(self, url):\n",
        "        \"\"\"Baixa o conteúdo do artigo a partir da URL.\"\"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers, verify=False, timeout=15)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Tenta extrair o título\n",
        "            titulo = soup.title.text.strip() if soup.title else \"Título não encontrado\"\n",
        "\n",
        "            # Tenta extrair o conteúdo do artigo (abordagem mais abrangente)\n",
        "            content = \"\"\n",
        "            # Procura por conteúdo em elementos comuns\n",
        "            for tag in ['article', 'main', '.content', '.post', '.entry', '#content', '.article-body', '.news-text']:\n",
        "                if tag.startswith('.') or tag.startswith('#'):\n",
        "                    elements = soup.select(tag)\n",
        "                else:\n",
        "                    elements = soup.find_all(tag)\n",
        "                if elements:\n",
        "                    for element in elements:\n",
        "                        content += element.get_text(separator=' ', strip=True) + \" \"\n",
        "                    break\n",
        "\n",
        "            # Se não encontrou conteúdo nos elementos específicos, pega todos os parágrafos\n",
        "            if not content:\n",
        "                paragraphs = soup.find_all('p')\n",
        "                content = ' '.join([p.get_text(strip=True) for p in paragraphs])\n",
        "\n",
        "            # Tenta encontrar a data\n",
        "            data = None\n",
        "            # Procura por metadados comuns\n",
        "            for meta in soup.find_all('meta'):\n",
        "                if meta.get('property') in ['article:published_time', 'og:published_time', 'publication_date'] or meta.get('name') in ['publication_date', 'date', 'pubdate']:\n",
        "                    data = meta.get('content')\n",
        "                    break\n",
        "\n",
        "            # Procura por elementos que possam conter autores\n",
        "            autores = []\n",
        "            for author_tag in ['author', 'byline', '.author', '.byline', '[rel=author]', '[name=author]']:\n",
        "                if author_tag.startswith('.') or author_tag.startswith('['):\n",
        "                    elements = soup.select(author_tag)\n",
        "                else:\n",
        "                    elements = soup.find_all(author_tag)\n",
        "\n",
        "                if elements:\n",
        "                    for element in elements:\n",
        "                        autor = element.get_text(strip=True)\n",
        "                        if autor and len(autor) < 100:  # Evitar pegar conteúdo grande por engano\n",
        "                            autores.append(autor)\n",
        "\n",
        "            # Remover duplicações nos autores\n",
        "            autores = list(dict.fromkeys(autores))\n",
        "\n",
        "            # Extrair imagens\n",
        "            imagens = []\n",
        "            for img in soup.find_all('img'):\n",
        "                src = img.get('src')\n",
        "                alt = img.get('alt', '')\n",
        "                if src and not src.startswith('data:'):\n",
        "                    # Garantir URL completa\n",
        "                    if src.startswith('/'):\n",
        "                        parsed_url = urlparse(url)\n",
        "                        base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
        "                        src = base_url + src\n",
        "                    imagens.append({\n",
        "                        'url': src,\n",
        "                        'alt': alt\n",
        "                    })\n",
        "\n",
        "            # Extrair palavras-chave\n",
        "            keywords = []\n",
        "            meta_keywords = soup.find('meta', attrs={'name': 'keywords'})\n",
        "            if meta_keywords:\n",
        "                keywords = meta_keywords.get('content', '').split(',')\n",
        "                keywords = [k.strip() for k in keywords]\n",
        "\n",
        "            return {\n",
        "                'titulo': titulo,\n",
        "                'conteudo': content,\n",
        "                'data': data,\n",
        "                'autores': autores,\n",
        "                'url': response.url,  # Use a URL final após redirecionamentos\n",
        "                'imagens': imagens,\n",
        "                'keywords': keywords,\n",
        "                'html': response.text  # Manter o HTML para possível análise adicional\n",
        "            }\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            logger.error(f\"Erro ao baixar artigo ({url}): {e}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro inesperado ao processar artigo ({url}): {e}\")\n",
        "            return None\n",
        "\n",
        "    def consultar_fact_check(self, query):\n",
        "        \"\"\"Consulta a API do Google Fact Check Tools.\"\"\"\n",
        "        if not self.fact_check_api_key:\n",
        "            logger.warning(\"Chave de API para fact check não configurada\")\n",
        "            return None\n",
        "\n",
        "        url_api = f\"{self.fact_check_api_url}?key={self.fact_check_api_key}&query={query}\"\n",
        "        try:\n",
        "            response = requests.get(url_api)\n",
        "            if response.status_code == 200:\n",
        "                return response.json()\n",
        "            else:\n",
        "                logger.warning(f\"Falha ao acessar API de Fact Check. Status: {response.status_code}\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro na API de Fact Check: {e}\")\n",
        "            return None\n",
        "\n",
        "    def analisar_linguagem(self, texto):\n",
        "        \"\"\"Analisa o conteúdo em busca de sensacionalismo, incerteza e opinatividade.\"\"\"\n",
        "        if not texto or len(texto) < 10:\n",
        "            return {\n",
        "                \"sensacionalismo\": 0,\n",
        "                \"incerteza\": 0,\n",
        "                \"opinativo\": 0,\n",
        "                \"exclamacoes\": 0\n",
        "            }\n",
        "\n",
        "        resultado = {}\n",
        "\n",
        "        # Análise de sensacionalismo\n",
        "        indice_sensacionalismo = sum(\n",
        "            len(re.findall(r'\\b' + re.escape(palavra) + r'\\b', texto.lower())) * peso\n",
        "            for palavra, peso in self.palavras_sensacionalistas.items()\n",
        "        )\n",
        "        resultado[\"sensacionalismo\"] = indice_sensacionalismo\n",
        "\n",
        "        # Normalizar por tamanho do texto (por 1000 palavras)\n",
        "        palavras_totais = len(texto.split())\n",
        "        if palavras_totais > 0:\n",
        "            indice_sensacionalismo_norm = (indice_sensacionalismo * 1000) / palavras_totais\n",
        "            resultado[\"sensacionalismo_normalizado\"] = indice_sensacionalismo_norm\n",
        "\n",
        "            # Adicionar alerta se o índice for alto\n",
        "            if indice_sensacionalismo_norm > 10:\n",
        "                resultado[\"alerta_sensacionalismo\"] = \"Alto índice de linguagem sensacionalista\"\n",
        "            elif indice_sensacionalismo_norm > 5:\n",
        "                resultado[\"alerta_sensacionalismo\"] = \"Moderado índice de linguagem sensacionalista\"\n",
        "\n",
        "        # Análise de incerteza\n",
        "        indice_incerteza = sum(\n",
        "            len(re.findall(r'\\b' + re.escape(palavra) + r'\\b', texto.lower()))\n",
        "            for palavra in self.palavras_incerteza\n",
        "        )\n",
        "        resultado[\"incerteza\"] = indice_incerteza\n",
        "\n",
        "        if palavras_totais > 0:\n",
        "            indice_incerteza_norm = (indice_incerteza * 1000) / palavras_totais\n",
        "            resultado[\"incerteza_normalizada\"] = indice_incerteza_norm\n",
        "\n",
        "            if indice_incerteza_norm > 8:\n",
        "                resultado[\"alerta_incerteza\"] = \"Alto índice de expressões de incerteza\"\n",
        "\n",
        "        # Análise de opinatividade\n",
        "        indice_opiniao = sum(\n",
        "            len(re.findall(r'\\b' + re.escape(expressao) + r'\\b', texto.lower()))\n",
        "            for expressao in self.expressoes_opinativas\n",
        "        )\n",
        "        resultado[\"opinativo\"] = indice_opiniao\n",
        "\n",
        "        if palavras_totais > 0:\n",
        "            indice_opiniao_norm = (indice_opiniao * 1000) / palavras_totais\n",
        "            resultado[\"opinativo_normalizado\"] = indice_opiniao_norm\n",
        "\n",
        "            if indice_opiniao_norm > 5:\n",
        "                resultado[\"alerta_opiniao\"] = \"Alto índice de expressões opinativas\"\n",
        "\n",
        "        # Contagem de pontos de exclamação (indicador de sensacionalismo)\n",
        "        exclamacoes = texto.count('!')\n",
        "        resultado[\"exclamacoes\"] = exclamacoes\n",
        "\n",
        "        if palavras_totais > 0:\n",
        "            exclamacoes_norm = (exclamacoes * 1000) / palavras_totais\n",
        "            if exclamacoes_norm > 10:\n",
        "                resultado[\"alerta_exclamacoes\"] = \"Uso excessivo de pontos de exclamação\"\n",
        "\n",
        "        # Análise de sentimento usando TextBlob\n",
        "        try:\n",
        "            analise = TextBlob(texto)\n",
        "            resultado[\"sentimento_polaridade\"] = analise.sentiment.polarity  # -1 a 1 (negativo a positivo)\n",
        "            resultado[\"sentimento_subjetividade\"] = analise.sentiment.subjectivity  # 0 a 1 (objetivo a subjetivo)\n",
        "\n",
        "            # Categorização baseada na polaridade\n",
        "            if resultado[\"sentimento_polaridade\"] < -0.5:\n",
        "                resultado[\"sentimento_categoria\"] = \"muito negativo\"\n",
        "            elif resultado[\"sentimento_polaridade\"] < -0.1:\n",
        "                resultado[\"sentimento_categoria\"] = \"negativo\"\n",
        "            elif resultado[\"sentimento_polaridade\"] <= 0.1:\n",
        "                resultado[\"sentimento_categoria\"] = \"neutro\"\n",
        "            elif resultado[\"sentimento_polaridade\"] <= 0.5:\n",
        "                resultado[\"sentimento_categoria\"] = \"positivo\"\n",
        "            else:\n",
        "                resultado[\"sentimento_categoria\"] = \"muito positivo\"\n",
        "\n",
        "            # Alerta para alto nível de subjetividade em notícias que deveriam ser objetivas\n",
        "            if resultado[\"sentimento_subjetividade\"] > 0.6:\n",
        "                resultado[\"alerta_subjetividade\"] = \"Conteúdo altamente subjetivo\"\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Erro na análise de sentimento: {e}\")\n",
        "\n",
        "        return resultado\n",
        "\n",
        "    def verificar_imagens(self, imagens):\n",
        "        \"\"\"Analisa as imagens associadas ao artigo.\"\"\"\n",
        "        resultados = {}\n",
        "        if not imagens:\n",
        "            resultados[\"alerta_imagens\"] = \"Nenhuma imagem encontrada no artigo\"\n",
        "            return resultados\n",
        "\n",
        "        resultados[\"num_imagens\"] = len(imagens)\n",
        "\n",
        "        # Verificar descrições das imagens (alt text)\n",
        "        imagens_sem_descricao = sum(1 for img in imagens if not img.get('alt'))\n",
        "        resultados[\"imagens_sem_descricao\"] = imagens_sem_descricao\n",
        "\n",
        "        if imagens_sem_descricao == len(imagens) and len(imagens) > 0:\n",
        "            resultados[\"alerta_imagens_descricao\"] = \"Nenhuma das imagens possui descrição adequada\"\n",
        "\n",
        "        # Calcular hash básico das URLs de imagens para potencial comparação futura\n",
        "        hashes_urls = [hashlib.md5(img['url'].encode()).hexdigest()[:8] for img in imagens]\n",
        "        resultados[\"hashes_imagens\"] = hashes_urls\n",
        "\n",
        "        # Analisar metadados das primeiras 5 imagens (limitar para evitar muito processamento)\n",
        "        try:\n",
        "            with ThreadPoolExecutor(max_workers=3) as executor:\n",
        "                metadados = list(executor.map(self.extrair_metadados_imagem, [img['url'] for img in imagens[:5]]))\n",
        "                resultados[\"metadados_imagens\"] = [m for m in metadados if m]  # Filtrar None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro ao analisar metadados das imagens: {e}\")\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def extrair_metadados_imagem(self, url_imagem):\n",
        "        \"\"\"Extrai metadados de uma imagem a partir da URL.\"\"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            response = requests.get(url_imagem, headers=headers, stream=True, timeout=5, verify=False)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Abrir a imagem com PIL\n",
        "            img = Image.open(io.BytesIO(response.content))\n",
        "\n",
        "            # Coletar metadados básicos\n",
        "            metadados = {\n",
        "                \"formato\": img.format,\n",
        "                \"tamanho\": img.size,\n",
        "                \"modo\": img.mode,\n",
        "                \"url\": url_imagem,\n",
        "                \"hash\": hashlib.md5(response.content).hexdigest()\n",
        "            }\n",
        "\n",
        "            # Tentar extrair EXIF se disponível\n",
        "            if hasattr(img, '_getexif') and img._getexif():\n",
        "                exif = img._getexif()\n",
        "                if exif:\n",
        "                    exif_data = {}\n",
        "                    for tag_id, value in exif.items():\n",
        "                        # Converter para string para garantir serialização JSON\n",
        "                        exif_data[str(tag_id)] = str(value)\n",
        "                    metadados[\"exif\"] = exif_data\n",
        "\n",
        "            return metadados\n",
        "        except (requests.RequestException, UnidentifiedImageError, IOError) as e:\n",
        "            logger.warning(f\"Erro ao extrair metadados da imagem {url_imagem}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def buscar_fact_check(self, texto, titulo=None):\n",
        "        \"\"\"Busca por verificações de fatos relacionadas ao conteúdo.\"\"\"\n",
        "        resultados = {}\n",
        "\n",
        "        try:\n",
        "            # Preparar termos de busca\n",
        "            query = titulo if titulo else texto[:100]\n",
        "\n",
        "            # Consultar API de fact check\n",
        "            data = self.consultar_fact_check(query)\n",
        "\n",
        "            if data:\n",
        "                resultados[\"fact_checks\"] = data.get('claims', [])\n",
        "                resultados[\"num_fact_checks\"] = len(resultados[\"fact_checks\"])\n",
        "\n",
        "                # Analisar resultados\n",
        "                if resultados[\"num_fact_checks\"] > 0:\n",
        "                    # Verificar ratings médios\n",
        "                    ratings = []\n",
        "                    for claim in resultados[\"fact_checks\"]:\n",
        "                        for review in claim.get('claimReview', []):\n",
        "                            if 'textualRating' in review:\n",
        "                                ratings.append(review['textualRating'].lower())\n",
        "\n",
        "                    # Contar frequência de cada rating\n",
        "                    if ratings:\n",
        "                        counter = Counter(ratings)\n",
        "                        resultados[\"ratings_frequencia\"] = counter\n",
        "\n",
        "                        # Detectar termos comuns de falsidade\n",
        "                        termos_falso = ['false', 'fake', 'falso', 'mentira', 'enganoso', 'misleading']\n",
        "                        count_falso = sum(counter.get(termo, 0) for termo in termos_falso)\n",
        "\n",
        "                        if count_falso > 0:\n",
        "                            resultados[\"alerta_fact_check\"] = \"Conteúdo marcado como falso ou enganoso por verificadores de fatos\"\n",
        "                            resultados[\"pontos_fact_check\"] = -25\n",
        "                        else:\n",
        "                            resultados[\"pontos_fact_check\"] = 0\n",
        "            else:\n",
        "                resultados[\"fact_checks\"] = []\n",
        "                resultados[\"num_fact_checks\"] = 0\n",
        "                resultados[\"pontos_fact_check\"] = 0\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro ao buscar fact checks: {e}\")\n",
        "            resultados[\"erro_fact_check\"] = str(e)\n",
        "            resultados[\"pontos_fact_check\"] = 0\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def buscar_noticias_similares(self, titulo, keywords, conteudo=None):\n",
        "        \"\"\"Busca por notícias similares na web usando palavras-chave do artigo.\"\"\"\n",
        "        resultados = {}\n",
        "        try:\n",
        "            # Preparar termos de busca\n",
        "            termos_busca = titulo\n",
        "            if keywords:\n",
        "                termos_busca += \" \" + \" \".join(keywords[:3])  # Limitar a 3 keywords\n",
        "\n",
        "            # Simular resultados para demonstração\n",
        "            # Em uma implementação real, você usaria uma API como Google News ou similar\n",
        "            resultados[\"artigos_similares\"] = 5  # Simulando 5 artigos similares\n",
        "            resultados[\"similaridade\"] = \"média\"  # baixa, média, alta\n",
        "\n",
        "            # Simular consistência entre fontes\n",
        "            resultados[\"consistencia_entre_fontes\"] = \"alta\"  # baixa, média, alta\n",
        "            resultados[\"pontos_consistencia\"] = 10  # Simulando pontos por alta consistência\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro ao buscar notícias similares: {e}\")\n",
        "            resultados[\"erro_busca\"] = str(e)\n",
        "            resultados[\"pontos_consistencia\"] = 0\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def converter_nivel_para_texto(self, nivel):\n",
        "        \"\"\"Converte um nível numérico de veracidade para texto.\"\"\"\n",
        "        nivel_texto = {\n",
        "            1: \"CONFIRMADO\",\n",
        "            2: \"PROVAVELMENTE VERDADEIRO\",\n",
        "            3: \"INCONCLUSIVO\",\n",
        "            4: \"PROVAVELMENTE FALSO\",\n",
        "            5: \"FALSO\"\n",
        "        }.get(nivel, \"INCONCLUSIVO\")\n",
        "        return nivel_texto\n",
        "\n",
        "    def analisar_credibilidade(self, url):\n",
        "        \"\"\"Análise completa de credibilidade de uma URL de notícia.\"\"\"\n",
        "        # 1. Baixar o artigo\n",
        "        artigo = self.baixar_artigo(url)\n",
        "        if not artigo or not artigo.get('conteudo'):\n",
        "            logger.error(f\"Não foi possível baixar o artigo: {url}\")\n",
        "            return {\"erro\": \"Não foi possível baixar o artigo para análise\"}\n",
        "\n",
        "        resultados = {}\n",
        "        conteudo = artigo['conteudo']\n",
        "\n",
        "        # 2. Verificação de fonte/domínio\n",
        "        status_fonte, pontos_fonte, info_fonte = self.verificar_fonte(url)\n",
        "        resultados.update(info_fonte)\n",
        "\n",
        "        # 3. Copiar informações básicas\n",
        "        resultados[\"titulo\"] = artigo['titulo']\n",
        "        resultados[\"data_publicacao\"] = artigo['data']\n",
        "        resultados[\"autores\"] = artigo['autores']\n",
        "        resultados[\"url\"] = artigo['url']\n",
        "\n",
        "        # 4. Análise básica do conteúdo\n",
        "        palavras = len(conteudo.split())\n",
        "        resultados[\"num_palavras\"] = palavras\n",
        "        if palavras < 100:\n",
        "            resultados[\"alerta_tamanho\"] = \"Texto muito curto, pode não ser um artigo completo\"\n",
        "\n",
        "        # 5. Análise detalhada da linguagem\n",
        "        analise_linguagem = self.analisar_linguagem(conteudo)\n",
        "        resultados.update(analise_linguagem)\n",
        "\n",
        "        # 6. Verificação de citações\n",
        "        citacoes = re.findall(r'\"([^\"]+)\"', conteudo)\n",
        "        resultados[\"num_citacoes\"] = len(citacoes)\n",
        "        if len(citacoes) == 0:\n",
        "            resultados[\"alerta_fontes\"] = \"Nenhuma citação direta encontrada\"\n",
        "\n",
        "        # 7. Verificação de URLs externos (referências)\n",
        "        urls = re.findall(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', conteudo)\n",
        "        resultados[\"num_urls\"] = len(urls)\n",
        "\n",
        "        # 8. Análise de imagens\n",
        "        resultados_imagens = self.verificar_imagens(artigo.get('imagens', []))\n",
        "        resultados.update(resultados_imagens)\n",
        "\n",
        "        # 9. Verificação cruzada com outras notícias\n",
        "        resultados_similares = self.buscar_noticias_similares(\n",
        "            artigo['titulo'],\n",
        "            artigo.get('keywords', []),\n",
        "            conteudo\n",
        "        )\n",
        "        resultados.update(resultados_similares)\n",
        "\n",
        "        # 10. Verificação em serviços de fact-checking\n",
        "        resultados_fact_check = self.buscar_fact_check(conteudo, artigo['titulo'])\n",
        "        resultados.update(resultados_fact_check)\n",
        "\n",
        "        # Cálculo mais sofisticado usando vários fatores\n",
        "# 11. Determinar um nível de veracidade\n",
        "# Cálculo mais sofisticado usando vários fatores\n",
        "base_pontuacao = 70  # Base inicial neutra\n",
        "\n",
        "# Fatores que afetam a pontuação\n",
        "pontuacao_fatores = [\n",
        "    resultados.get(\"pontos_fonte\", 0),\n",
        "    resultados.get(\"pontos_consistencia\", 0),\n",
        "    resultados.get(\"pontos_evidencia\", 0),\n",
        "    resultados.get(\"pontos_conhecimento_previo\", 0),\n",
        "    resultados.get(\"pontos_contexto\", 0),\n",
        "    resultados.get(\"pontos_bias\", 0)\n",
        "]\n",
        "\n",
        "# Aplicar pesos aos fatores\n",
        "pesos = [1.5, 1.2, 2.0, 1.0, 1.0, 0.8]\n",
        "pontuacao_ponderada = sum(p * f for p, f in zip(pesos, pontuacao_fatores))\n",
        "\n",
        "# Calcular pontuação final\n",
        "pontuacao_final = max(0, min(100, base_pontuacao + pontuacao_ponderada))\n",
        "\n",
        "# Determinar nível de veracidade baseado na pontuação final\n",
        "if pontuacao_final >= 90:\n",
        "    nivel_veracidade = \"Altamente verificado\"\n",
        "elif pontuacao_final >= 75:\n",
        "    nivel_veracidade = \"Provavelmente verdadeiro\"\n",
        "elif pontuacao_final >= 60:\n",
        "    nivel_veracidade = \"Parcialmente verificado\"\n",
        "elif pontuacao_final >= 40:\n",
        "    nivel_veracidade = \"Inconclusivo\"\n",
        "elif pontuacao_final >= 25:\n",
        "    nivel_veracidade = \"Provavelmente falso\"\n",
        "else:\n",
        "    nivel_veracidade = \"Falso\"\n",
        "\n",
        "resultados[\"pontuacao_veracidade\"] = pontuacao_final\n",
        "resultados[\"nivel_veracidade\"] = nivel_veracidade\n",
        "\n",
        "# 12. Sumarizar os resultados\n",
        "resumo = {\n",
        "    \"nivel_veracidade\": nivel_veracidade,\n",
        "    \"pontuacao\": pontuacao_final,\n",
        "    \"analise_detalhada\": resultados,\n",
        "    \"pontos_chave\": {\n",
        "        \"fontes\": resultados.get(\"qualidade_fonte\", \"Não analisado\"),\n",
        "        \"consistencia\": resultados.get(\"consistencia_narrativa\", \"Não analisado\"),\n",
        "        \"evidencias\": resultados.get(\"qualidade_evidencia\", \"Não analisado\"),\n",
        "        \"contexto\": resultados.get(\"analise_contexto\", \"Não analisado\")\n",
        "    },\n",
        "    \"recomendacao\": \"\"\n",
        "}\n",
        "\n",
        "# Adicionar uma recomendação baseada no nível de veracidade\n",
        "if pontuacao_final >= 75:\n",
        "    resumo[\"recomendacao\"] = \"Informação confiável para uso e disseminação.\"\n",
        "elif pontuacao_final >= 50:\n",
        "    resumo[\"recomendacao\"] = \"Verificar informações adicionais antes de aceitar completamente.\"\n",
        "else:\n",
        "    resumo[\"recomendacao\"] = \"Não recomendado para uso ou compartilhamento sem verificação adicional.\"\n",
        "    return resumo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1I5oDOq91DRx",
        "outputId": "bd44c32e-b620-4651-8daf-ae01a0847afe"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting verificacao_simples.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile verificacao_simples.py\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import ssl\n",
        "import urllib3\n",
        "import json\n",
        "from urllib.parse import urlparse, urlencode\n",
        "import hashlib\n",
        "from collections import Counter\n",
        "import time\n",
        "import logging\n",
        "import base64\n",
        "import io\n",
        "import nltk\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from textblob import TextBlob\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "from datetime import datetime\n",
        "\n",
        "# Configurar o logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(\"VerificadorNoticias\")\n",
        "\n",
        "# Desabilitar avisos SSL\n",
        "urllib3.disable_warnings()\n",
        "\n",
        "class VerificadorNoticias:\n",
        "    def __init__(self):\n",
        "        logger.info(\"Verificador de Notícias inicializado!\")\n",
        "\n",
        "        # Carregar variáveis do ambiente\n",
        "        load_dotenv()\n",
        "        self.fact_check_api_key = os.getenv(\"FACT_CHECK_API_KEY\")\n",
        "        self.fact_check_api_url = \"https://factchecktools.googleapis.com/v1alpha1/claims:search\"\n",
        "\n",
        "        # Carregar listas de fontes confiáveis e não confiáveis\n",
        "        self.fontes_confiaveis = [\n",
        "            'g1.globo.com', 'bbc.com', 'bbc.co.uk', 'reuters.com', 'apnews.com',\n",
        "            'nytimes.com', 'washingtonpost.com', 'estadao.com.br', 'folha.uol.com.br',\n",
        "            'cnn.com', 'valor.globo.com', 'info.abril.com.br', 'oglobo.globo.com',\n",
        "            'dw.com', 'npr.org', 'tecmundo.com.br', 'canaltech.com.br', 'olhardigital.com.br'\n",
        "        ]\n",
        "        self.fontes_suspeitas = [\n",
        "            'theonion.com', 'sensacionalista.com.br', 'breitbart.com', 'infowars.com',\n",
        "            'clickhole.com', 'nationalreport.net', 'worldnewsdailyreport.com',\n",
        "            'dailycurrant.com', 'empirenews.net'\n",
        "        ]\n",
        "\n",
        "        # Palavras para análise linguística\n",
        "        self.palavras_sensacionalistas = {\n",
        "            \"chocante\": 2, \"inacreditável\": 2, \"surpreendente\": 1, \"sensacional\": 2,\n",
        "            \"impressionante\": 1, \"alarmante\": 1.5, \"exclusivo\": 1, \"urgente\": 1.5,\n",
        "            \"shocking\": 2, \"unbelievable\": 2, \"amazing\": 1, \"surprising\": 1,\n",
        "            \"imperdível\": 1.5, \"incrível\": 1.5, \"absurdo\": 1.5, \"escandaloso\": 2,\n",
        "            \"bomba\": 1.5, \"inédito\": 1, \"revelado\": 1, \"segredo\": 1.5,\n",
        "            \"polêmico\": 1.5, \"controverso\": 1, \"escândalo\": 1.5, \"viral\": 1.5,\n",
        "            \"bizarro\": 1.5, \"aterrorizante\": 2, \"assustador\": 1.5, \"terrível\": 1.5,\n",
        "            \"estarrecedor\": 2, \"impossível\": 1.5\n",
        "        }\n",
        "\n",
        "        self.palavras_incerteza = [\n",
        "            \"talvez\", \"possivelmente\", \"provavelmente\", \"pode ser\", \"especula-se\",\n",
        "            \"acredita-se\", \"sugere\", \"alega\", \"supostamente\", \"rumores\",\n",
        "            \"maybe\", \"possibly\", \"probably\", \"might be\", \"it is speculated\",\n",
        "            \"it is believed\", \"suggests\", \"allegedly\", \"supposedly\", \"rumors\",\n",
        "            \"parece que\", \"há relatos\", \"não confirmado\", \"segundo fontes\",\n",
        "            \"indícios\", \"sinalizam\", \"aparentemente\", \"estaria\", \"poderia\",\n",
        "            \"teria\", \"estima-se\"\n",
        "        ]\n",
        "\n",
        "        self.expressoes_opinativas = [\n",
        "            \"eu acho\", \"na minha opinião\", \"parece que\", \"acredito que\",\n",
        "            \"i think\", \"in my opinion\", \"it seems that\", \"i believe\",\n",
        "            \"na visão de\", \"do meu ponto de vista\", \"considero\", \"entendo que\",\n",
        "            \"me parece\", \"creio que\", \"penso que\", \"sinto que\"\n",
        "        ]\n",
        "\n",
        "        # Tentar baixar recursos NLTK necessários para análise de texto\n",
        "        try:\n",
        "            nltk.download('punkt', quiet=True)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Não foi possível baixar recursos NLTK: {e}\")\n",
        "\n",
        "    def extrair_dominio(self, url):\n",
        "        \"\"\"Extrai o domínio base da URL.\"\"\"\n",
        "        parsed_url = urlparse(url)\n",
        "        return parsed_url.netloc\n",
        "\n",
        "    def verificar_url_simples(self, url):\n",
        "        \"\"\"Executa uma verificação simples em uma URL de notícia.\"\"\"\n",
        "        try:\n",
        "            # Extrair domínio\n",
        "            parsed_url = urlparse(url)\n",
        "            dominio = parsed_url.netloc\n",
        "\n",
        "            # Tentar acessar a URL\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers, timeout=10, verify=False)\n",
        "            response.raise_for_status()  # Levantar exceção para status HTTP de erro\n",
        "\n",
        "            # Verificar status HTTP\n",
        "            if response.status_code == 200:\n",
        "                logger.info(f\"URL acessada com sucesso: {url}\")\n",
        "                logger.info(f\"Domínio: {dominio}\")\n",
        "                logger.info(f\"Tamanho do conteúdo: {len(response.text)} caracteres.\")\n",
        "                return {\n",
        "                    \"status\": \"sucesso\",\n",
        "                    \"dominio\": dominio,\n",
        "                    \"tamanho_conteudo\": len(response.text),\n",
        "                    \"mensagem\": \"URL acessada com sucesso\"\n",
        "                }\n",
        "            else:\n",
        "                logger.warning(f\"Erro ao acessar a URL. Código HTTP: {response.status_code}\")\n",
        "                return {\n",
        "                    \"status\": \"erro\",\n",
        "                    \"mensagem\": f\"Erro ao acessar a URL. Código HTTP: {response.status_code}\"\n",
        "                }\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            logger.error(f\"Erro ao acessar a URL: {e}\")\n",
        "            return {\n",
        "                \"status\": \"erro\",\n",
        "                \"mensagem\": str(e)\n",
        "            }\n",
        "\n",
        "    def analisar_sentimento(self, texto):\n",
        "        \"\"\"Analisa o sentimento do texto usando TextBlob.\"\"\"\n",
        "        try:\n",
        "            sentiment = TextBlob(texto).sentiment\n",
        "            return {\n",
        "                \"polaridade\": sentiment.polarity,\n",
        "                \"subjetividade\": sentiment.subjectivity\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro na análise de sentimento: {e}\")\n",
        "            return {\n",
        "                \"polaridade\": 0,\n",
        "                \"subjetividade\": 0\n",
        "            }\n",
        "\n",
        "    def verificar_fonte(self, url):\n",
        "        \"\"\"Verifica o domínio da URL contra as listas de fontes.\"\"\"\n",
        "        dominio = self.extrair_dominio(url)\n",
        "        resultado = {\"dominio\": dominio}\n",
        "\n",
        "        # Verificar se o domínio está nas listas\n",
        "        if any(dominio.endswith(fonte) for fonte in self.fontes_confiaveis):\n",
        "            resultado[\"status_fonte\"] = \"confiável\"\n",
        "            resultado[\"pontos_fonte\"] = 20\n",
        "            return \"Fonte confiável\", 20, resultado\n",
        "        elif any(dominio.endswith(fonte) for fonte in self.fontes_suspeitas):\n",
        "            resultado[\"status_fonte\"] = \"suspeita\"\n",
        "            resultado[\"pontos_fonte\"] = -30\n",
        "            resultado[\"alerta_fonte\"] = \"Fonte conhecida por conteúdo satírico ou de baixa credibilidade\"\n",
        "            return \"Fonte suspeita\", -30, resultado\n",
        "        else:\n",
        "            resultado[\"status_fonte\"] = \"não categorizada\"\n",
        "            resultado[\"pontos_fonte\"] = 0\n",
        "            return \"Fonte não categorizada\", 0, resultado\n",
        "\n",
        "    def baixar_artigo(self, url):\n",
        "        \"\"\"Baixa o conteúdo do artigo a partir da URL.\"\"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers, verify=False, timeout=15)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Tenta extrair o título\n",
        "            titulo = soup.title.text.strip() if soup.title else \"Título não encontrado\"\n",
        "\n",
        "            # Tenta extrair o conteúdo do artigo (abordagem mais abrangente)\n",
        "            content = \"\"\n",
        "            # Procura por conteúdo em elementos comuns\n",
        "            for tag in ['article', 'main', '.content', '.post', '.entry', '#content', '.article-body', '.news-text']:\n",
        "                if tag.startswith('.') or tag.startswith('#'):\n",
        "                    elements = soup.select(tag)\n",
        "                else:\n",
        "                    elements = soup.find_all(tag)\n",
        "                if elements:\n",
        "                    for element in elements:\n",
        "                        content += element.get_text(separator=' ', strip=True) + \" \"\n",
        "                    break\n",
        "\n",
        "            # Se não encontrou conteúdo nos elementos específicos, pega todos os parágrafos\n",
        "            if not content:\n",
        "                paragraphs = soup.find_all('p')\n",
        "                content = ' '.join([p.get_text(strip=True) for p in paragraphs])\n",
        "\n",
        "            # Tenta encontrar a data\n",
        "            data = None\n",
        "            # Procura por metadados comuns\n",
        "            for meta in soup.find_all('meta'):\n",
        "                if meta.get('property') in ['article:published_time', 'og:published_time', 'publication_date'] or meta.get('name') in ['publication_date', 'date', 'pubdate']:\n",
        "                    data = meta.get('content')\n",
        "                    break\n",
        "\n",
        "            # Procura por elementos que possam conter autores\n",
        "            autores = []\n",
        "            for author_tag in ['author', 'byline', '.author', '.byline', '[rel=author]', '[name=author]']:\n",
        "                if author_tag.startswith('.') or author_tag.startswith('['):\n",
        "                    elements = soup.select(author_tag)\n",
        "                else:\n",
        "                    elements = soup.find_all(author_tag)\n",
        "\n",
        "                if elements:\n",
        "                    for element in elements:\n",
        "                        autor = element.get_text(strip=True)\n",
        "                        if autor and len(autor) < 100:  # Evitar pegar conteúdo grande por engano\n",
        "                            autores.append(autor)\n",
        "\n",
        "            # Remover duplicações nos autores\n",
        "            autores = list(dict.fromkeys(autores))\n",
        "\n",
        "            # Extrair imagens\n",
        "            imagens = []\n",
        "            for img in soup.find_all('img'):\n",
        "                src = img.get('src')\n",
        "                alt = img.get('alt', '')\n",
        "                if src and not src.startswith('data:'):\n",
        "                    # Garantir URL completa\n",
        "                    if src.startswith('/'):\n",
        "                        parsed_url = urlparse(url)\n",
        "                        base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
        "                        src = base_url + src\n",
        "                    imagens.append({\n",
        "                        'url': src,\n",
        "                        'alt': alt\n",
        "                    })\n",
        "\n",
        "            # Extrair palavras-chave\n",
        "            keywords = []\n",
        "            meta_keywords = soup.find('meta', attrs={'name': 'keywords'})\n",
        "            if meta_keywords:\n",
        "                keywords = meta_keywords.get('content', '').split(',')\n",
        "                keywords = [k.strip() for k in keywords]\n",
        "\n",
        "            return {\n",
        "                'titulo': titulo,\n",
        "                'conteudo': content,\n",
        "                'data': data,\n",
        "                'autores': autores,\n",
        "                'url': response.url,  # Use a URL final após redirecionamentos\n",
        "                'imagens': imagens,\n",
        "                'keywords': keywords,\n",
        "                'html': response.text  # Manter o HTML para possível análise adicional\n",
        "            }\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            logger.error(f\"Erro ao baixar artigo ({url}): {e}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro inesperado ao processar artigo ({url}): {e}\")\n",
        "            return None\n",
        "\n",
        "    def consultar_fact_check(self, query):\n",
        "        \"\"\"Consulta a API do Google Fact Check Tools.\"\"\"\n",
        "        if not self.fact_check_api_key:\n",
        "            logger.warning(\"Chave de API para fact check não configurada\")\n",
        "            return None\n",
        "\n",
        "        url_api = f\"{self.fact_check_api_url}?key={self.fact_check_api_key}&query={query}\"\n",
        "        try:\n",
        "            response = requests.get(url_api)\n",
        "            if response.status_code == 200:\n",
        "                return response.json()\n",
        "            else:\n",
        "                logger.warning(f\"Falha ao acessar API de Fact Check. Status: {response.status_code}\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro na API de Fact Check: {e}\")\n",
        "            return None\n",
        "\n",
        "    def analisar_linguagem(self, texto):\n",
        "        \"\"\"Analisa o conteúdo em busca de sensacionalismo, incerteza e opinatividade.\"\"\"\n",
        "        if not texto or len(texto) < 10:\n",
        "            return {\n",
        "                \"sensacionalismo\": 0,\n",
        "                \"incerteza\": 0,\n",
        "                \"opinativo\": 0,\n",
        "                \"exclamacoes\": 0\n",
        "            }\n",
        "\n",
        "        resultado = {}\n",
        "\n",
        "        # Análise de sensacionalismo\n",
        "        indice_sensacionalismo = sum(\n",
        "            len(re.findall(r'\\b' + re.escape(palavra) + r'\\b', texto.lower())) * peso\n",
        "            for palavra, peso in self.palavras_sensacionalistas.items()\n",
        "        )\n",
        "        resultado[\"sensacionalismo\"] = indice_sensacionalismo\n",
        "\n",
        "        # Normalizar por tamanho do texto (por 1000 palavras)\n",
        "        palavras_totais = len(texto.split())\n",
        "        if palavras_totais > 0:\n",
        "            indice_sensacionalismo_norm = (indice_sensacionalismo * 1000) / palavras_totais\n",
        "            resultado[\"sensacionalismo_normalizado\"] = indice_sensacionalismo_norm\n",
        "\n",
        "            # Adicionar alerta se o índice for alto\n",
        "            if indice_sensacionalismo_norm > 10:\n",
        "                resultado[\"alerta_sensacionalismo\"] = \"Alto índice de linguagem sensacionalista\"\n",
        "            elif indice_sensacionalismo_norm > 5:\n",
        "                resultado[\"alerta_sensacionalismo\"] = \"Moderado índice de linguagem sensacionalista\"\n",
        "\n",
        "        # Análise de incerteza\n",
        "        indice_incerteza = sum(\n",
        "            len(re.findall(r'\\b' + re.escape(palavra) + r'\\b', texto.lower()))\n",
        "            for palavra in self.palavras_incerteza\n",
        "        )\n",
        "        resultado[\"incerteza\"] = indice_incerteza\n",
        "\n",
        "        if palavras_totais > 0:\n",
        "            indice_incerteza_norm = (indice_incerteza * 1000) / palavras_totais\n",
        "            resultado[\"incerteza_normalizada\"] = indice_incerteza_norm\n",
        "\n",
        "            if indice_incerteza_norm > 8:\n",
        "                resultado[\"alerta_incerteza\"] = \"Alto índice de expressões de incerteza\"\n",
        "\n",
        "        # Análise de opinatividade\n",
        "        indice_opiniao = sum(\n",
        "            len(re.findall(r'\\b' + re.escape(expressao) + r'\\b', texto.lower()))\n",
        "            for expressao in self.expressoes_opinativas\n",
        "        )\n",
        "        resultado[\"opinativo\"] = indice_opiniao\n",
        "\n",
        "        if palavras_totais > 0:\n",
        "            indice_opiniao_norm = (indice_opiniao * 1000) / palavras_totais\n",
        "            resultado[\"opinativo_normalizado\"] = indice_opiniao_norm\n",
        "\n",
        "            if indice_opiniao_norm > 5:\n",
        "                resultado[\"alerta_opiniao\"] = \"Alto índice de expressões opinativas\"\n",
        "\n",
        "        # Contagem de pontos de exclamação (indicador de sensacionalismo)\n",
        "        exclamacoes = texto.count('!')\n",
        "        resultado[\"exclamacoes\"] = exclamacoes\n",
        "\n",
        "        if palavras_totais > 0:\n",
        "            exclamacoes_norm = (exclamacoes * 1000) / palavras_totais\n",
        "            if exclamacoes_norm > 10:\n",
        "                resultado[\"alerta_exclamacoes\"] = \"Uso excessivo de pontos de exclamação\"\n",
        "\n",
        "        # Análise de sentimento usando TextBlob\n",
        "        try:\n",
        "            analise = TextBlob(texto)\n",
        "            resultado[\"sentimento_polaridade\"] = analise.sentiment.polarity  # -1 a 1 (negativo a positivo)\n",
        "            resultado[\"sentimento_subjetividade\"] = analise.sentiment.subjectivity  # 0 a 1 (objetivo a subjetivo)\n",
        "\n",
        "            # Categorização baseada na polaridade\n",
        "            if resultado[\"sentimento_polaridade\"] < -0.5:\n",
        "                resultado[\"sentimento_categoria\"] = \"muito negativo\"\n",
        "            elif resultado[\"sentimento_polaridade\"] < -0.1:\n",
        "                resultado[\"sentimento_categoria\"] = \"negativo\"\n",
        "            elif resultado[\"sentimento_polaridade\"] <= 0.1:\n",
        "                resultado[\"sentimento_categoria\"] = \"neutro\"\n",
        "            elif resultado[\"sentimento_polaridade\"] <= 0.5:\n",
        "                resultado[\"sentimento_categoria\"] = \"positivo\"\n",
        "            else:\n",
        "                resultado[\"sentimento_categoria\"] = \"muito positivo\"\n",
        "\n",
        "            # Alerta para alto nível de subjetividade em notícias que deveriam ser objetivas\n",
        "            if resultado[\"sentimento_subjetividade\"] > 0.6:\n",
        "                resultado[\"alerta_subjetividade\"] = \"Conteúdo altamente subjetivo\"\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Erro na análise de sentimento: {e}\")\n",
        "\n",
        "        return resultado\n",
        "\n",
        "    def verificar_imagens(self, imagens):\n",
        "        \"\"\"Analisa as imagens associadas ao artigo.\"\"\"\n",
        "        resultados = {}\n",
        "        if not imagens:\n",
        "            resultados[\"alerta_imagens\"] = \"Nenhuma imagem encontrada no artigo\"\n",
        "            return resultados\n",
        "\n",
        "        resultados[\"num_imagens\"] = len(imagens)\n",
        "\n",
        "        # Verificar descrições das imagens (alt text)\n",
        "        imagens_sem_descricao = sum(1 for img in imagens if not img.get('alt'))\n",
        "        resultados[\"imagens_sem_descricao\"] = imagens_sem_descricao\n",
        "\n",
        "        if imagens_sem_descricao == len(imagens) and len(imagens) > 0:\n",
        "            resultados[\"alerta_imagens_descricao\"] = \"Nenhuma das imagens possui descrição adequada\"\n",
        "\n",
        "        # Calcular hash básico das URLs de imagens para potencial comparação futura\n",
        "        hashes_urls = [hashlib.md5(img['url'].encode()).hexdigest()[:8] for img in imagens]\n",
        "        resultados[\"hashes_imagens\"] = hashes_urls\n",
        "\n",
        "        # Analisar metadados das primeiras 5 imagens (limitar para evitar muito processamento)\n",
        "        try:\n",
        "            with ThreadPoolExecutor(max_workers=3) as executor:\n",
        "                metadados = list(executor.map(self.extrair_metadados_imagem, [img['url'] for img in imagens[:5]]))\n",
        "                resultados[\"metadados_imagens\"] = [m for m in metadados if m]  # Filtrar None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro ao analisar metadados das imagens: {e}\")\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def extrair_metadados_imagem(self, url_imagem):\n",
        "        \"\"\"Extrai metadados de uma imagem a partir da URL.\"\"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            response = requests.get(url_imagem, headers=headers, stream=True, timeout=5, verify=False)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Abrir a imagem com PIL\n",
        "            img = Image.open(io.BytesIO(response.content))\n",
        "\n",
        "            # Coletar metadados básicos\n",
        "            metadados = {\n",
        "                \"formato\": img.format,\n",
        "                \"tamanho\": img.size,\n",
        "                \"modo\": img.mode,\n",
        "                \"url\": url_imagem,\n",
        "                \"hash\": hashlib.md5(response.content).hexdigest()\n",
        "            }\n",
        "\n",
        "            # Tentar extrair EXIF se disponível\n",
        "            if hasattr(img, '_getexif') and img._getexif():\n",
        "                exif = img._getexif()\n",
        "                if exif:\n",
        "                    exif_data = {}\n",
        "                    for tag_id, value in exif.items():\n",
        "                        # Converter para string para garantir serialização JSON\n",
        "                        exif_data[str(tag_id)] = str(value)\n",
        "                    metadados[\"exif\"] = exif_data\n",
        "\n",
        "            return metadados\n",
        "        except (requests.RequestException, UnidentifiedImageError, IOError) as e:\n",
        "            logger.warning(f\"Erro ao extrair metadados da imagem {url_imagem}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def buscar_fact_check(self, texto, titulo=None):\n",
        "        \"\"\"Busca por verificações de fatos relacionadas ao conteúdo.\"\"\"\n",
        "        resultados = {}\n",
        "\n",
        "        try:\n",
        "            # Preparar termos de busca\n",
        "            query = titulo if titulo else texto[:100]\n",
        "\n",
        "            # Consultar API de fact check\n",
        "            data = self.consultar_fact_check(query)\n",
        "\n",
        "            if data:\n",
        "                resultados[\"fact_checks\"] = data.get('claims', [])\n",
        "                resultados[\"num_fact_checks\"] = len(resultados[\"fact_checks\"])\n",
        "\n",
        "                # Analisar resultados\n",
        "                if resultados[\"num_fact_checks\"] > 0:\n",
        "                    # Verificar ratings médios\n",
        "                    ratings = []\n",
        "                    for claim in resultados[\"fact_checks\"]:\n",
        "                        for review in claim.get('claimReview', []):\n",
        "                            if 'textualRating' in review:\n",
        "                                ratings.append(review['textualRating'].lower())\n",
        "\n",
        "                    # Contar frequência de cada rating\n",
        "                    if ratings:\n",
        "                        counter = Counter(ratings)\n",
        "                        resultados[\"ratings_frequencia\"] = counter\n",
        "\n",
        "                        # Detectar termos comuns de falsidade\n",
        "                        termos_falso = ['false', 'fake', 'falso', 'mentira', 'enganoso', 'misleading']\n",
        "                        count_falso = sum(counter.get(termo, 0) for termo in termos_falso)\n",
        "\n",
        "                        if count_falso > 0:\n",
        "                            resultados[\"alerta_fact_check\"] = \"Conteúdo marcado como falso ou enganoso por verificadores de fatos\"\n",
        "                            resultados[\"pontos_fact_check\"] = -25\n",
        "                        else:\n",
        "                            resultados[\"pontos_fact_check\"] = 0\n",
        "            else:\n",
        "                resultados[\"fact_checks\"] = []\n",
        "                resultados[\"num_fact_checks\"] = 0\n",
        "                resultados[\"pontos_fact_check\"] = 0\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro ao buscar fact checks: {e}\")\n",
        "            resultados[\"erro_fact_check\"] = str(e)\n",
        "            resultados[\"pontos_fact_check\"] = 0\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def buscar_noticias_similares(self, titulo, keywords, conteudo=None):\n",
        "        \"\"\"Busca por notícias similares na web usando palavras-chave do artigo.\"\"\"\n",
        "        resultados = {}\n",
        "        try:\n",
        "            # Preparar termos de busca\n",
        "            termos_busca = titulo\n",
        "            if keywords:\n",
        "                termos_busca += \" \" + \" \".join(keywords[:3])  # Limitar a 3 keywords\n",
        "\n",
        "            # Simular resultados para demonstração\n",
        "            # Em uma implementação real, você usaria uma API como Google News ou similar\n",
        "            resultados[\"artigos_similares\"] = 5  # Simulando 5 artigos similares\n",
        "            resultados[\"similaridade\"] = \"média\"  # baixa, média, alta\n",
        "\n",
        "            # Simular consistência entre fontes\n",
        "            resultados[\"consistencia_entre_fontes\"] = \"alta\"  # baixa, média, alta\n",
        "            resultados[\"pontos_consistencia\"] = 10  # Simulando pontos por alta consistência\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro ao buscar notícias similares: {e}\")\n",
        "            resultados[\"erro_busca\"] = str(e)\n",
        "            resultados[\"pontos_consistencia\"] = 0\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def converter_nivel_para_texto(self, nivel):\n",
        "        \"\"\"Converte um nível numérico de veracidade para texto.\"\"\"\n",
        "        nivel_texto = {\n",
        "            1: \"CONFIRMADO\",\n",
        "            2: \"PROVAVELMENTE VERDADEIRO\",\n",
        "            3: \"INCONCLUSIVO\",\n",
        "            4: \"PROVAVELMENTE FALSO\",\n",
        "            5: \"FALSO\"\n",
        "        }.get(nivel, \"INCONCLUSIVO\")\n",
        "        return nivel_texto\n",
        "\n",
        "    def analisar_credibilidade(self, url):\n",
        "        \"\"\"Análise completa de credibilidade de uma URL de notícia.\"\"\"\n",
        "        # 1. Baixar o artigo\n",
        "        artigo = self.baixar_artigo(url)\n",
        "        if not artigo or not artigo.get('conteudo'):\n",
        "            logger.error(f\"Não foi possível baixar o artigo: {url}\")\n",
        "            return {\"erro\": \"Não foi possível baixar o artigo para análise\"}\n",
        "\n",
        "        resultados = {}\n",
        "        conteudo = artigo['conteudo']\n",
        "\n",
        "        # 2. Verificação de fonte/domínio\n",
        "        status_fonte, pontos_fonte, info_fonte = self.verificar_fonte(url)\n",
        "        resultados.update(info_fonte)\n",
        "\n",
        "        # 3. Copiar informações básicas\n",
        "        resultados[\"titulo\"] = artigo['titulo']\n",
        "        resultados[\"data_publicacao\"] = artigo['data']\n",
        "        resultados[\"autores\"] = artigo['autores']\n",
        "        resultados[\"url\"] = artigo['url']\n",
        "\n",
        "        # 4. Análise básica do conteúdo\n",
        "        palavras = len(conteudo.split())\n",
        "        resultados[\"num_palavras\"] = palavras\n",
        "        if palavras < 100:\n",
        "            resultados[\"alerta_tamanho\"] = \"Texto muito curto, pode não ser um artigo completo\"\n",
        "\n",
        "        # 5. Análise detalhada da linguagem\n",
        "        analise_linguagem = self.analisar_linguagem(conteudo)\n",
        "        resultados.update(analise_linguagem)\n",
        "\n",
        "        # 6. Verificação de citações\n",
        "        citacoes = re.findall(r'\"([^\"]+)\"', conteudo)\n",
        "        resultados[\"num_citacoes\"] = len(citacoes)\n",
        "        if len(citacoes) == 0:\n",
        "            resultados[\"alerta_fontes\"] = \"Nenhuma citação direta encontrada\"\n",
        "\n",
        "        # 7. Verificação de URLs externos (referências)\n",
        "        urls = re.findall(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', conteudo)\n",
        "        resultados[\"num_urls\"] = len(urls)\n",
        "\n",
        "        # 8. Análise de imagens\n",
        "        resultados_imagens = self.verificar_imagens(artigo.get('imagens', []))\n",
        "        resultados.update(resultados_imagens)\n",
        "\n",
        "        # 9. Verificação cruzada com outras notícias\n",
        "        resultados_similares = self.buscar_noticias_similares(\n",
        "            artigo['titulo'],\n",
        "            artigo.get('keywords', []),\n",
        "            conteudo\n",
        "        )\n",
        "        resultados.update(resultados_similares)\n",
        "\n",
        "        # 10. Verificação em serviços de fact-checking\n",
        "        resultados_fact_check = self.buscar_fact_check(conteudo, artigo['titulo'])\n",
        "        resultados.update(resultados_fact_check)\n",
        "\n",
        "        # Cálculo mais sofisticado usando vários fatores\n",
        "# 11. Determinar um nível de veracidade\n",
        "# Cálculo mais sofisticado usando vários fatores\n",
        "base_pontuacao = 70  # Base inicial neutra\n",
        "\n",
        "# Fatores que afetam a pontuação\n",
        "pontuacao_fatores = [\n",
        "    resultados.get(\"pontos_fonte\", 0),\n",
        "    resultados.get(\"pontos_consistencia\", 0),\n",
        "    resultados.get(\"pontos_evidencia\", 0),\n",
        "    resultados.get(\"pontos_conhecimento_previo\", 0),\n",
        "    resultados.get(\"pontos_contexto\", 0),\n",
        "    resultados.get(\"pontos_bias\", 0)\n",
        "]\n",
        "\n",
        "# Aplicar pesos aos fatores\n",
        "pesos = [1.5, 1.2, 2.0, 1.0, 1.0, 0.8]\n",
        "pontuacao_ponderada = sum(p * f for p, f in zip(pesos, pontuacao_fatores))\n",
        "\n",
        "# Calcular pontuação final\n",
        "pontuacao_final = max(0, min(100, base_pontuacao + pontuacao_ponderada))\n",
        "\n",
        "# Determinar nível de veracidade baseado na pontuação final\n",
        "if pontuacao_final >= 90:\n",
        "    nivel_veracidade = \"Altamente verificado\"\n",
        "elif pontuacao_final >= 75:\n",
        "    nivel_veracidade = \"Provavelmente verdadeiro\"\n",
        "elif pontuacao_final >= 60:\n",
        "    nivel_veracidade = \"Parcialmente verificado\"\n",
        "elif pontuacao_final >= 40:\n",
        "    nivel_veracidade = \"Inconclusivo\"\n",
        "elif pontuacao_final >= 25:\n",
        "    nivel_veracidade = \"Provavelmente falso\"\n",
        "else:\n",
        "    nivel_veracidade = \"Falso\"\n",
        "\n",
        "resultados[\"pontuacao_veracidade\"] = pontuacao_final\n",
        "resultados[\"nivel_veracidade\"] = nivel_veracidade\n",
        "\n",
        "# 12. Sumarizar os resultados\n",
        "resumo = {\n",
        "    \"nivel_veracidade\": nivel_veracidade,\n",
        "    \"pontuacao\": pontuacao_final,\n",
        "    \"analise_detalhada\": resultados,\n",
        "    \"pontos_chave\": {\n",
        "        \"fontes\": resultados.get(\"qualidade_fonte\", \"Não analisado\"),\n",
        "        \"consistencia\": resultados.get(\"consistencia_narrativa\", \"Não analisado\"),\n",
        "        \"evidencias\": resultados.get(\"qualidade_evidencia\", \"Não analisado\"),\n",
        "        \"contexto\": resultados.get(\"analise_contexto\", \"Não analisado\")\n",
        "    },\n",
        "    \"recomendacao\": \"\"\n",
        "}\n",
        "\n",
        "# Adicionar uma recomendação baseada no nível de veracidade\n",
        "if pontuacao_final >= 75:\n",
        "    resumo[\"recomendacao\"] = \"Informação confiável para uso e disseminação.\"\n",
        "elif pontuacao_final >= 50:\n",
        "    resumo[\"recomendacao\"] = \"Verificar informações adicionais antes de aceitar completamente.\"\n",
        "else:\n",
        "    resumo[\"recomendacao\"] = \"Não recomendado para uso ou compartilhamento sem verificação adicional.\"\n",
        "\n",
        "return resumo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YQLnGXI0hRG",
        "outputId": "611cbc36-b500-4f60-86d6-082a7c5ac191"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting verificacao_simples.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile verificacao_simples.py\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import ssl\n",
        "import urllib3\n",
        "import json\n",
        "from urllib.parse import urlparse, urlencode\n",
        "import hashlib\n",
        "from collections import Counter\n",
        "import time\n",
        "import logging\n",
        "import base64\n",
        "import io\n",
        "import nltk\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from textblob import TextBlob\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "from datetime import datetime\n",
        "\n",
        "# Configurar o logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(\"VerificadorNoticias\")\n",
        "\n",
        "# Desabilitar avisos SSL\n",
        "urllib3.disable_warnings()\n",
        "\n",
        "class VerificadorNoticias:\n",
        "    def __init__(self):\n",
        "        logger.info(\"Verificador de Notícias inicializado!\")\n",
        "\n",
        "        # Carregar variáveis do ambiente\n",
        "        load_dotenv()\n",
        "        self.fact_check_api_key = os.getenv(\"FACT_CHECK_API_KEY\")\n",
        "        self.fact_check_api_url = \"https://factchecktools.googleapis.com/v1alpha1/claims:search\"\n",
        "\n",
        "        # Carregar listas de fontes confiáveis e não confiáveis\n",
        "        self.fontes_confiaveis = [\n",
        "            'g1.globo.com', 'bbc.com', 'bbc.co.uk', 'reuters.com', 'apnews.com',\n",
        "            'nytimes.com', 'washingtonpost.com', 'estadao.com.br', 'folha.uol.com.br',\n",
        "            'cnn.com', 'valor.globo.com', 'info.abril.com.br', 'oglobo.globo.com',\n",
        "            'dw.com', 'npr.org', 'tecmundo.com.br', 'canaltech.com.br', 'olhardigital.com.br'\n",
        "        ]\n",
        "        self.fontes_suspeitas = [\n",
        "            'theonion.com', 'sensacionalista.com.br', 'breitbart.com', 'infowars.com',\n",
        "            'clickhole.com', 'nationalreport.net', 'worldnewsdailyreport.com',\n",
        "            'dailycurrant.com', 'empirenews.net'\n",
        "        ]\n",
        "\n",
        "        # Palavras para análise linguística\n",
        "        self.palavras_sensacionalistas = {\n",
        "            \"chocante\": 2, \"inacreditável\": 2, \"surpreendente\": 1, \"sensacional\": 2,\n",
        "            \"impressionante\": 1, \"alarmante\": 1.5, \"exclusivo\": 1, \"urgente\": 1.5,\n",
        "            \"shocking\": 2, \"unbelievable\": 2, \"amazing\": 1, \"surprising\": 1,\n",
        "            \"imperdível\": 1.5, \"incrível\": 1.5, \"absurdo\": 1.5, \"escandaloso\": 2,\n",
        "            \"bomba\": 1.5, \"inédito\": 1, \"revelado\": 1, \"segredo\": 1.5,\n",
        "            \"polêmico\": 1.5, \"controverso\": 1, \"escândalo\": 1.5, \"viral\": 1.5,\n",
        "            \"bizarro\": 1.5, \"aterrorizante\": 2, \"assustador\": 1.5, \"terrível\": 1.5,\n",
        "            \"estarrecedor\": 2, \"impossível\": 1.5\n",
        "        }\n",
        "\n",
        "        self.palavras_incerteza = [\n",
        "            \"talvez\", \"possivelmente\", \"provavelmente\", \"pode ser\", \"especula-se\",\n",
        "            \"acredita-se\", \"sugere\", \"alega\", \"supostamente\", \"rumores\",\n",
        "            \"maybe\", \"possibly\", \"probably\", \"might be\", \"it is speculated\",\n",
        "            \"it is believed\", \"suggests\", \"allegedly\", \"supposedly\", \"rumors\",\n",
        "            \"parece que\", \"há relatos\", \"não confirmado\", \"segundo fontes\",\n",
        "            \"indícios\", \"sinalizam\", \"aparentemente\", \"estaria\", \"poderia\",\n",
        "            \"teria\", \"estima-se\"\n",
        "        ]\n",
        "\n",
        "        self.expressoes_opinativas = [\n",
        "            \"eu acho\", \"na minha opinião\", \"parece que\", \"acredito que\",\n",
        "            \"i think\", \"in my opinion\", \"it seems that\", \"i believe\",\n",
        "            \"na visão de\", \"do meu ponto de vista\", \"considero\", \"entendo que\",\n",
        "            \"me parece\", \"creio que\", \"penso que\", \"sinto que\"\n",
        "        ]\n",
        "\n",
        "        # Tentar baixar recursos NLTK necessários para análise de texto\n",
        "        try:\n",
        "            nltk.download('punkt', quiet=True)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Não foi possível baixar recursos NLTK: {e}\")\n",
        "\n",
        "    def extrair_dominio(self, url):\n",
        "        \"\"\"Extrai o domínio base da URL.\"\"\"\n",
        "        parsed_url = urlparse(url)\n",
        "        return parsed_url.netloc\n",
        "\n",
        "    def verificar_url_simples(self, url):\n",
        "        \"\"\"Executa uma verificação simples em uma URL de notícia.\"\"\"\n",
        "        try:\n",
        "            # Extrair domínio\n",
        "            parsed_url = urlparse(url)\n",
        "            dominio = parsed_url.netloc\n",
        "\n",
        "            # Tentar acessar a URL\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers, timeout=10, verify=False)\n",
        "            response.raise_for_status()  # Levantar exceção para status HTTP de erro\n",
        "\n",
        "            # Verificar status HTTP\n",
        "            if response.status_code == 200:\n",
        "                logger.info(f\"URL acessada com sucesso: {url}\")\n",
        "                logger.info(f\"Domínio: {dominio}\")\n",
        "                logger.info(f\"Tamanho do conteúdo: {len(response.text)} caracteres.\")\n",
        "                return {\n",
        "                    \"status\": \"sucesso\",\n",
        "                    \"dominio\": dominio,\n",
        "                    \"tamanho_conteudo\": len(response.text),\n",
        "                    \"mensagem\": \"URL acessada com sucesso\"\n",
        "                }\n",
        "            else:\n",
        "                logger.warning(f\"Erro ao acessar a URL. Código HTTP: {response.status_code}\")\n",
        "                return {\n",
        "                    \"status\": \"erro\",\n",
        "                    \"mensagem\": f\"Erro ao acessar a URL. Código HTTP: {response.status_code}\"\n",
        "                }\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            logger.error(f\"Erro ao acessar a URL: {e}\")\n",
        "            return {\n",
        "                \"status\": \"erro\",\n",
        "                \"mensagem\": str(e)\n",
        "            }\n",
        "\n",
        "    def analisar_sentimento(self, texto):\n",
        "        \"\"\"Analisa o sentimento do texto usando TextBlob.\"\"\"\n",
        "        try:\n",
        "            sentiment = TextBlob(texto).sentiment\n",
        "            return {\n",
        "                \"polaridade\": sentiment.polarity,\n",
        "                \"subjetividade\": sentiment.subjectivity\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro na análise de sentimento: {e}\")\n",
        "            return {\n",
        "                \"polaridade\": 0,\n",
        "                \"subjetividade\": 0\n",
        "            }\n",
        "\n",
        "    def verificar_fonte(self, url):\n",
        "        \"\"\"Verifica o domínio da URL contra as listas de fontes.\"\"\"\n",
        "        dominio = self.extrair_dominio(url)\n",
        "        resultado = {\"dominio\": dominio}\n",
        "\n",
        "        # Verificar se o domínio está nas listas\n",
        "        if any(dominio.endswith(fonte) for fonte in self.fontes_confiaveis):\n",
        "            resultado[\"status_fonte\"] = \"confiável\"\n",
        "            resultado[\"pontos_fonte\"] = 20\n",
        "            return \"Fonte confiável\", 20, resultado\n",
        "        elif any(dominio.endswith(fonte) for fonte in self.fontes_suspeitas):\n",
        "            resultado[\"status_fonte\"] = \"suspeita\"\n",
        "            resultado[\"pontos_fonte\"] = -30\n",
        "            resultado[\"alerta_fonte\"] = \"Fonte conhecida por conteúdo satírico ou de baixa credibilidade\"\n",
        "            return \"Fonte suspeita\", -30, resultado\n",
        "        else:\n",
        "            resultado[\"status_fonte\"] = \"não categorizada\"\n",
        "            resultado[\"pontos_fonte\"] = 0\n",
        "            return \"Fonte não categorizada\", 0, resultado\n",
        "\n",
        "    def baixar_artigo(self, url):\n",
        "        \"\"\"Baixa o conteúdo do artigo a partir da URL.\"\"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers, verify=False, timeout=15)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Tenta extrair o título\n",
        "            titulo = soup.title.text.strip() if soup.title else \"Título não encontrado\"\n",
        "\n",
        "            # Tenta extrair o conteúdo do artigo (abordagem mais abrangente)\n",
        "            content = \"\"\n",
        "            # Procura por conteúdo em elementos comuns\n",
        "            for tag in ['article', 'main', '.content', '.post', '.entry', '#content', '.article-body', '.news-text']:\n",
        "                if tag.startswith('.') or tag.startswith('#'):\n",
        "                    elements = soup.select(tag)\n",
        "                else:\n",
        "                    elements = soup.find_all(tag)\n",
        "                if elements:\n",
        "                    for element in elements:\n",
        "                        content += element.get_text(separator=' ', strip=True) + \" \"\n",
        "                    break\n",
        "\n",
        "            # Se não encontrou conteúdo nos elementos específicos, pega todos os parágrafos\n",
        "            if not content:\n",
        "                paragraphs = soup.find_all('p')\n",
        "                content = ' '.join([p.get_text(strip=True) for p in paragraphs])\n",
        "\n",
        "            # Tenta encontrar a data\n",
        "            data = None\n",
        "            # Procura por metadados comuns\n",
        "            for meta in soup.find_all('meta'):\n",
        "                if meta.get('property') in ['article:published_time', 'og:published_time', 'publication_date'] or meta.get('name') in ['publication_date', 'date', 'pubdate']:\n",
        "                    data = meta.get('content')\n",
        "                    break\n",
        "\n",
        "            # Procura por elementos que possam conter autores\n",
        "            autores = []\n",
        "            for author_tag in ['author', 'byline', '.author', '.byline', '[rel=author]', '[name=author]']:\n",
        "                if author_tag.startswith('.') or author_tag.startswith('['):\n",
        "                    elements = soup.select(author_tag)\n",
        "                else:\n",
        "                    elements = soup.find_all(author_tag)\n",
        "\n",
        "                if elements:\n",
        "                    for element in elements:\n",
        "                        autor = element.get_text(strip=True)\n",
        "                        if autor and len(autor) < 100:  # Evitar pegar conteúdo grande por engano\n",
        "                            autores.append(autor)\n",
        "\n",
        "            # Remover duplicações nos autores\n",
        "            autores = list(dict.fromkeys(autores))\n",
        "\n",
        "            # Extrair imagens\n",
        "            imagens = []\n",
        "            for img in soup.find_all('img'):\n",
        "                src = img.get('src')\n",
        "                alt = img.get('alt', '')\n",
        "                if src and not src.startswith('data:'):\n",
        "                    # Garantir URL completa\n",
        "                    if src.startswith('/'):\n",
        "                        parsed_url = urlparse(url)\n",
        "                        base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
        "                        src = base_url + src\n",
        "                    imagens.append({\n",
        "                        'url': src,\n",
        "                        'alt': alt\n",
        "                    })\n",
        "\n",
        "            # Extrair palavras-chave\n",
        "            keywords = []\n",
        "            meta_keywords = soup.find('meta', attrs={'name': 'keywords'})\n",
        "            if meta_keywords:\n",
        "                keywords = meta_keywords.get('content', '').split(',')\n",
        "                keywords = [k.strip() for k in keywords]\n",
        "\n",
        "            return {\n",
        "                'titulo': titulo,\n",
        "                'conteudo': content,\n",
        "                'data': data,\n",
        "                'autores': autores,\n",
        "                'url': response.url,  # Use a URL final após redirecionamentos\n",
        "                'imagens': imagens,\n",
        "                'keywords': keywords,\n",
        "                'html': response.text  # Manter o HTML para possível análise adicional\n",
        "            }\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            logger.error(f\"Erro ao baixar artigo ({url}): {e}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro inesperado ao processar artigo ({url}): {e}\")\n",
        "            return None\n",
        "\n",
        "    def consultar_fact_check(self, query):\n",
        "        \"\"\"Consulta a API do Google Fact Check Tools.\"\"\"\n",
        "        if not self.fact_check_api_key:\n",
        "            logger.warning(\"Chave de API para fact check não configurada\")\n",
        "            return None\n",
        "\n",
        "        url_api = f\"{self.fact_check_api_url}?key={self.fact_check_api_key}&query={query}\"\n",
        "        try:\n",
        "            response = requests.get(url_api)\n",
        "            if response.status_code == 200:\n",
        "                return response.json()\n",
        "            else:\n",
        "                logger.warning(f\"Falha ao acessar API de Fact Check. Status: {response.status_code}\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro na API de Fact Check: {e}\")\n",
        "            return None\n",
        "\n",
        "    def analisar_linguagem(self, texto):\n",
        "        \"\"\"Analisa o conteúdo em busca de sensacionalismo, incerteza e opinatividade.\"\"\"\n",
        "        if not texto or len(texto) < 10:\n",
        "            return {\n",
        "                \"sensacionalismo\": 0,\n",
        "                \"incerteza\": 0,\n",
        "                \"opinativo\": 0,\n",
        "                \"exclamacoes\": 0\n",
        "            }\n",
        "\n",
        "        resultado = {}\n",
        "\n",
        "        # Análise de sensacionalismo\n",
        "        indice_sensacionalismo = sum(\n",
        "            len(re.findall(r'\\b' + re.escape(palavra) + r'\\b', texto.lower())) * peso\n",
        "            for palavra, peso in self.palavras_sensacionalistas.items()\n",
        "        )\n",
        "        resultado[\"sensacionalismo\"] = indice_sensacionalismo\n",
        "\n",
        "        # Normalizar por tamanho do texto (por 1000 palavras)\n",
        "        palavras_totais = len(texto.split())\n",
        "        if palavras_totais > 0:\n",
        "            indice_sensacionalismo_norm = (indice_sensacionalismo * 1000) / palavras_totais\n",
        "            resultado[\"sensacionalismo_normalizado\"] = indice_sensacionalismo_norm\n",
        "\n",
        "            # Adicionar alerta se o índice for alto\n",
        "            if indice_sensacionalismo_norm > 10:\n",
        "                resultado[\"alerta_sensacionalismo\"] = \"Alto índice de linguagem sensacionalista\"\n",
        "            elif indice_sensacionalismo_norm > 5:\n",
        "                resultado[\"alerta_sensacionalismo\"] = \"Moderado índice de linguagem sensacionalista\"\n",
        "\n",
        "        # Análise de incerteza\n",
        "        indice_incerteza = sum(\n",
        "            len(re.findall(r'\\b' + re.escape(palavra) + r'\\b', texto.lower()))\n",
        "            for palavra in self.palavras_incerteza\n",
        "        )\n",
        "        resultado[\"incerteza\"] = indice_incerteza\n",
        "\n",
        "        if palavras_totais > 0:\n",
        "            indice_incerteza_norm = (indice_incerteza * 1000) / palavras_totais\n",
        "            resultado[\"incerteza_normalizada\"] = indice_incerteza_norm\n",
        "\n",
        "            if indice_incerteza_norm > 8:\n",
        "                resultado[\"alerta_incerteza\"] = \"Alto índice de expressões de incerteza\"\n",
        "\n",
        "        # Análise de opinatividade\n",
        "        indice_opiniao = sum(\n",
        "            len(re.findall(r'\\b' + re.escape(expressao) + r'\\b', texto.lower()))\n",
        "            for expressao in self.expressoes_opinativas\n",
        "        )\n",
        "        resultado[\"opinativo\"] = indice_opiniao\n",
        "\n",
        "        if palavras_totais > 0:\n",
        "            indice_opiniao_norm = (indice_opiniao * 1000) / palavras_totais\n",
        "            resultado[\"opinativo_normalizado\"] = indice_opiniao_norm\n",
        "\n",
        "            if indice_opiniao_norm > 5:\n",
        "                resultado[\"alerta_opiniao\"] = \"Alto índice de expressões opinativas\"\n",
        "\n",
        "        # Contagem de pontos de exclamação (indicador de sensacionalismo)\n",
        "        exclamacoes = texto.count('!')\n",
        "        resultado[\"exclamacoes\"] = exclamacoes\n",
        "\n",
        "        if palavras_totais > 0:\n",
        "            exclamacoes_norm = (exclamacoes * 1000) / palavras_totais\n",
        "            if exclamacoes_norm > 10:\n",
        "                resultado[\"alerta_exclamacoes\"] = \"Uso excessivo de pontos de exclamação\"\n",
        "\n",
        "        # Análise de sentimento usando TextBlob\n",
        "        try:\n",
        "            analise = TextBlob(texto)\n",
        "            resultado[\"sentimento_polaridade\"] = analise.sentiment.polarity  # -1 a 1 (negativo a positivo)\n",
        "            resultado[\"sentimento_subjetividade\"] = analise.sentiment.subjectivity  # 0 a 1 (objetivo a subjetivo)\n",
        "\n",
        "            # Categorização baseada na polaridade\n",
        "            if resultado[\"sentimento_polaridade\"] < -0.5:\n",
        "                resultado[\"sentimento_categoria\"] = \"muito negativo\"\n",
        "            elif resultado[\"sentimento_polaridade\"] < -0.1:\n",
        "                resultado[\"sentimento_categoria\"] = \"negativo\"\n",
        "            elif resultado[\"sentimento_polaridade\"] <= 0.1:\n",
        "                resultado[\"sentimento_categoria\"] = \"neutro\"\n",
        "            elif resultado[\"sentimento_polaridade\"] <= 0.5:\n",
        "                resultado[\"sentimento_categoria\"] = \"positivo\"\n",
        "            else:\n",
        "                resultado[\"sentimento_categoria\"] = \"muito positivo\"\n",
        "\n",
        "            # Alerta para alto nível de subjetividade em notícias que deveriam ser objetivas\n",
        "            if resultado[\"sentimento_subjetividade\"] > 0.6:\n",
        "                resultado[\"alerta_subjetividade\"] = \"Conteúdo altamente subjetivo\"\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Erro na análise de sentimento: {e}\")\n",
        "\n",
        "        return resultado\n",
        "\n",
        "    def verificar_imagens(self, imagens):\n",
        "        \"\"\"Analisa as imagens associadas ao artigo.\"\"\"\n",
        "        resultados = {}\n",
        "        if not imagens:\n",
        "            resultados[\"alerta_imagens\"] = \"Nenhuma imagem encontrada no artigo\"\n",
        "            return resultados\n",
        "\n",
        "        resultados[\"num_imagens\"] = len(imagens)\n",
        "\n",
        "        # Verificar descrições das imagens (alt text)\n",
        "        imagens_sem_descricao = sum(1 for img in imagens if not img.get('alt'))\n",
        "        resultados[\"imagens_sem_descricao\"] = imagens_sem_descricao\n",
        "\n",
        "        if imagens_sem_descricao == len(imagens) and len(imagens) > 0:\n",
        "            resultados[\"alerta_imagens_descricao\"] = \"Nenhuma das imagens possui descrição adequada\"\n",
        "\n",
        "        # Calcular hash básico das URLs de imagens para potencial comparação futura\n",
        "        hashes_urls = [hashlib.md5(img['url'].encode()).hexdigest()[:8] for img in imagens]\n",
        "        resultados[\"hashes_imagens\"] = hashes_urls\n",
        "\n",
        "        # Analisar metadados das primeiras 5 imagens (limitar para evitar muito processamento)\n",
        "        try:\n",
        "            with ThreadPoolExecutor(max_workers=3) as executor:\n",
        "                metadados = list(executor.map(self.extrair_metadados_imagem, [img['url'] for img in imagens[:5]]))\n",
        "                resultados[\"metadados_imagens\"] = [m for m in metadados if m]  # Filtrar None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro ao analisar metadados das imagens: {e}\")\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def extrair_metadados_imagem(self, url_imagem):\n",
        "        \"\"\"Extrai metadados de uma imagem a partir da URL.\"\"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            response = requests.get(url_imagem, headers=headers, stream=True, timeout=5, verify=False)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Abrir a imagem com PIL\n",
        "            img = Image.open(io.BytesIO(response.content))\n",
        "\n",
        "            # Coletar metadados básicos\n",
        "            metadados = {\n",
        "                \"formato\": img.format,\n",
        "                \"tamanho\": img.size,\n",
        "                \"modo\": img.mode,\n",
        "                \"url\": url_imagem,\n",
        "                \"hash\": hashlib.md5(response.content).hexdigest()\n",
        "            }\n",
        "\n",
        "            # Tentar extrair EXIF se disponível\n",
        "            if hasattr(img, '_getexif') and img._getexif():\n",
        "                exif = img._getexif()\n",
        "                if exif:\n",
        "                    exif_data = {}\n",
        "                    for tag_id, value in exif.items():\n",
        "                        # Converter para string para garantir serialização JSON\n",
        "                        exif_data[str(tag_id)] = str(value)\n",
        "                    metadados[\"exif\"] = exif_data\n",
        "\n",
        "            return metadados\n",
        "        except (requests.RequestException, UnidentifiedImageError, IOError) as e:\n",
        "            logger.warning(f\"Erro ao extrair metadados da imagem {url_imagem}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def buscar_fact_check(self, texto, titulo=None):\n",
        "        \"\"\"Busca por verificações de fatos relacionadas ao conteúdo.\"\"\"\n",
        "        resultados = {}\n",
        "\n",
        "        try:\n",
        "            # Preparar termos de busca\n",
        "            query = titulo if titulo else texto[:100]\n",
        "\n",
        "            # Consultar API de fact check\n",
        "            data = self.consultar_fact_check(query)\n",
        "\n",
        "            if data:\n",
        "                resultados[\"fact_checks\"] = data.get('claims', [])\n",
        "                resultados[\"num_fact_checks\"] = len(resultados[\"fact_checks\"])\n",
        "\n",
        "                # Analisar resultados\n",
        "                if resultados[\"num_fact_checks\"] > 0:\n",
        "                    # Verificar ratings médios\n",
        "                    ratings = []\n",
        "                    for claim in resultados[\"fact_checks\"]:\n",
        "                        for review in claim.get('claimReview', []):\n",
        "                            if 'textualRating' in review:\n",
        "                                ratings.append(review['textualRating'].lower())\n",
        "\n",
        "                    # Contar frequência de cada rating\n",
        "                    if ratings:\n",
        "                        counter = Counter(ratings)\n",
        "                        resultados[\"ratings_frequencia\"] = counter\n",
        "\n",
        "                        # Detectar termos comuns de falsidade\n",
        "                        termos_falso = ['false', 'fake', 'falso', 'mentira', 'enganoso', 'misleading']\n",
        "                        count_falso = sum(counter.get(termo, 0) for termo in termos_falso)\n",
        "\n",
        "                        if count_falso > 0:\n",
        "                            resultados[\"alerta_fact_check\"] = \"Conteúdo marcado como falso ou enganoso por verificadores de fatos\"\n",
        "                            resultados[\"pontos_fact_check\"] = -25\n",
        "                        else:\n",
        "                            resultados[\"pontos_fact_check\"] = 0\n",
        "            else:\n",
        "                resultados[\"fact_checks\"] = []\n",
        "                resultados[\"num_fact_checks\"] = 0\n",
        "                resultados[\"pontos_fact_check\"] = 0\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro ao buscar fact checks: {e}\")\n",
        "            resultados[\"erro_fact_check\"] = str(e)\n",
        "            resultados[\"pontos_fact_check\"] = 0\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def buscar_noticias_similares(self, titulo, keywords, conteudo=None):\n",
        "        \"\"\"Busca por notícias similares na web usando palavras-chave do artigo.\"\"\"\n",
        "        resultados = {}\n",
        "        try:\n",
        "            # Preparar termos de busca\n",
        "            termos_busca = titulo\n",
        "            if keywords:\n",
        "                termos_busca += \" \" + \" \".join(keywords[:3])  # Limitar a 3 keywords\n",
        "\n",
        "            # Simular resultados para demonstração\n",
        "            # Em uma implementação real, você usaria uma API como Google News ou similar\n",
        "            resultados[\"artigos_similares\"] = 5  # Simulando 5 artigos similares\n",
        "            resultados[\"similaridade\"] = \"média\"  # baixa, média, alta\n",
        "\n",
        "            # Simular consistência entre fontes\n",
        "            resultados[\"consistencia_entre_fontes\"] = \"alta\"  # baixa, média, alta\n",
        "            resultados[\"pontos_consistencia\"] = 10  # Simulando pontos por alta consistência\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro ao buscar notícias similares: {e}\")\n",
        "            resultados[\"erro_busca\"] = str(e)\n",
        "            resultados[\"pontos_consistencia\"] = 0\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def converter_nivel_para_texto(self, nivel):\n",
        "        \"\"\"Converte um nível numérico de veracidade para texto.\"\"\"\n",
        "        nivel_texto = {\n",
        "            1: \"CONFIRMADO\",\n",
        "            2: \"PROVAVELMENTE VERDADEIRO\",\n",
        "            3: \"INCONCLUSIVO\",\n",
        "            4: \"PROVAVELMENTE FALSO\",\n",
        "            5: \"FALSO\"\n",
        "        }.get(nivel, \"INCONCLUSIVO\")\n",
        "        return nivel_texto\n",
        "\n",
        "    def analisar_credibilidade(self, url):\n",
        "        \"\"\"Análise completa de credibilidade de uma URL de notícia.\"\"\"\n",
        "        # 1. Baixar o artigo\n",
        "        artigo = self.baixar_artigo(url)\n",
        "        if not artigo or not artigo.get('conteudo'):\n",
        "            logger.error(f\"Não foi possível baixar o artigo: {url}\")\n",
        "            return {\"erro\": \"Não foi possível baixar o artigo para análise\"}\n",
        "\n",
        "        resultados = {}\n",
        "        conteudo = artigo['conteudo']\n",
        "\n",
        "        # 2. Verificação de fonte/domínio\n",
        "        status_fonte, pontos_fonte, info_fonte = self.verificar_fonte(url)\n",
        "        resultados.update(info_fonte)\n",
        "\n",
        "        # 3. Copiar informações básicas\n",
        "        resultados[\"titulo\"] = artigo['titulo']\n",
        "        resultados[\"data_publicacao\"] = artigo['data']\n",
        "        resultados[\"autores\"] = artigo['autores']\n",
        "        resultados[\"url\"] = artigo['url']\n",
        "\n",
        "        # 4. Análise básica do conteúdo\n",
        "        palavras = len(conteudo.split())\n",
        "        resultados[\"num_palavras\"] = palavras\n",
        "        if palavras < 100:\n",
        "            resultados[\"alerta_tamanho\"] = \"Texto muito curto, pode não ser um artigo completo\"\n",
        "\n",
        "        # 5. Análise detalhada da linguagem\n",
        "        analise_linguagem = self.analisar_linguagem(conteudo)\n",
        "        resultados.update(analise_linguagem)\n",
        "\n",
        "        # 6. Verificação de citações\n",
        "        citacoes = re.findall(r'\"([^\"]+)\"', conteudo)\n",
        "        resultados[\"num_citacoes\"] = len(citacoes)\n",
        "        if len(citacoes) == 0:\n",
        "            resultados[\"alerta_fontes\"] = \"Nenhuma citação direta encontrada\"\n",
        "\n",
        "        # 7. Verificação de URLs externos (referências)\n",
        "        urls = re.findall(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', conteudo)\n",
        "        resultados[\"num_urls\"] = len(urls)\n",
        "\n",
        "        # 8. Análise de imagens\n",
        "        resultados_imagens = self.verificar_imagens(artigo.get('imagens', []))\n",
        "        resultados.update(resultados_imagens)\n",
        "\n",
        "        # 9. Verificação cruzada com outras notícias\n",
        "        resultados_similares = self.buscar_noticias_similares(\n",
        "            artigo['titulo'],\n",
        "            artigo.get('keywords', []),\n",
        "            conteudo\n",
        "        )\n",
        "        resultados.update(resultados_similares)\n",
        "\n",
        "        # 10. Verificação em serviços de fact-checking\n",
        "        resultados_fact_check = self.buscar_fact_check(conteudo, artigo['titulo'])\n",
        "        resultados.update(resultados_fact_check)\n",
        "\n",
        "        # Cálculo mais sofisticado usando vários fatores\n",
        "# 11. Determinar um nível de veracidade\n",
        "        # Cálculo mais sofisticado usando vários fatores\n",
        "        base_pontuacao = 70  # Base inicial neutra\n",
        "               # Fatores que afetam a pontuação\n",
        "        pontuacao_fatores = [\n",
        "            resultados.get(\"pontos_fonte\", 0),\n",
        "            resultados.get(\"pontos_consistencia\", 0),\n",
        "            resultados.get(\"pontos_evidencia\", 0),\n",
        "            resultados.get(\"pontos_conhecimento_previo\", 0),\n",
        "            resultados.get(\"pontos_contexto\", 0),\n",
        "            resultados.get(\"pontos_bias\", 0)\n",
        "        ]\n",
        "                # Aplicar pesos aos fatores\n",
        "        pesos = [1.5, 1.2, 2.0, 1.0, 1.0, 0.8]\n",
        "        pontuacao_ponderada = sum(p * f for p, f in zip(pesos, pontuacao_fatores))\n",
        "                # Calcular pontuação final\n",
        "        pontuacao_final = max(0, min(100, base_pontuacao + pontuacao_ponderada))\n",
        "                # Determinar nível de veracidade baseado na pontuação final\n",
        "        if pontuacao_final >= 90:\n",
        "            nivel_veracidade = \"Altamente verificado\"\n",
        "        elif pontuacao_final >= 75:\n",
        "            nivel_veracidade = \"Provavelmente verdadeiro\"\n",
        "        elif pontuacao_final >= 60:\n",
        "            nivel_veracidade = \"Parcialmente verificado\"\n",
        "        elif pontuacao_final >= 40:\n",
        "            nivel_veracidade = \"Inconclusivo\"\n",
        "        elif pontuacao_final >= 25:\n",
        "            nivel_veracidade = \"Provavelmente falso\"\n",
        "        else:\n",
        "            nivel_veracidade = \"Falso\"\n",
        "\n",
        "        resultados[\"pontuacao_veracidade\"] = pontuacao_final\n",
        "        resultados[\"nivel_veracidade\"] = nivel_veracidade\n",
        "                # 12. Sumarizar os resultados\n",
        "        resumo = {\n",
        "            \"nivel_veracidade\": nivel_veracidade,\n",
        "            \"pontuacao\": pontuacao_final,\n",
        "            \"analise_detalhada\": resultados,\n",
        "            \"pontos_chave\": {\n",
        "                \"fontes\": resultados.get(\"qualidade_fonte\", \"Não analisado\"),\n",
        "                \"consistencia\": resultados.get(\"consistencia_narrativa\", \"Não analisado\"),\n",
        "                \"evidencias\": resultados.get(\"qualidade_evidencia\", \"Não analisado\"),\n",
        "                \"contexto\": resultados.get(\"analise_contexto\", \"Não analisado\")\n",
        "            },\n",
        "            \"recomendacao\": \"\"\n",
        "        }\n",
        "\n",
        "        # Adicionar uma recomendação baseada no nível de veracidade\n",
        "        if pontuacao_final >= 75:\n",
        "            resumo[\"recomendacao\"] = \"Informação confiável para uso e disseminação.\"\n",
        "        elif pontuacao_final >= 50:\n",
        "            resumo[\"recomendacao\"] = \"Verificar informações adicionais antes de aceitar completamente.\"\n",
        "        else:\n",
        "            resumo[\"recomendacao\"] = \"Não recomendado para uso ou compartilhamento sem verificação adicional.\"\n",
        "                    return resumo\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHwuByN2zTKa",
        "outputId": "15a32a3f-96ac-41f6-af80-4d55bbb07ae0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting verificacao_simples.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run verificacao_simples.py"
      ],
      "metadata": {
        "id": "44-4JA8YvqgL"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile verificacao_simples.py\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import ssl\n",
        "import urllib3\n",
        "import json\n",
        "from urllib.parse import urlparse, urlencode\n",
        "import hashlib\n",
        "from collections import Counter\n",
        "import time\n",
        "import logging\n",
        "import base64\n",
        "import io\n",
        "import nltk\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from textblob import TextBlob  # Para análise de sentimento\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "from datetime import datetime\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Configurar o logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(\"VerificadorNoticias\")\n",
        "\n",
        "# Desabilitar avisos SSL\n",
        "urllib3.disable_warnings()\n",
        "\n",
        "class VerificadorNoticias:\n",
        "    def __init__(self):\n",
        "        logger.info(\"Verificador de Notícias inicializado!\")\n",
        "\n",
        "        # Carregar listas de fontes confiáveis e não confiáveis\n",
        "        self.fontes_confiaveis = [\n",
        "            'g1.globo.com', 'bbc.com', 'bbc.co.uk', 'reuters.com', 'apnews.com',\n",
        "            'nytimes.com', 'washingtonpost.com', 'estadao.com.br', 'folha.uol.com.br',\n",
        "            'cnn.com', 'valor.globo.com', 'info.abril.com.br', 'oglobo.globo.com',\n",
        "            'dw.com', 'npr.org', 'tecmundo.com.br', 'canaltech.com.br', 'olhardigital.com.br'\n",
        "        ]\n",
        "        self.fontes_suspeitas = [\n",
        "            'theonion.com', 'sensacionalista.com.br', 'breitbart.com', 'infowars.com',\n",
        "            'clickhole.com', 'nationalreport.net', 'worldnewsdailyreport.com',\n",
        "            'dailycurrant.com', 'empirenews.net'\n",
        "        ]\n",
        "\n",
        "        # Palavras para análise linguística\n",
        "        self.palavras_sensacionalistas = {\n",
        "            \"chocante\": 2, \"inacreditável\": 2, \"surpreendente\": 1, \"sensacional\": 2,\n",
        "            \"impressionante\": 1, \"alarmante\": 1.5, \"exclusivo\": 1, \"urgente\": 1.5,\n",
        "            \"shocking\": 2, \"unbelievable\": 2, \"amazing\": 1, \"surprising\": 1,\n",
        "            \"imperdível\": 1.5, \"incrível\": 1.5, \"absurdo\": 1.5, \"escandaloso\": 2,\n",
        "            \"bomba\": 1.5, \"inédito\": 1, \"revelado\": 1, \"segredo\": 1.5,\n",
        "            \"polêmico\": 1.5, \"controverso\": 1, \"escândalo\": 1.5, \"viral\": 1.5,\n",
        "            \"bizarro\": 1.5, \"aterrorizante\": 2, \"assustador\": 1.5, \"terrível\": 1.5,\n",
        "            \"estarrecedor\": 2, \"impossível\": 1.5\n",
        "        }\n",
        "\n",
        "        self.palavras_incerteza = [\n",
        "            \"talvez\", \"possivelmente\", \"provavelmente\", \"pode ser\", \"especula-se\",\n",
        "            \"acredita-se\", \"sugere\", \"alega\", \"supostamente\", \"rumores\",\n",
        "            \"maybe\", \"possibly\", \"probably\", \"might be\", \"it is speculated\",\n",
        "            \"it is believed\", \"suggests\", \"allegedly\", \"supposedly\", \"rumors\",\n",
        "            \"parece que\", \"há relatos\", \"não confirmado\", \"segundo fontes\",\n",
        "            \"indícios\", \"sinalizam\", \"aparentemente\", \"estaria\", \"poderia\",\n",
        "            \"teria\", \"estima-se\"\n",
        "        ]\n",
        "\n",
        "        self.expressoes_opinativas = [\n",
        "            \"eu acho\", \"na minha opinião\", \"parece que\", \"acredito que\",\n",
        "            \"i think\", \"in my opinion\", \"it seems that\", \"i believe\",\n",
        "            \"na visão de\", \"do meu ponto de vista\", \"considero\", \"entendo que\",\n",
        "            \"me parece\", \"creio que\", \"penso que\", \"sinto que\"\n",
        "        ]\n",
        "\n",
        "        # URLs de APIs para verificação de fatos\n",
        "        self.fact_check_api_url = \"https://factchecktools.googleapis.com/v1alpha1/claims:search\"\n",
        "        self.fact_check_api_key = \"SUA_API_KEY_AQUI\"  # Substitua pela sua chave de API real\n",
        "\n",
        "        # Tentar baixar recursos NLTK necessários para análise de texto\n",
        "        try:\n",
        "            nltk.download('punkt', quiet=True)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Não foi possível baixar recursos NLTK: {e}\")\n",
        "\n",
        "    def extrair_dominio(self, url):\n",
        "        \"\"\"Extrai o domínio base da URL.\"\"\"\n",
        "        parsed_url = urlparse(url)\n",
        "        return parsed_url.netloc\n",
        "\n",
        "    def verificar_url_simples(self, url):\n",
        "        \"\"\"Executa uma verificação simples em uma URL de notícia.\"\"\"\n",
        "        try:\n",
        "            # Extrair domínio\n",
        "            parsed_url = urlparse(url)\n",
        "            dominio = parsed_url.netloc\n",
        "\n",
        "            # Tentar acessar a URL\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers, timeout=10, verify=False)\n",
        "            response.raise_for_status()  # Levantar exceção para status HTTP de erro\n",
        "\n",
        "            # Verificar status HTTP\n",
        "            if response.status_code == 200:\n",
        "                logger.info(f\"URL acessada com sucesso: {url}\")\n",
        "                logger.info(f\"Domínio: {dominio}\")\n",
        "                logger.info(f\"Tamanho do conteúdo: {len(response.text)} caracteres.\")\n",
        "                return {\n",
        "                    \"status\": \"sucesso\",\n",
        "                    \"dominio\": dominio,\n",
        "                    \"tamanho_conteudo\": len(response.text),\n",
        "                    \"mensagem\": \"URL acessada com sucesso\"\n",
        "                }\n",
        "            else:\n",
        "                logger.warning(f\"Erro ao acessar a URL. Código HTTP: {response.status_code}\")\n",
        "                return {\n",
        "                    \"status\": \"erro\",\n",
        "                    \"mensagem\": f\"Erro ao acessar a URL. Código HTTP: {response.status_code}\"\n",
        "                }\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            logger.error(f\"Erro ao acessar a URL: {e}\")\n",
        "            return {\n",
        "                \"status\": \"erro\",\n",
        "                \"mensagem\": str(e)\n",
        "            }\n",
        "def analisar_sentimento(self, texto):\n",
        "    sentiment = TextBlob(texto).sentiment\n",
        "    return {\n",
        "        \"polaridade\": sentiment.polarity,\n",
        "        \"subjetividade\": sentiment.subjectivity\n",
        "    }\n",
        "\n",
        "    def verificar_fonte(self, url):\n",
        "        \"\"\"Verifica o domínio da URL contra as listas de fontes.\"\"\"\n",
        "        dominio = self.extrair_dominio(url)\n",
        "        resultado = {\"dominio\": dominio}\n",
        "\n",
        "        # Verificar se o domínio está nas listas\n",
        "        if any(dominio.endswith(fonte) for fonte in self.fontes_confiaveis):\n",
        "            resultado[\"status_fonte\"] = \"confiável\"\n",
        "            resultado[\"pontos_fonte\"] = 20\n",
        "            return \"Fonte confiável\", 20, resultado\n",
        "        elif any(dominio.endswith(fonte) for fonte in self.fontes_suspeitas):\n",
        "            resultado[\"status_fonte\"] = \"suspeita\"\n",
        "            resultado[\"pontos_fonte\"] = -30\n",
        "            resultado[\"alerta_fonte\"] = \"Fonte conhecida por conteúdo satírico ou de baixa credibilidade\"\n",
        "            return \"Fonte suspeita\", -30, resultado\n",
        "        else:\n",
        "            resultado[\"status_fonte\"] = \"não categorizada\"\n",
        "            resultado[\"pontos_fonte\"] = 0\n",
        "            return \"Fonte não categorizada\", 0, resultado\n",
        "\n",
        "    def baixar_artigo(self, url):\n",
        "        \"\"\"Baixa o conteúdo do artigo a partir da URL.\"\"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers, verify=False, timeout=15)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Tenta extrair o título\n",
        "            titulo = soup.title.text.strip() if soup.title else \"Título não encontrado\"\n",
        "\n",
        "            # Tenta extrair o conteúdo do artigo (abordagem mais abrangente)\n",
        "            content = \"\"\n",
        "            # Procura por conteúdo em elementos comuns\n",
        "            for tag in ['article', 'main', '.content', '.post', '.entry', '#content', '.article-body', '.news-text']:\n",
        "                if tag.startswith('.') or tag.startswith('#'):\n",
        "                    elements = soup.select(tag)\n",
        "                else:\n",
        "                    elements = soup.find_all(tag)\n",
        "                if elements:\n",
        "                    for element in elements:\n",
        "                        content += element.get_text(separator=' ', strip=True) + \" \"\n",
        "                    break\n",
        "\n",
        "            # Se não encontrou conteúdo nos elementos específicos, pega todos os parágrafos\n",
        "            if not content:\n",
        "                paragraphs = soup.find_all('p')\n",
        "                content = ' '.join([p.get_text(strip=True) for p in paragraphs])\n",
        "\n",
        "            # Tenta encontrar a data\n",
        "            data = None\n",
        "            # Procura por metadados comuns\n",
        "            for meta in soup.find_all('meta'):\n",
        "                if meta.get('property') in ['article:published_time', 'og:published_time', 'publication_date'] or meta.get('name') in ['publication_date', 'date', 'pubdate']:\n",
        "                    data = meta.get('content')\n",
        "                    break\n",
        "\n",
        "            # Procura por elementos que possam conter autores\n",
        "            autores = []\n",
        "            for author_tag in ['author', 'byline', '.author', '.byline', '[rel=author]', '[name=author]']:\n",
        "                if author_tag.startswith('.') or author_tag.startswith('['):\n",
        "                    elements = soup.select(author_tag)\n",
        "                else:\n",
        "                    elements = soup.find_all(author_tag)\n",
        "\n",
        "                if elements:\n",
        "                    for element in elements:\n",
        "                        autor = element.get_text(strip=True)\n",
        "                        if autor and len(autor) < 100:  # Evitar pegar conteúdo grande por engano\n",
        "                            autores.append(autor)\n",
        "\n",
        "            # Remover duplicações nos autores\n",
        "            autores = list(dict.fromkeys(autores))\n",
        "\n",
        "            # Extrair imagens\n",
        "            imagens = []\n",
        "            for img in soup.find_all('img'):\n",
        "                src = img.get('src')\n",
        "                alt = img.get('alt', '')\n",
        "                if src and not src.startswith('data:'):\n",
        "                    # Garantir URL completa\n",
        "                    if src.startswith('/'):\n",
        "                        parsed_url = urlparse(url)\n",
        "                        base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
        "                        src = base_url + src\n",
        "                    imagens.append({\n",
        "                        'url': src,\n",
        "                        'alt': alt\n",
        "                    })\n",
        "\n",
        "            # Extrair palavras-chave\n",
        "            keywords = []\n",
        "            meta_keywords = soup.find('meta', attrs={'name': 'keywords'})\n",
        "            if meta_keywords:\n",
        "                keywords = meta_keywords.get('content', '').split(',')\n",
        "                keywords = [k.strip() for k in keywords]\n",
        "\n",
        "            return {\n",
        "                'titulo': titulo,\n",
        "                'conteudo': content,\n",
        "                'data': data,\n",
        "                'autores': autores,\n",
        "                'url': response.url,  # Use a URL final após redirecionamentos\n",
        "                'imagens': imagens,\n",
        "                'keywords': keywords,\n",
        "                'html': response.text  # Manter o HTML para possível análise adicional\n",
        "            }\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            logger.error(f\"Erro ao baixar artigo ({url}): {e}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro inesperado ao processar artigo ({url}): {e}\")\n",
        "            return None\n",
        "# Integra API do Google Fact Check.\n",
        "def consultar_fact_check(self, query):\n",
        "    url_api = f\"https://factchecktools.googleapis.com/v1alpha1/claims:search?query={query}\"\n",
        "    try:\n",
        "        response = requests.get(url_api)\n",
        "        if response.status_code == 200:\n",
        "            return response.json()\n",
        "        else:\n",
        "            logger.warning(\"Falha ao acessar API de Fact Check.\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Erro na API de Fact Check: {e}\")\n",
        "        return None\n",
        "    def analisar_linguagem(self, texto):\n",
        "        \"\"\"Analisa o conteúdo em busca de sensacionalismo, incerteza e opinatividade.\"\"\"\n",
        "        if not texto or len(texto) < 10:\n",
        "            return {\n",
        "                \"sensacionalismo\": 0,\n",
        "                \"incerteza\": 0,\n",
        "                \"opinativo\": 0,\n",
        "                \"exclamacoes\": 0\n",
        "            }\n",
        "\n",
        "        resultado = {}\n",
        "\n",
        "        # Análise de sensacionalismo\n",
        "        indice_sensacionalismo = sum(\n",
        "            len(re.findall(r'\\b' + re.escape(palavra) + r'\\b', texto.lower())) * peso\n",
        "            for palavra, peso in self.palavras_sensacionalistas.items()\n",
        "        )\n",
        "        resultado[\"sensacionalismo\"] = indice_sensacionalismo\n",
        "\n",
        "        # Normalizar por tamanho do texto (por 1000 palavras)\n",
        "        palavras_totais = len(texto.split())\n",
        "        if palavras_totais > 0:\n",
        "            indice_sensacionalismo_norm = (indice_sensacionalismo * 1000) / palavras_totais\n",
        "            resultado[\"sensacionalismo_normalizado\"] = indice_sensacionalismo_norm\n",
        "\n",
        "            # Adicionar alerta se o índice for alto\n",
        "            if indice_sensacionalismo_norm > 10:\n",
        "                resultado[\"alerta_sensacionalismo\"] = \"Alto índice de linguagem sensacionalista\"\n",
        "            elif indice_sensacionalismo_norm > 5:\n",
        "                resultado[\"alerta_sensacionalismo\"] = \"Moderado índice de linguagem sensacionalista\"\n",
        "\n",
        "        # Análise de incerteza\n",
        "        indice_incerteza = sum(\n",
        "            len(re.findall(r'\\b' + re.escape(palavra) + r'\\b', texto.lower()))\n",
        "            for palavra in self.palavras_incerteza\n",
        "        )\n",
        "        resultado[\"incerteza\"] = indice_incerteza\n",
        "\n",
        "        if palavras_totais > 0:\n",
        "            indice_incerteza_norm = (indice_incerteza * 1000) / palavras_totais\n",
        "            resultado[\"incerteza_normalizada\"] = indice_incerteza_norm\n",
        "\n",
        "            if indice_incerteza_norm > 8:\n",
        "                resultado[\"alerta_incerteza\"] = \"Alto índice de expressões de incerteza\"\n",
        "\n",
        "        # Análise de opinatividade\n",
        "        indice_opiniao = sum(\n",
        "            len(re.findall(r'\\b' + re.escape(expressao) + r'\\b', texto.lower()))\n",
        "            for expressao in self.expressoes_opinativas\n",
        "        )\n",
        "        resultado[\"opinativo\"] = indice_opiniao\n",
        "\n",
        "        if palavras_totais > 0:\n",
        "            indice_opiniao_norm = (indice_opiniao * 1000) / palavras_totais\n",
        "            resultado[\"opinativo_normalizado\"] = indice_opiniao_norm\n",
        "\n",
        "            if indice_opiniao_norm > 5:\n",
        "                resultado[\"alerta_opiniao\"] = \"Alto índice de expressões opinativas\"\n",
        "\n",
        "        # Contagem de pontos de exclamação (indicador de sensacionalismo)\n",
        "        exclamacoes = texto.count('!')\n",
        "        resultado[\"exclamacoes\"] = exclamacoes\n",
        "\n",
        "        if palavras_totais > 0:\n",
        "            exclamacoes_norm = (exclamacoes * 1000) / palavras_totais\n",
        "            if exclamacoes_norm > 10:\n",
        "                resultado[\"alerta_exclamacoes\"] = \"Uso excessivo de pontos de exclamação\"\n",
        "\n",
        "        # Análise de sentimento usando TextBlob\n",
        "        try:\n",
        "            analise = TextBlob(texto)\n",
        "            resultado[\"sentimento_polaridade\"] = analise.sentiment.polarity  # -1 a 1 (negativo a positivo)\n",
        "            resultado[\"sentimento_subjetividade\"] = analise.sentiment.subjectivity  # 0 a 1 (objetivo a subjetivo)\n",
        "\n",
        "            # Categorização baseada na polaridade\n",
        "            if resultado[\"sentimento_polaridade\"] < -0.5:\n",
        "                resultado[\"sentimento_categoria\"] = \"muito negativo\"\n",
        "            elif resultado[\"sentimento_polaridade\"] < -0.1:\n",
        "                resultado[\"sentimento_categoria\"] = \"negativo\"\n",
        "            elif resultado[\"sentimento_polaridade\"] <= 0.1:\n",
        "                resultado[\"sentimento_categoria\"] = \"neutro\"\n",
        "            elif resultado[\"sentimento_polaridade\"] <= 0.5:\n",
        "                resultado[\"sentimento_categoria\"] = \"positivo\"\n",
        "            else:\n",
        "                resultado[\"sentimento_categoria\"] = \"muito positivo\"\n",
        "\n",
        "            # Alerta para alto nível de subjetividade em notícias que deveriam ser objetivas\n",
        "            if resultado[\"sentimento_subjetividade\"] > 0.6:\n",
        "                resultado[\"alerta_subjetividade\"] = \"Conteúdo altamente subjetivo\"\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Erro na análise de sentimento: {e}\")\n",
        "\n",
        "        return resultado\n",
        "\n",
        "    def verificar_imagens(self, imagens):\n",
        "        \"\"\"Analisa as imagens associadas ao artigo.\"\"\"\n",
        "        resultados = {}\n",
        "        if not imagens:\n",
        "            resultados[\"alerta_imagens\"] = \"Nenhuma imagem encontrada no artigo\"\n",
        "            return resultados\n",
        "\n",
        "        resultados[\"num_imagens\"] = len(imagens)\n",
        "\n",
        "        # Verificar descrições das imagens (alt text)\n",
        "        imagens_sem_descricao = sum(1 for img in imagens if not img.get('alt'))\n",
        "        resultados[\"imagens_sem_descricao\"] = imagens_sem_descricao\n",
        "\n",
        "        if imagens_sem_descricao == len(imagens) and len(imagens) > 0:\n",
        "            resultados[\"alerta_imagens_descricao\"] = \"Nenhuma das imagens possui descrição adequada\"\n",
        "\n",
        "        # Calcular hash básico das URLs de imagens para potencial comparação futura\n",
        "        hashes_urls = [hashlib.md5(img['url'].encode()).hexdigest()[:8] for img in imagens]\n",
        "        resultados[\"hashes_imagens\"] = hashes_urls\n",
        "\n",
        "        # Analisar metadados das primeiras 5 imagens (limitar para evitar muito processamento)\n",
        "        try:\n",
        "            with ThreadPoolExecutor(max_workers=3) as executor:\n",
        "                metadados = list(executor.map(self.extrair_metadados_imagem, [img['url'] for img in imagens[:5]]))\n",
        "                resultados[\"metadados_imagens\"] = [m for m in metadados if m]  # Filtrar None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro ao analisar metadados das imagens: {e}\")\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def extrair_metadados_imagem(self, url_imagem):\n",
        "        \"\"\"Extrai metadados de uma imagem a partir da URL.\"\"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            response = requests.get(url_imagem, headers=headers, stream=True, timeout=5, verify=False)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Abrir a imagem com PIL\n",
        "            img = Image.open(io.BytesIO(response.content))\n",
        "\n",
        "            # Coletar metadados básicos\n",
        "            metadados = {\n",
        "                \"formato\": img.format,\n",
        "                \"tamanho\": img.size,\n",
        "                \"modo\": img.mode,\n",
        "                \"url\": url_imagem,\n",
        "                \"hash\": hashlib.md5(response.content).hexdigest()\n",
        "            }\n",
        "\n",
        "            # Tentar extrair EXIF se disponível\n",
        "            if hasattr(img, '_getexif') and img._getexif():\n",
        "                exif = img._getexif()\n",
        "                if exif:\n",
        "                    exif_data = {}\n",
        "                    for tag_id, value in exif.items():\n",
        "                        # Converter para string para garantir serialização JSON\n",
        "                        exif_data[str(tag_id)] = str(value)\n",
        "                    metadados[\"exif\"] = exif_data\n",
        "\n",
        "            return metadados\n",
        "        except (requests.RequestException, UnidentifiedImageError, IOError) as e:\n",
        "            logger.warning(f\"Erro ao extrair metadados da imagem {url_imagem}: {e}\")\n",
        "            return None\n",
        "\n",
        "class SistemaVerificacaoUnificado:\n",
        "    def __init__(self):\n",
        "        # Carregar variáveis do arquivo .env\n",
        "        load_dotenv()\n",
        "        self.fact_check_api_key = os.getenv(\"FACT_CHECK_API_KEY\")\n",
        "\n",
        "        # Verificar se a chave de API está disponível\n",
        "        if not self.fact_check_api_key or self.fact_check_api_key.strip() == \"\":\n",
        "            raise ValueError(\"Chave de API de verificação de fatos não encontrada. Verifique o arquivo .env.\")\n",
        "\n",
        "            return resultados\n",
        "                    try:\n",
        "            # Preparar termos de busca\n",
        "            query = titulo if titulo else texto[:100]\n",
        "                        # Parâmetros da consulta\n",
        "            params = {\n",
        "                'key': self.fact_check_api_key,\n",
        "                'query': query,\n",
        "                'languageCode': 'pt'  # Você pode ajustar ou detectar automaticamente\n",
        "            }\n",
        "\n",
        "            # Fazer a requisição\n",
        "            response = requests.get(\n",
        "                self.fact_check_api_url,\n",
        "                params=params\n",
        "            )\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                resultados[\"fact_checks\"] = data.get('claims', [])\n",
        "                resultados[\"num_fact_checks\"] = len(resultados[\"fact_checks\"])\n",
        "\n",
        "                # Analisar resultados\n",
        "                if resultados[\"num_fact_checks\"] > 0:\n",
        "                    # Verificar ratings médios\n",
        "                    ratings = []\n",
        "                    for claim in resultados[\"fact_checks\"]:\n",
        "                        for review in claim.get('claimReview', []):\n",
        "                            if 'textualRating' in review:\n",
        "                                ratings.append(review['textualRating'].lower())\n",
        "\n",
        "                    # Contar frequência de cada rating\n",
        "                    if ratings:\n",
        "                        counter = Counter(ratings)\n",
        "                        resultados[\"ratings_frequencia\"] = counter\n",
        "\n",
        "                        # Detectar termos comuns de falsidade\n",
        "                        termos_falso = ['false', 'fake', 'falso', 'mentira', 'enganoso', 'misleading']\n",
        "                        count_falso = sum(counter.get(termo, 0) for termo in termos_falso)\n",
        "\n",
        "                        if count_falso > 0:\n",
        "                            resultados[\"alerta_fact_check\"] = \"Conteúdo marcado como falso ou enganoso por verificadores de fatos\"\n",
        "                            resultados[\"pontos_fact_check\"] = -25\n",
        "                        else:\n",
        "                            resultados[\"pontos_fact_check\"] = 0\n",
        "            else:\n",
        "                logger.warning(f\"Erro na API de Fact Check. Status: {response.status_code}\")\n",
        "                resultados[\"erro_fact_check\"] = f\"Erro na API. Status: {response.status_code}\"\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro ao buscar fact checks: {e}\")\n",
        "            resultados[\"erro_fact_check\"] = str(e)\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def buscar_noticias_similares(self, titulo, keywords, conteudo=None):\n",
        "        \"\"\"Busca por notícias similares na web usando palavras-chave do artigo.\"\"\"\n",
        "        resultados = {}\n",
        "        try:\n",
        "            # Preparar termos de busca\n",
        "            termos_busca = titulo\n",
        "            if keywords:\n",
        "                termos_busca += \" \" + \" \".join(keywords[:3])  # Limitar a 3 keywords\n",
        "\n",
        "            # Simular resultados para demonstração\n",
        "            # Em uma implementação real, você usaria uma API como Google News ou similar\n",
        "            resultados[\"artigos_similares\"] = 5  # Simulando 5 artigos similares\n",
        "            resultados[\"similaridade\"] = \"média\"  # baixa, média, alta\n",
        "\n",
        "            # Simular consistência entre fontes\n",
        "            resultados[\"consistencia_entre_fontes\"] = \"alta\"  # baixa, média, alta\n",
        "            resultados[\"pontos_consistencia\"] = 10  # Simulando pontos por alta consistência\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro ao buscar notícias similares: {e}\")\n",
        "            resultados[\"erro_busca\"] = str(e)\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def converter_nivel_para_texto(self, nivel):\n",
        "        \"\"\"Converte um nível numérico de veracidade para texto.\"\"\"\n",
        "        nivel_texto = {\n",
        "            1: \"CONFIRMADO\",\n",
        "            2: \"PROVAVELMENTE VERDADEIRO\",\n",
        "            3: \"INCONCLUSIVO\",\n",
        "            4: \"PROVAVELMENTE FALSO\",\n",
        "            5: \"FALSO\"\n",
        "        }.get(nivel, \"INCONCLUSIVO\")\n",
        "        return nivel_texto\n",
        "\n",
        "    def analisar_credibilidade(self, artigo):\n",
        "        \"\"\"Análise combinada de credibilidade baseada em características do texto e fonte.\"\"\"\n",
        "        if not artigo or not artigo.get('conteudo'):\n",
        "            return {\"erro\": \"Artigo não disponível para análise\"}\n",
        "\n",
        "        resultados = {}\n",
        "        conteudo = artigo['conteudo']\n",
        "\n",
        "        # 1. Verificação de fonte/domínio\n",
        "        _, _, info_fonte = self.verificar_fonte(artigo['url'])\n",
        "        resultados.update(info_fonte)\n",
        "\n",
        "        # 2. Copiar informações básicas\n",
        "        resultados[\"titulo\"] = artigo['titulo']\n",
        "        resultados[\"data_publicacao\"] = artigo['data']\n",
        "        resultados[\"autores\"] = artigo['autores']\n",
        "        resultados[\"url\"] = artigo['url']\n",
        "\n",
        "        # 3. Análise básica do conteúdo\n",
        "        palavras = len(conteudo.split())\n",
        "        resultados[\"num_palavras\"] = palavras\n",
        "        if palavras < 100:\n",
        "            resultados[\"alerta_tamanho\"] = \"Texto muito curto, pode não ser um artigo completo\"\n",
        "\n",
        "        # 4. Análise detalhada da linguagem\n",
        "        analise_linguagem = self.analisar_linguagem(conteudo)\n",
        "        resultados.update(analise_linguagem)\n",
        "\n",
        "        # 5. Verificação de citações\n",
        "        citacoes = re.findall(r'\"([^\"]+)\"', conteudo)\n",
        "        resultados[\"num_citacoes\"] = len(citacoes)\n",
        "        if len(citacoes) == 0:\n",
        "            resultados[\"alerta_fontes\"] = \"Nenhuma citação direta encontrada\"\n",
        "\n",
        "        # 6. Verificação de URLs externos (referências)\n",
        "        urls = re.findall(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', conteudo)\n",
        "        resultados[\"num_urls\"] = len(urls)\n",
        "\n",
        "        # 7. Análise de imagens\n",
        "        resultados_imagens = self.verificar_imagens(artigo.get('imagens', []))\n",
        "        resultados.update(resultados_imagens)\n",
        "\n",
        "        # 8. Verificação cruzada com outras notícias\n",
        "        resultados_similares = self.buscar_noticias_similares(\n",
        "            artigo['titulo'],\n",
        "            artigo.get('keywords', []),\n",
        "            conteudo\n",
        "        )\n",
        "        resultados.update(resultados_similares)\n",
        "\n",
        "        # 9. Verificação em serviços de fact-checking\n",
        "        resultados_fact_check = self.buscar_fact_check(conteudo, artigo['titulo'])\n",
        "        resultados.update(resultados_fact_check)\n",
        "                # 10. Determinar um nível de veracidade\n",
        "        # Cálculo mais sofisticado usando vários fatores\n",
        "        base_pontuacao = 70  # Base inicial neutra\n",
        "                # Fatores que afetam a pontuação\n",
        "        pontuacao_fatores = [\n",
        "            resultados.get(\"pontos_fonte\", 0),\n",
        "            resultados.get(\"pontos_fact_check\", 0),\n",
        "            resultados.get(\"pontos_consistencia\", 0)\n",
        "        ]\n",
        "\n",
        "        # Penalidades\n",
        "        if \"alerta_tamanho\" in resultados:\n",
        "            pontuacao_fatores.append(-15)\n",
        "        if \"alerta_sensacionalismo\" in resultados:\n",
        "            if \"Alto índice\" in resultados[\"alerta_sensacionalismo\"]:\n",
        "                pontuacao_fatores.append(-20)\n",
        "            else:\n",
        "                pontuacao_fatores.append(-10)\n",
        "        if \"alerta_fontes\" in resultados:\n",
        "            pontuacao_fatores.append(-10)\n",
        "        if \"alerta_opiniao\" in resultados:\n",
        "            pontuacao_fatores.append(-15)\n",
        "        if \"alerta_incerteza\" in resultados and resultados[\"incerteza\"] > 3:\n",
        "            pontuacao_fatores.append(-10)\n",
        "        if \"alerta_exclamacoes\" in resultados:\n",
        "            pontuacao_fatores.append(-5)\n",
        "        if \"alerta_imagens_descricao\" in resultados:\n",
        "            pontuacao_fatores.append(-5)\n",
        "        if \"alerta_fact_check\" in resultados:\n",
        "            pontuacao_fatores.append(-25)\n",
        "        if resultados.get(\"sentimento_subjetividade\", 0) > 0.7:\n",
        "            pontuacao_fatores.append(-15)\n",
        "\n",
        "        # Bônus\n",
        "        if resultados.get(\"num_citacoes\", 0) > 3:\n",
        "            pontuacao_fatores.append(10)\n",
        "        elif resultados.get(\"num_citacoes\", 0) > 0:\n",
        "            pontuacao_fatores.append(5)\n",
        "        if resultados.get(\"num_urls\", 0) > 2:\n",
        "            pontuacao_fatores.append(5)\n",
        "        if resultados.get(\"num_imagens\", 0) > 1:\n",
        "            pontuacao_fatores.append(5)\n",
        "        if resultados.get(\"artigos_similares\", 0) > 3:\n",
        "            pontuacao_fatores.append(5)\n",
        "\n",
        "        # Calcular pontuação final\n",
        "        pontuacao_final = base_pontuacao + sum(pontuacao_fatores)\n",
        "        pontuacao_final = max(0, min(100, pontuacao_final))\n",
        "        resultados[\"pontuacao_credibilidade\"] = pontuacao_final\n",
        "# Converter para nível de veracidade\n",
        "nivel_veracidade = 3  # Default: Inconclusivo\n",
        "if pontuacao_final >= 95:\n",
        "    nivel_veracidade = 1  # Confirmado\n",
        "elif pontuacao_final >= 85:\n",
        "    nivel_veracidade = 2  # Provavelmente verdadeiro\n",
        "elif pontuacao_final <= 30:\n",
        "    nivel_veracidade = 4  # Provavelmente falso\n",
        "elif pontuacao_final < 85 and pontuacao_final > 30:\n",
        "    nivel_veracidade = 3  # Inconclusivo\n",
        "# Adicionar o nível de veracidade aos resultados\n",
        "resultados[\"nivel_veracidade\"] = nivel_veracidade\n",
        "# Exibir os resultados (opcional)\n",
        "print(f\"Pontuação de credibilidade: {resultados['pontuacao_credibilidade']}\")\n",
        "print(f\"Nível de veracidade: {resultados['nivel_veracidade']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRjA4La8vWFc",
        "outputId": "8623c980-d18d-43db-b2ee-34c34fe50513"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting verificacao_simples.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile verificacao_simples.py\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import ssl\n",
        "import urllib3\n",
        "import json\n",
        "from urllib.parse import urlparse, urlencode\n",
        "import hashlib\n",
        "from collections import Counter\n",
        "import time\n",
        "import logging\n",
        "import base64\n",
        "import io\n",
        "import nltk\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from textblob import TextBlob  # Para análise de sentimento\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "from datetime import datetime\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Configurar o logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(\"VerificadorNoticias\")\n",
        "\n",
        "# Desabilitar avisos SSL\n",
        "urllib3.disable_warnings()\n",
        "\n",
        "class VerificadorNoticias:\n",
        "    def __init__(self):\n",
        "        logger.info(\"Verificador de Notícias inicializado!\")\n",
        "\n",
        "        # Carregar listas de fontes confiáveis e não confiáveis\n",
        "        self.fontes_confiaveis = [\n",
        "            'g1.globo.com', 'bbc.com', 'bbc.co.uk', 'reuters.com', 'apnews.com',\n",
        "            'nytimes.com', 'washingtonpost.com', 'estadao.com.br', 'folha.uol.com.br',\n",
        "            'cnn.com', 'valor.globo.com', 'info.abril.com.br', 'oglobo.globo.com',\n",
        "            'dw.com', 'npr.org', 'tecmundo.com.br', 'canaltech.com.br', 'olhardigital.com.br'\n",
        "        ]\n",
        "                self.fontes_suspeitas = [\n",
        "            'theonion.com', 'sensacionalista.com.br', 'breitbart.com', 'infowars.com',\n",
        "            'clickhole.com', 'nationalreport.net', 'worldnewsdailyreport.com',\n",
        "            'dailycurrant.com', 'empirenews.net'\n",
        "        ]\n",
        "\n",
        "        # Palavras para análise linguística\n",
        "        self.palavras_sensacionalistas = {\n",
        "            \"chocante\": 2, \"inacreditável\": 2, \"surpreendente\": 1, \"sensacional\": 2,\n",
        "            \"impressionante\": 1, \"alarmante\": 1.5, \"exclusivo\": 1, \"urgente\": 1.5,\n",
        "            \"shocking\": 2, \"unbelievable\": 2, \"amazing\": 1, \"surprising\": 1,\n",
        "            \"imperdível\": 1.5, \"incrível\": 1.5, \"absurdo\": 1.5, \"escandaloso\": 2,\n",
        "            \"bomba\": 1.5, \"inédito\": 1, \"revelado\": 1, \"segredo\": 1.5,\n",
        "            \"polêmico\": 1.5, \"controverso\": 1, \"escândalo\": 1.5, \"viral\": 1.5,\n",
        "            \"bizarro\": 1.5, \"aterrorizante\": 2, \"assustador\": 1.5, \"terrível\": 1.5,\n",
        "            \"estarrecedor\": 2, \"impossível\": 1.5\n",
        "        }\n",
        "\n",
        "        self.palavras_incerteza = [\n",
        "            \"talvez\", \"possivelmente\", \"provavelmente\", \"pode ser\", \"especula-se\",\n",
        "            \"acredita-se\", \"sugere\", \"alega\", \"supostamente\", \"rumores\",\n",
        "            \"maybe\", \"possibly\", \"probably\", \"might be\", \"it is speculated\",\n",
        "            \"it is believed\", \"suggests\", \"allegedly\", \"supposedly\", \"rumors\",\n",
        "            \"parece que\", \"há relatos\", \"não confirmado\", \"segundo fontes\",\n",
        "            \"indícios\", \"sinalizam\", \"aparentemente\", \"estaria\", \"poderia\",\n",
        "            \"teria\", \"estima-se\"\n",
        "        ]\n",
        "\n",
        "        self.expressoes_opinativas = [\n",
        "            \"eu acho\", \"na minha opinião\", \"parece que\", \"acredito que\",\n",
        "            \"i think\", \"in my opinion\", \"it seems that\", \"i believe\",\n",
        "            \"na visão de\", \"do meu ponto de vista\", \"considero\", \"entendo que\",\n",
        "            \"me parece\", \"creio que\", \"penso que\", \"sinto que\"\n",
        "        ]\n",
        "\n",
        "        # URLs de APIs para verificação de fatos\n",
        "        self.fact_check_api_url = \"https://factchecktools.googleapis.com/v1alpha1/claims:search\"\n",
        "        self.fact_check_api_key = \"SUA_API_KEY_AQUI\"  # Substitua pela sua chave de API real\n",
        "\n",
        "        # Tentar baixar recursos NLTK necessários para análise de texto\n",
        "        try:\n",
        "            nltk.download('punkt', quiet=True)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Não foi possível baixar recursos NLTK: {e}\")\n",
        "\n",
        "    def extrair_dominio(self, url):\n",
        "        \"\"\"Extrai o domínio base da URL.\"\"\"\n",
        "        parsed_url = urlparse(url)\n",
        "        return parsed_url.netloc\n",
        "\n",
        "    def verificar_url_simples(self, url):\n",
        "        \"\"\"Executa uma verificação simples em uma URL de notícia.\"\"\"\n",
        "        try:\n",
        "            # Extrair domínio\n",
        "            parsed_url = urlparse(url)\n",
        "            dominio = parsed_url.netloc\n",
        "\n",
        "            # Tentar acessar a URL\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers, timeout=10, verify=False)\n",
        "            response.raise_for_status()  # Levantar exceção para status HTTP de erro\n",
        "\n",
        "            # Verificar status HTTP\n",
        "            if response.status_code == 200:\n",
        "                logger.info(f\"URL acessada com sucesso: {url}\")\n",
        "                logger.info(f\"Domínio: {dominio}\")\n",
        "                logger.info(f\"Tamanho do conteúdo: {len(response.text)} caracteres.\")\n",
        "                return {\n",
        "                    \"status\": \"sucesso\",\n",
        "                    \"dominio\": dominio,\n",
        "                    \"tamanho_conteudo\": len(response.text),\n",
        "                    \"mensagem\": \"URL acessada com sucesso\"\n",
        "                }\n",
        "            else:\n",
        "                logger.warning(f\"Erro ao acessar a URL. Código HTTP: {response.status_code}\")\n",
        "                return {\n",
        "                    \"status\": \"erro\",\n",
        "                    \"mensagem\": f\"Erro ao acessar a URL. Código HTTP: {response.status_code}\"\n",
        "                }\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            logger.error(f\"Erro ao acessar a URL: {e}\")\n",
        "            return {\n",
        "                \"status\": \"erro\",\n",
        "                \"mensagem\": str(e)\n",
        "            }\n",
        "def analisar_sentimento(self, texto):\n",
        "    sentiment = TextBlob(texto).sentiment\n",
        "    return {\n",
        "        \"polaridade\": sentiment.polarity,\n",
        "        \"subjetividade\": sentiment.subjectivity\n",
        "    }\n",
        "\n",
        "    def verificar_fonte(self, url):\n",
        "        \"\"\"Verifica o domínio da URL contra as listas de fontes.\"\"\"\n",
        "        dominio = self.extrair_dominio(url)\n",
        "        resultado = {\"dominio\": dominio}\n",
        "\n",
        "        # Verificar se o domínio está nas listas\n",
        "        if any(dominio.endswith(fonte) for fonte in self.fontes_confiaveis):\n",
        "            resultado[\"status_fonte\"] = \"confiável\"\n",
        "            resultado[\"pontos_fonte\"] = 20\n",
        "            return \"Fonte confiável\", 20, resultado\n",
        "        elif any(dominio.endswith(fonte) for fonte in self.fontes_suspeitas):\n",
        "            resultado[\"status_fonte\"] = \"suspeita\"\n",
        "            resultado[\"pontos_fonte\"] = -30\n",
        "            resultado[\"alerta_fonte\"] = \"Fonte conhecida por conteúdo satírico ou de baixa credibilidade\"\n",
        "            return \"Fonte suspeita\", -30, resultado\n",
        "        else:\n",
        "            resultado[\"status_fonte\"] = \"não categorizada\"\n",
        "            resultado[\"pontos_fonte\"] = 0\n",
        "            return \"Fonte não categorizada\", 0, resultado\n",
        "\n",
        "    def baixar_artigo(self, url):\n",
        "        \"\"\"Baixa o conteúdo do artigo a partir da URL.\"\"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers, verify=False, timeout=15)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Tenta extrair o título\n",
        "            titulo = soup.title.text.strip() if soup.title else \"Título não encontrado\"\n",
        "\n",
        "            # Tenta extrair o conteúdo do artigo (abordagem mais abrangente)\n",
        "            content = \"\"\n",
        "            # Procura por conteúdo em elementos comuns\n",
        "            for tag in ['article', 'main', '.content', '.post', '.entry', '#content', '.article-body', '.news-text']:\n",
        "                if tag.startswith('.') or tag.startswith('#'):\n",
        "                    elements = soup.select(tag)\n",
        "                else:\n",
        "                    elements = soup.find_all(tag)\n",
        "                if elements:\n",
        "                    for element in elements:\n",
        "                        content += element.get_text(separator=' ', strip=True) + \" \"\n",
        "                    break\n",
        "\n",
        "            # Se não encontrou conteúdo nos elementos específicos, pega todos os parágrafos\n",
        "            if not content:\n",
        "                paragraphs = soup.find_all('p')\n",
        "                content = ' '.join([p.get_text(strip=True) for p in paragraphs])\n",
        "\n",
        "            # Tenta encontrar a data\n",
        "            data = None\n",
        "            # Procura por metadados comuns\n",
        "            for meta in soup.find_all('meta'):\n",
        "                if meta.get('property') in ['article:published_time', 'og:published_time', 'publication_date'] or meta.get('name') in ['publication_date', 'date', 'pubdate']:\n",
        "                    data = meta.get('content')\n",
        "                    break\n",
        "\n",
        "            # Procura por elementos que possam conter autores\n",
        "            autores = []\n",
        "            for author_tag in ['author', 'byline', '.author', '.byline', '[rel=author]', '[name=author]']:\n",
        "                if author_tag.startswith('.') or author_tag.startswith('['):\n",
        "                    elements = soup.select(author_tag)\n",
        "                else:\n",
        "                    elements = soup.find_all(author_tag)\n",
        "\n",
        "                if elements:\n",
        "                    for element in elements:\n",
        "                        autor = element.get_text(strip=True)\n",
        "                        if autor and len(autor) < 100:  # Evitar pegar conteúdo grande por engano\n",
        "                            autores.append(autor)\n",
        "\n",
        "            # Remover duplicações nos autores\n",
        "            autores = list(dict.fromkeys(autores))\n",
        "\n",
        "            # Extrair imagens\n",
        "            imagens = []\n",
        "            for img in soup.find_all('img'):\n",
        "                src = img.get('src')\n",
        "                alt = img.get('alt', '')\n",
        "                if src and not src.startswith('data:'):\n",
        "                    # Garantir URL completa\n",
        "                    if src.startswith('/'):\n",
        "                        parsed_url = urlparse(url)\n",
        "                        base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
        "                        src = base_url + src\n",
        "                    imagens.append({\n",
        "                        'url': src,\n",
        "                        'alt': alt\n",
        "                    })\n",
        "\n",
        "            # Extrair palavras-chave\n",
        "            keywords = []\n",
        "            meta_keywords = soup.find('meta', attrs={'name': 'keywords'})\n",
        "            if meta_keywords:\n",
        "                keywords = meta_keywords.get('content', '').split(',')\n",
        "                keywords = [k.strip() for k in keywords]\n",
        "\n",
        "            return {\n",
        "                'titulo': titulo,\n",
        "                'conteudo': content,\n",
        "                'data': data,\n",
        "                'autores': autores,\n",
        "                'url': response.url,  # Use a URL final após redirecionamentos\n",
        "                'imagens': imagens,\n",
        "                'keywords': keywords,\n",
        "                'html': response.text  # Manter o HTML para possível análise adicional\n",
        "            }\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            logger.error(f\"Erro ao baixar artigo ({url}): {e}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro inesperado ao processar artigo ({url}): {e}\")\n",
        "            return None\n",
        "# Integra API do Google Fact Check.\n",
        "def consultar_fact_check(self, query):\n",
        "    url_api = f\"https://factchecktools.googleapis.com/v1alpha1/claims:search?query={query}\"\n",
        "    try:\n",
        "        response = requests.get(url_api)\n",
        "        if response.status_code == 200:\n",
        "            return response.json()\n",
        "        else:\n",
        "            logger.warning(\"Falha ao acessar API de Fact Check.\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Erro na API de Fact Check: {e}\")\n",
        "        return None\n",
        "    def analisar_linguagem(self, texto):\n",
        "        \"\"\"Analisa o conteúdo em busca de sensacionalismo, incerteza e opinatividade.\"\"\"\n",
        "        if not texto or len(texto) < 10:\n",
        "            return {\n",
        "                \"sensacionalismo\": 0,\n",
        "                \"incerteza\": 0,\n",
        "                \"opinativo\": 0,\n",
        "                \"exclamacoes\": 0\n",
        "            }\n",
        "\n",
        "        resultado = {}\n",
        "\n",
        "        # Análise de sensacionalismo\n",
        "        indice_sensacionalismo = sum(\n",
        "            len(re.findall(r'\\b' + re.escape(palavra) + r'\\b', texto.lower())) * peso\n",
        "            for palavra, peso in self.palavras_sensacionalistas.items()\n",
        "        )\n",
        "        resultado[\"sensacionalismo\"] = indice_sensacionalismo\n",
        "\n",
        "        # Normalizar por tamanho do texto (por 1000 palavras)\n",
        "        palavras_totais = len(texto.split())\n",
        "        if palavras_totais > 0:\n",
        "            indice_sensacionalismo_norm = (indice_sensacionalismo * 1000) / palavras_totais\n",
        "            resultado[\"sensacionalismo_normalizado\"] = indice_sensacionalismo_norm\n",
        "\n",
        "            # Adicionar alerta se o índice for alto\n",
        "            if indice_sensacionalismo_norm > 10:\n",
        "                resultado[\"alerta_sensacionalismo\"] = \"Alto índice de linguagem sensacionalista\"\n",
        "            elif indice_sensacionalismo_norm > 5:\n",
        "                resultado[\"alerta_sensacionalismo\"] = \"Moderado índice de linguagem sensacionalista\"\n",
        "\n",
        "        # Análise de incerteza\n",
        "        indice_incerteza = sum(\n",
        "            len(re.findall(r'\\b' + re.escape(palavra) + r'\\b', texto.lower()))\n",
        "            for palavra in self.palavras_incerteza\n",
        "        )\n",
        "        resultado[\"incerteza\"] = indice_incerteza\n",
        "\n",
        "        if palavras_totais > 0:\n",
        "            indice_incerteza_norm = (indice_incerteza * 1000) / palavras_totais\n",
        "            resultado[\"incerteza_normalizada\"] = indice_incerteza_norm\n",
        "\n",
        "            if indice_incerteza_norm > 8:\n",
        "                resultado[\"alerta_incerteza\"] = \"Alto índice de expressões de incerteza\"\n",
        "\n",
        "        # Análise de opinatividade\n",
        "        indice_opiniao = sum(\n",
        "            len(re.findall(r'\\b' + re.escape(expressao) + r'\\b', texto.lower()))\n",
        "            for expressao in self.expressoes_opinativas\n",
        "        )\n",
        "        resultado[\"opinativo\"] = indice_opiniao\n",
        "\n",
        "        if palavras_totais > 0:\n",
        "            indice_opiniao_norm = (indice_opiniao * 1000) / palavras_totais\n",
        "            resultado[\"opinativo_normalizado\"] = indice_opiniao_norm\n",
        "\n",
        "            if indice_opiniao_norm > 5:\n",
        "                resultado[\"alerta_opiniao\"] = \"Alto índice de expressões opinativas\"\n",
        "\n",
        "        # Contagem de pontos de exclamação (indicador de sensacionalismo)\n",
        "        exclamacoes = texto.count('!')\n",
        "        resultado[\"exclamacoes\"] = exclamacoes\n",
        "\n",
        "        if palavras_totais > 0:\n",
        "            exclamacoes_norm = (exclamacoes * 1000) / palavras_totais\n",
        "            if exclamacoes_norm > 10:\n",
        "                resultado[\"alerta_exclamacoes\"] = \"Uso excessivo de pontos de exclamação\"\n",
        "\n",
        "        # Análise de sentimento usando TextBlob\n",
        "        try:\n",
        "            analise = TextBlob(texto)\n",
        "            resultado[\"sentimento_polaridade\"] = analise.sentiment.polarity  # -1 a 1 (negativo a positivo)\n",
        "            resultado[\"sentimento_subjetividade\"] = analise.sentiment.subjectivity  # 0 a 1 (objetivo a subjetivo)\n",
        "\n",
        "            # Categorização baseada na polaridade\n",
        "            if resultado[\"sentimento_polaridade\"] < -0.5:\n",
        "                resultado[\"sentimento_categoria\"] = \"muito negativo\"\n",
        "            elif resultado[\"sentimento_polaridade\"] < -0.1:\n",
        "                resultado[\"sentimento_categoria\"] = \"negativo\"\n",
        "            elif resultado[\"sentimento_polaridade\"] <= 0.1:\n",
        "                resultado[\"sentimento_categoria\"] = \"neutro\"\n",
        "            elif resultado[\"sentimento_polaridade\"] <= 0.5:\n",
        "                resultado[\"sentimento_categoria\"] = \"positivo\"\n",
        "            else:\n",
        "                resultado[\"sentimento_categoria\"] = \"muito positivo\"\n",
        "\n",
        "            # Alerta para alto nível de subjetividade em notícias que deveriam ser objetivas\n",
        "            if resultado[\"sentimento_subjetividade\"] > 0.6:\n",
        "                resultado[\"alerta_subjetividade\"] = \"Conteúdo altamente subjetivo\"\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Erro na análise de sentimento: {e}\")\n",
        "\n",
        "        return resultado\n",
        "\n",
        "    def verificar_imagens(self, imagens):\n",
        "        \"\"\"Analisa as imagens associadas ao artigo.\"\"\"\n",
        "        resultados = {}\n",
        "        if not imagens:\n",
        "            resultados[\"alerta_imagens\"] = \"Nenhuma imagem encontrada no artigo\"\n",
        "            return resultados\n",
        "\n",
        "        resultados[\"num_imagens\"] = len(imagens)\n",
        "\n",
        "        # Verificar descrições das imagens (alt text)\n",
        "        imagens_sem_descricao = sum(1 for img in imagens if not img.get('alt'))\n",
        "        resultados[\"imagens_sem_descricao\"] = imagens_sem_descricao\n",
        "\n",
        "        if imagens_sem_descricao == len(imagens) and len(imagens) > 0:\n",
        "            resultados[\"alerta_imagens_descricao\"] = \"Nenhuma das imagens possui descrição adequada\"\n",
        "\n",
        "        # Calcular hash básico das URLs de imagens para potencial comparação futura\n",
        "        hashes_urls = [hashlib.md5(img['url'].encode()).hexdigest()[:8] for img in imagens]\n",
        "        resultados[\"hashes_imagens\"] = hashes_urls\n",
        "\n",
        "        # Analisar metadados das primeiras 5 imagens (limitar para evitar muito processamento)\n",
        "        try:\n",
        "            with ThreadPoolExecutor(max_workers=3) as executor:\n",
        "                metadados = list(executor.map(self.extrair_metadados_imagem, [img['url'] for img in imagens[:5]]))\n",
        "                resultados[\"metadados_imagens\"] = [m for m in metadados if m]  # Filtrar None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro ao analisar metadados das imagens: {e}\")\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def extrair_metadados_imagem(self, url_imagem):\n",
        "        \"\"\"Extrai metadados de uma imagem a partir da URL.\"\"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            response = requests.get(url_imagem, headers=headers, stream=True, timeout=5, verify=False)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Abrir a imagem com PIL\n",
        "            img = Image.open(io.BytesIO(response.content))\n",
        "\n",
        "            # Coletar metadados básicos\n",
        "            metadados = {\n",
        "                \"formato\": img.format,\n",
        "                \"tamanho\": img.size,\n",
        "                \"modo\": img.mode,\n",
        "                \"url\": url_imagem,\n",
        "                \"hash\": hashlib.md5(response.content).hexdigest()\n",
        "            }\n",
        "\n",
        "            # Tentar extrair EXIF se disponível\n",
        "            if hasattr(img, '_getexif') and img._getexif():\n",
        "                exif = img._getexif()\n",
        "                if exif:\n",
        "                    exif_data = {}\n",
        "                    for tag_id, value in exif.items():\n",
        "                        # Converter para string para garantir serialização JSON\n",
        "                        exif_data[str(tag_id)] = str(value)\n",
        "                    metadados[\"exif\"] = exif_data\n",
        "\n",
        "            return metadados\n",
        "        except (requests.RequestException, UnidentifiedImageError, IOError) as e:\n",
        "            logger.warning(f\"Erro ao extrair metadados da imagem {url_imagem}: {e}\")\n",
        "            return None\n",
        "\n",
        "class SistemaVerificacaoUnificado:\n",
        "    def __init__(self):\n",
        "        # Carregar variáveis do arquivo .env\n",
        "        load_dotenv()\n",
        "        self.fact_check_api_key = os.getenv(\"FACT_CHECK_API_KEY\")\n",
        "\n",
        "        # Verificar se a chave de API está disponível\n",
        "        if not self.fact_check_api_key or self.fact_check_api_key.strip() == \"\":\n",
        "            raise ValueError(\"Chave de API de verificação de fatos não encontrada. Verifique o arquivo .env.\")\n",
        "\n",
        "            return resultados\n",
        "                    try:\n",
        "            # Preparar termos de busca\n",
        "            query = titulo if titulo else texto[:100]\n",
        "                        # Parâmetros da consulta\n",
        "            params = {\n",
        "                'key': self.fact_check_api_key,\n",
        "                'query': query,\n",
        "                'languageCode': 'pt'  # Você pode ajustar ou detectar automaticamente\n",
        "            }\n",
        "\n",
        "            # Fazer a requisição\n",
        "            response = requests.get(\n",
        "                self.fact_check_api_url,\n",
        "                params=params\n",
        "            )\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                resultados[\"fact_checks\"] = data.get('claims', [])\n",
        "                resultados[\"num_fact_checks\"] = len(resultados[\"fact_checks\"])\n",
        "\n",
        "                # Analisar resultados\n",
        "                if resultados[\"num_fact_checks\"] > 0:\n",
        "                    # Verificar ratings médios\n",
        "                    ratings = []\n",
        "                    for claim in resultados[\"fact_checks\"]:\n",
        "                        for review in claim.get('claimReview', []):\n",
        "                            if 'textualRating' in review:\n",
        "                                ratings.append(review['textualRating'].lower())\n",
        "\n",
        "                    # Contar frequência de cada rating\n",
        "                    if ratings:\n",
        "                        counter = Counter(ratings)\n",
        "                        resultados[\"ratings_frequencia\"] = counter\n",
        "\n",
        "                        # Detectar termos comuns de falsidade\n",
        "                        termos_falso = ['false', 'fake', 'falso', 'mentira', 'enganoso', 'misleading']\n",
        "                        count_falso = sum(counter.get(termo, 0) for termo in termos_falso)\n",
        "\n",
        "                        if count_falso > 0:\n",
        "                            resultados[\"alerta_fact_check\"] = \"Conteúdo marcado como falso ou enganoso por verificadores de fatos\"\n",
        "                            resultados[\"pontos_fact_check\"] = -25\n",
        "                        else:\n",
        "                            resultados[\"pontos_fact_check\"] = 0\n",
        "            else:\n",
        "                logger.warning(f\"Erro na API de Fact Check. Status: {response.status_code}\")\n",
        "                resultados[\"erro_fact_check\"] = f\"Erro na API. Status: {response.status_code}\"\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro ao buscar fact checks: {e}\")\n",
        "            resultados[\"erro_fact_check\"] = str(e)\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def buscar_noticias_similares(self, titulo, keywords, conteudo=None):\n",
        "        \"\"\"Busca por notícias similares na web usando palavras-chave do artigo.\"\"\"\n",
        "        resultados = {}\n",
        "        try:\n",
        "            # Preparar termos de busca\n",
        "            termos_busca = titulo\n",
        "            if keywords:\n",
        "                termos_busca += \" \" + \" \".join(keywords[:3])  # Limitar a 3 keywords\n",
        "\n",
        "            # Simular resultados para demonstração\n",
        "            # Em uma implementação real, você usaria uma API como Google News ou similar\n",
        "            resultados[\"artigos_similares\"] = 5  # Simulando 5 artigos similares\n",
        "            resultados[\"similaridade\"] = \"média\"  # baixa, média, alta\n",
        "\n",
        "            # Simular consistência entre fontes\n",
        "            resultados[\"consistencia_entre_fontes\"] = \"alta\"  # baixa, média, alta\n",
        "            resultados[\"pontos_consistencia\"] = 10  # Simulando pontos por alta consistência\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro ao buscar notícias similares: {e}\")\n",
        "            resultados[\"erro_busca\"] = str(e)\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def converter_nivel_para_texto(self, nivel):\n",
        "        \"\"\"Converte um nível numérico de veracidade para texto.\"\"\"\n",
        "        nivel_texto = {\n",
        "            1: \"CONFIRMADO\",\n",
        "            2: \"PROVAVELMENTE VERDADEIRO\",\n",
        "            3: \"INCONCLUSIVO\",\n",
        "            4: \"PROVAVELMENTE FALSO\",\n",
        "            5: \"FALSO\"\n",
        "        }.get(nivel, \"INCONCLUSIVO\")\n",
        "        return nivel_texto\n",
        "\n",
        "    def analisar_credibilidade(self, artigo):\n",
        "        \"\"\"Análise combinada de credibilidade baseada em características do texto e fonte.\"\"\"\n",
        "        if not artigo or not artigo.get('conteudo'):\n",
        "            return {\"erro\": \"Artigo não disponível para análise\"}\n",
        "\n",
        "        resultados = {}\n",
        "        conteudo = artigo['conteudo']\n",
        "\n",
        "        # 1. Verificação de fonte/domínio\n",
        "        _, _, info_fonte = self.verificar_fonte(artigo['url'])\n",
        "        resultados.update(info_fonte)\n",
        "\n",
        "        # 2. Copiar informações básicas\n",
        "        resultados[\"titulo\"] = artigo['titulo']\n",
        "        resultados[\"data_publicacao\"] = artigo['data']\n",
        "        resultados[\"autores\"] = artigo['autores']\n",
        "        resultados[\"url\"] = artigo['url']\n",
        "\n",
        "        # 3. Análise básica do conteúdo\n",
        "        palavras = len(conteudo.split())\n",
        "        resultados[\"num_palavras\"] = palavras\n",
        "        if palavras < 100:\n",
        "            resultados[\"alerta_tamanho\"] = \"Texto muito curto, pode não ser um artigo completo\"\n",
        "\n",
        "        # 4. Análise detalhada da linguagem\n",
        "        analise_linguagem = self.analisar_linguagem(conteudo)\n",
        "        resultados.update(analise_linguagem)\n",
        "\n",
        "        # 5. Verificação de citações\n",
        "        citacoes = re.findall(r'\"([^\"]+)\"', conteudo)\n",
        "        resultados[\"num_citacoes\"] = len(citacoes)\n",
        "        if len(citacoes) == 0:\n",
        "            resultados[\"alerta_fontes\"] = \"Nenhuma citação direta encontrada\"\n",
        "\n",
        "        # 6. Verificação de URLs externos (referências)\n",
        "        urls = re.findall(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', conteudo)\n",
        "        resultados[\"num_urls\"] = len(urls)\n",
        "\n",
        "        # 7. Análise de imagens\n",
        "        resultados_imagens = self.verificar_imagens(artigo.get('imagens', []))\n",
        "        resultados.update(resultados_imagens)\n",
        "\n",
        "        # 8. Verificação cruzada com outras notícias\n",
        "        resultados_similares = self.buscar_noticias_similares(\n",
        "            artigo['titulo'],\n",
        "            artigo.get('keywords', []),\n",
        "            conteudo\n",
        "        )\n",
        "        resultados.update(resultados_similares)\n",
        "\n",
        "        # 9. Verificação em serviços de fact-checking\n",
        "        resultados_fact_check = self.buscar_fact_check(conteudo, artigo['titulo'])\n",
        "        resultados.update(resultados_fact_check)\n",
        "                # 10. Determinar um nível de veracidade\n",
        "        # Cálculo mais sofisticado usando vários fatores\n",
        "        base_pontuacao = 70  # Base inicial neutra\n",
        "                # Fatores que afetam a pontuação\n",
        "        pontuacao_fatores = [\n",
        "            resultados.get(\"pontos_fonte\", 0),\n",
        "            resultados.get(\"pontos_fact_check\", 0),\n",
        "            resultados.get(\"pontos_consistencia\", 0)\n",
        "        ]\n",
        "\n",
        "        # Penalidades\n",
        "        if \"alerta_tamanho\" in resultados:\n",
        "            pontuacao_fatores.append(-15)\n",
        "        if \"alerta_sensacionalismo\" in resultados:\n",
        "            if \"Alto índice\" in resultados[\"alerta_sensacionalismo\"]:\n",
        "                pontuacao_fatores.append(-20)\n",
        "            else:\n",
        "                pontuacao_fatores.append(-10)\n",
        "        if \"alerta_fontes\" in resultados:\n",
        "            pontuacao_fatores.append(-10)\n",
        "        if \"alerta_opiniao\" in resultados:\n",
        "            pontuacao_fatores.append(-15)\n",
        "        if \"alerta_incerteza\" in resultados and resultados[\"incerteza\"] > 3:\n",
        "            pontuacao_fatores.append(-10)\n",
        "        if \"alerta_exclamacoes\" in resultados:\n",
        "            pontuacao_fatores.append(-5)\n",
        "        if \"alerta_imagens_descricao\" in resultados:\n",
        "            pontuacao_fatores.append(-5)\n",
        "        if \"alerta_fact_check\" in resultados:\n",
        "            pontuacao_fatores.append(-25)\n",
        "        if resultados.get(\"sentimento_subjetividade\", 0) > 0.7:\n",
        "            pontuacao_fatores.append(-15)\n",
        "\n",
        "        # Bônus\n",
        "        if resultados.get(\"num_citacoes\", 0) > 3:\n",
        "            pontuacao_fatores.append(10)\n",
        "        elif resultados.get(\"num_citacoes\", 0) > 0:\n",
        "            pontuacao_fatores.append(5)\n",
        "        if resultados.get(\"num_urls\", 0) > 2:\n",
        "            pontuacao_fatores.append(5)\n",
        "        if resultados.get(\"num_imagens\", 0) > 1:\n",
        "            pontuacao_fatores.append(5)\n",
        "        if resultados.get(\"artigos_similares\", 0) > 3:\n",
        "            pontuacao_fatores.append(5)\n",
        "\n",
        "        # Calcular pontuação final\n",
        "        pontuacao_final = base_pontuacao + sum(pontuacao_fatores)\n",
        "        pontuacao_final = max(0, min(100, pontuacao_final))\n",
        "        resultados[\"pontuacao_credibilidade\"] = pontuacao_final\n",
        "# Converter para nível de veracidade\n",
        "nivel_veracidade = 3  # Default: Inconclusivo\n",
        "if pontuacao_final >= 95:\n",
        "    nivel_veracidade = 1  # Confirmado\n",
        "elif pontuacao_final >= 85:\n",
        "    nivel_veracidade = 2  # Provavelmente verdadeiro\n",
        "elif pontuacao_final <= 30:\n",
        "    nivel_veracidade = 4  # Provavelmente falso\n",
        "elif pontuacao_final < 85 and pontuacao_final > 30:\n",
        "    nivel_veracidade = 3  # Inconclusivo\n",
        "# Adicionar o nível de veracidade aos resultados\n",
        "resultados[\"nivel_veracidade\"] = nivel_veracidade\n",
        "# Exibir os resultados (opcional)\n",
        "print(f\"Pontuação de credibilidade: {resultados['pontuacao_credibilidade']}\")\n",
        "print(f\"Nível de veracidade: {resultados['nivel_veracidade']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTSOrBTGuujB",
        "outputId": "30c4b24f-b7c0-4b65-a64c-016afda54006"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting verificacao_simples.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run verificacao_simples.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEA6RvmWhPw2",
        "outputId": "c7c2cc53-aaef-4936-94e9-28b18750bc2e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analisando a URL: https://ultimosegundo.ig.com.br/mundo/2025-04-19/casa-branca--site-defende-teoria-de-que-coronavirus-vazou-de-laboratorio-na-china.html\n",
            "Verificador de Notícias Avançado inicializado!\n",
            "{'titulo': 'Trump defende que coronavírus teve origem em laboratório chinês', 'fonte_status': 'Fonte não categorizada', 'analise_linguagem': {'sensacionalismo': 0.0, 'incerteza': 0, 'opinativo': 0}, 'pontuacao': 50.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile verificacao_simples.py\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import ssl\n",
        "import urllib3\n",
        "import json\n",
        "from urllib.parse import urlparse\n",
        "import hashlib\n",
        "from collections import Counter\n",
        "import time\n",
        "\n",
        "# Desabilitar avisos SSL\n",
        "urllib3.disable_warnings()\n",
        "\n",
        "class VerificadorNoticiasAvancado:\n",
        "    def __init__(self):\n",
        "        print(\"Verificador de Notícias Avançado inicializado!\")\n",
        "        # Carregar listas de fontes confiáveis e não confiáveis\n",
        "        self.fontes_confiaveis = [\n",
        "            'g1.globo.com', 'bbc.com', 'bbc.co.uk', 'reuters.com', 'apnews.com',\n",
        "            'nytimes.com', 'washingtonpost.com', 'estadao.com.br', 'folha.uol.com.br',\n",
        "            'cnn.com', 'valor.globo.com', 'info.abril.com.br', 'oglobo.globo.com',\n",
        "            'dw.com', 'npr.org', 'tecmundo.com.br', 'canaltech.com.br', 'olhardigital.com.br'\n",
        "        ]\n",
        "        self.fontes_suspeitas = [\n",
        "            'theonion.com', 'sensacionalista.com.br', 'breitbart.com', 'infowars.com',\n",
        "            'clickhole.com', 'nationalreport.net', 'worldnewsdailyreport.com',\n",
        "            'dailycurrant.com', 'empirenews.net'\n",
        "        ]\n",
        "        # Palavras para análise linguística\n",
        "        self.palavras_sensacionalistas = {\n",
        "            \"chocante\": 2, \"inacreditável\": 2, \"surpreendente\": 1, \"sensacional\": 2,\n",
        "            \"impressionante\": 1, \"alarmante\": 1.5, \"exclusivo\": 1, \"urgente\": 1.5,\n",
        "            \"shocking\": 2, \"unbelievable\": 2, \"amazing\": 1, \"surprising\": 1,\n",
        "            \"imperdível\": 1.5, \"incrível\": 1.5, \"absurdo\": 1.5, \"escandaloso\": 2,\n",
        "            \"bomba\": 1.5, \"inédito\": 1, \"revelado\": 1, \"segredo\": 1.5,\n",
        "            \"polêmico\": 1.5, \"controverso\": 1, \"escândalo\": 1.5\n",
        "        }\n",
        "        self.palavras_incerteza = [\n",
        "            \"talvez\", \"possivelmente\", \"provavelmente\", \"pode ser\", \"especula-se\",\n",
        "            \"acredita-se\", \"sugere\", \"alega\", \"supostamente\", \"rumores\",\n",
        "            \"maybe\", \"possibly\", \"probably\", \"might be\", \"it is speculated\",\n",
        "            \"it is believed\", \"suggests\", \"allegedly\", \"supposedly\", \"rumors\"\n",
        "        ]\n",
        "        self.expressoes_opinativas = [\n",
        "            \"eu acho\", \"na minha opinião\", \"parece que\", \"acredito que\",\n",
        "            \"i think\", \"in my opinion\", \"it seems that\", \"i believe\"\n",
        "        ]\n",
        "\n",
        "    def extrair_dominio(self, url):\n",
        "        \"\"\"Extrai o domínio base da URL.\"\"\"\n",
        "        parsed_url = urlparse(url)\n",
        "        return parsed_url.netloc\n",
        "\n",
        "    def verificar_fonte(self, url):\n",
        "        \"\"\"Verifica o domínio da URL contra as listas de fontes.\"\"\"\n",
        "        dominio = self.extrair_dominio(url)\n",
        "        resultado = {}\n",
        "\n",
        "        # Verificar se o domínio está nas listas\n",
        "        if any(dominio.endswith(fonte) for fonte in self.fontes_confiaveis):\n",
        "            resultado[\"status_fonte\"] = \"confiável\"\n",
        "            resultado[\"pontos_fonte\"] = 20\n",
        "            return \"Fonte confiável\", 20\n",
        "        elif any(dominio.endswith(fonte) for fonte in self.fontes_suspeitas):\n",
        "            resultado[\"status_fonte\"] = \"suspeita\"\n",
        "            resultado[\"pontos_fonte\"] = -30\n",
        "            resultado[\"alerta_fonte\"] = \"Fonte conhecida por conteúdo satírico ou de baixa credibilidade\"\n",
        "            return \"Fonte suspeita\", -30\n",
        "        else:\n",
        "            resultado[\"status_fonte\"] = \"não categorizada\"\n",
        "            resultado[\"pontos_fonte\"] = 0\n",
        "            return \"Fonte não categorizada\", 0\n",
        "\n",
        "        resultado[\"dominio\"] = dominio\n",
        "        return resultado\n",
        "\n",
        "    def baixar_artigo(self, url):\n",
        "        \"\"\"Baixa o conteúdo do artigo a partir da URL.\"\"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers, verify=False, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Tenta extrair o título\n",
        "            titulo = soup.title.text.strip() if soup.title else \"Título não encontrado\"\n",
        "\n",
        "            # Tenta extrair o conteúdo do artigo (abordagem mais abrangente)\n",
        "            content = \"\"\n",
        "            # Procura por conteúdo em elementos comuns\n",
        "            for tag in ['article', 'main', '.content', '.post', '.entry', '#content', '.article-body', '.news-text']:\n",
        "                if tag.startswith('.') or tag.startswith('#'):\n",
        "                    elements = soup.select(tag)\n",
        "                else:\n",
        "                    elements = soup.find_all(tag)\n",
        "                if elements:\n",
        "                    for element in elements:\n",
        "                        content += element.get_text(separator=' ', strip=True) + \" \"\n",
        "                    break\n",
        "\n",
        "            # Se não encontrou conteúdo nos elementos específicos, pega todos os parágrafos\n",
        "            if not content:\n",
        "                paragraphs = soup.find_all('p')\n",
        "                content = ' '.join([p.get_text(strip=True) for p in paragraphs])\n",
        "\n",
        "            # Tenta encontrar a data\n",
        "            data = None\n",
        "            # Procura por metadados comuns\n",
        "            for meta in soup.find_all('meta'):\n",
        "                if meta.get('property') in ['article:published_time', 'og:published_time', 'publication_date']:\n",
        "                    data = meta.get('content')\n",
        "                    break\n",
        "\n",
        "            # Procura por elementos que possam conter autores\n",
        "            autores = []\n",
        "            for author_tag in ['author', 'byline', '.author', '.byline', '[rel=author]']:\n",
        "                if author_tag.startswith('.') or author_tag.startswith('['):\n",
        "                    elements = soup.select(author_tag)\n",
        "                else:\n",
        "                    elements = soup.find_all(author_tag)\n",
        "\n",
        "                if elements:\n",
        "                    for element in elements:\n",
        "                        autor = element.get_text(strip=True)\n",
        "                        if autor and len(autor) < 100:  # Evitar pegar conteúdo grande por engano\n",
        "                            autores.append(autor)\n",
        "\n",
        "            # Remover duplicações nos autores\n",
        "            autores = list(dict.fromkeys(autores))\n",
        "\n",
        "            # Extrair imagens\n",
        "            imagens = []\n",
        "            for img in soup.find_all('img'):\n",
        "                src = img.get('src')\n",
        "                alt = img.get('alt', '')\n",
        "                if src and not src.startswith('data:'):\n",
        "                    # Garantir URL completa\n",
        "                    if src.startswith('/'):\n",
        "                        parsed_url = urlparse(url)\n",
        "                        base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
        "                        src = base_url + src\n",
        "                    imagens.append({\n",
        "                        'url': src,\n",
        "                        'alt': alt\n",
        "                    })\n",
        "\n",
        "            # Extrair palavras-chave\n",
        "            keywords = []\n",
        "            meta_keywords = soup.find('meta', attrs={'name': 'keywords'})\n",
        "            if meta_keywords:\n",
        "                keywords = meta_keywords.get('content', '').split(',')\n",
        "                keywords = [k.strip() for k in keywords]\n",
        "\n",
        "            return {\n",
        "                'titulo': titulo,\n",
        "                'conteudo': content,\n",
        "                'data': data,\n",
        "                'autores': autores,\n",
        "                'url': response.url,  # Use a URL final após redirecionamentos\n",
        "                'imagens': imagens,\n",
        "                'keywords': keywords\n",
        "            }\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Erro ao baixar artigo ({url}): {e}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"Erro inesperado ao processar artigo ({url}): {e}\")\n",
        "            return None\n",
        "\n",
        "    def analisar_linguagem(self, texto):\n",
        "        \"\"\"Analisa o conteúdo em busca de sensacionalismo, incerteza e opinatividade.\"\"\"\n",
        "        indice_sensacionalismo = sum(\n",
        "            len(re.findall(r'\\b' + palavra + r'\\b', texto.lower())) * peso\n",
        "            for palavra, peso in self.palavras_sensacionalistas.items()\n",
        "        )\n",
        "        indice_incerteza = sum(\n",
        "            len(re.findall(r'\\b' + palavra + r'\\b', texto.lower())) for palavra in self.palavras_incerteza\n",
        "        )\n",
        "        indice_opiniao = sum(\n",
        "            len(re.findall(r'\\b' + expressao + r'\\b', texto.lower())) for expressao in self.expressoes_opinativas\n",
        "        )\n",
        "        return {\n",
        "            \"sensacionalismo\": indice_sensacionalismo,\n",
        "            \"incerteza\": indice_incerteza,\n",
        "            \"opinativo\": indice_opiniao\n",
        "        }\n",
        "\n",
        "    def verificar_imagens(self, imagens):\n",
        "        \"\"\"Analisa as imagens associadas ao artigo.\"\"\"\n",
        "        resultados = {}\n",
        "        if not imagens:\n",
        "            resultados[\"alerta_imagens\"] = \"Nenhuma imagem encontrada no artigo\"\n",
        "            return resultados\n",
        "        resultados[\"num_imagens\"] = len(imagens)\n",
        "        # Verificar descrições das imagens (alt text)\n",
        "        imagens_sem_descricao = sum(1 for img in imagens if not img.get('alt'))\n",
        "        resultados[\"imagens_sem_descricao\"] = imagens_sem_descricao\n",
        "        if imagens_sem_descricao == len(imagens) and len(imagens) > 0:\n",
        "            resultados[\"alerta_imagens_descricao\"] = \"Nenhuma das imagens possui descrição adequada\"\n",
        "        # Calcular hash básico das URLs de imagens para potencial comparação futura\n",
        "        hashes_urls = [hashlib.md5(img['url'].encode()).hexdigest()[:8] for img in imagens]\n",
        "        resultados[\"hashes_imagens\"] = hashes_urls\n",
        "        return resultados\n",
        "\n",
        "    def verificar_noticia(self, url):\n",
        "        \"\"\"Realiza a verificação completa da notícia.\"\"\"\n",
        "        artigo = self.baixar_artigo(url)\n",
        "        if not artigo:\n",
        "            return {\"erro\": \"Não foi possível processar o artigo.\"}\n",
        "\n",
        "        fonte_status, pontos_fonte = self.verificar_fonte(url)\n",
        "        analise_linguagem = self.analisar_linguagem(artigo[\"conteudo\"])\n",
        "        pontuacao = 50 + pontos_fonte - analise_linguagem[\"sensacionalismo\"] - analise_linguagem[\"incerteza\"]\n",
        "\n",
        "        return {\n",
        "            \"titulo\": artigo[\"titulo\"],\n",
        "            \"fonte_status\": fonte_status,\n",
        "            \"analise_linguagem\": analise_linguagem,\n",
        "            \"pontuacao\": max(0, min(100, pontuacao))\n",
        "        }\n",
        "\n",
        "    def buscar_noticias_similares(self, titulo, keywords):\n",
        "        \"\"\"Busca por notícias similares na web usando palavras-chave do artigo.\"\"\"\n",
        "        resultados = {}\n",
        "        try:\n",
        "            # Preparar termos de busca\n",
        "            termos_busca = titulo\n",
        "            if keywords:\n",
        "                termos_busca += \" \" + \" \".join(keywords[:3])  # Limitar a 3 keywords\n",
        "\n",
        "            # Simular resultados para demonstração\n",
        "            # Em uma implementação real, você usaria uma API como Google News ou similar\n",
        "            resultados[\"artigos_similares\"] = 5  # Simulando 5 artigos similares\n",
        "            resultados[\"similaridade\"] = \"média\"  # baixa, média, alta\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao buscar notícias similares: {e}\")\n",
        "            resultados[\"erro_busca\"] = str(e)\n",
        "        return resultados\n",
        "\n",
        "    def converter_nivel_para_texto(self, nivel):\n",
        "        \"\"\"Converte um nível numérico de veracidade para texto.\"\"\"\n",
        "        nivel_texto = {\n",
        "            1: \"CONFIRMADO\",\n",
        "            2: \"PROVAVELMENTE VERDADEIRO\",\n",
        "            3: \"INCONCLUSIVO\",\n",
        "            4: \"PROVAVELMENTE FALSO\",\n",
        "            5: \"FALSO\"\n",
        "        }.get(nivel, \"INCONCLUSIVO\")\n",
        "        return nivel_texto\n",
        "\n",
        "    def analisar_credibilidade(self, artigo):\n",
        "        \"\"\"Análise combinada de credibilidade baseada em características do texto e fonte.\"\"\"\n",
        "        if not artigo or not artigo.get('conteudo'):\n",
        "            return {\"erro\": \"Artigo não disponível para análise\"}\n",
        "\n",
        "        resultados = {}\n",
        "        conteudo = artigo['conteudo']\n",
        "\n",
        "        # 1. Verificação de fonte/domínio\n",
        "        info_fonte = self.verificar_fonte(artigo['url'])\n",
        "        resultados.update(info_fonte)\n",
        "\n",
        "        # 2. Copiar informações básicas\n",
        "        resultados[\"titulo\"] = artigo['titulo']\n",
        "        resultados[\"data_publicacao\"] = artigo['data']\n",
        "        resultados[\"autores\"] = artigo['autores']\n",
        "        resultados[\"url\"] = artigo['url']\n",
        "\n",
        "        # 3. Análise básica do conteúdo\n",
        "        palavras = len(conteudo.split())\n",
        "        resultados[\"num_palavras\"] = palavras\n",
        "        if palavras < 100:\n",
        "            resultados[\"alerta_tamanho\"] = \"Texto muito curto, pode não ser um artigo completo\"\n",
        "\n",
        "        # 4. Análise detalhada da linguagem\n",
        "        analise_linguagem = self.analisar_linguagem(conteudo)\n",
        "        resultados.update(analise_linguagem)\n",
        "\n",
        "        # 5. Verificação de citações\n",
        "        citacoes = re.findall(r'\"([^\"]+)\"', conteudo)\n",
        "        resultados[\"num_citacoes\"] = len(citacoes)\n",
        "        if len(citacoes) == 0:\n",
        "            resultados[\"alerta_fontes\"] = \"Nenhuma citação direta encontrada\"\n",
        "\n",
        "        # 6. Verificação de URLs externos (referências)\n",
        "        urls = re.findall(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', conteudo)\n",
        "        resultados[\"num_urls\"] = len(urls)\n",
        "\n",
        "        # 7. Análise de imagens\n",
        "        resultados_imagens = self.verificar_imagens(artigo.get('imagens', []))\n",
        "        resultados.update(resultados_imagens)\n",
        "\n",
        "        # 8. Verificação cruzada com outras notícias\n",
        "        resultados_similares = self.buscar_noticias_similares(\n",
        "            artigo['titulo'],\n",
        "            artigo.get('keywords', [])\n",
        "        )\n",
        "        resultados.update(resultados_similares)\n",
        "\n",
        "        # 9. Determinar um nível de veracidade (ESTA LÓGICA PRECISA SER IMPLEMENTADA)\n",
        "        nivel_de_veracidade_numerico = 3  # Exemplo: Artigo considerado inconclusivo\n",
        "        resultados[\"nivel_veracidade_numerico\"] = nivel_de_veracidade_numerico\n",
        "\n",
        "        # Converter o nível numérico para texto\n",
        "        resultados[\"nivel_veracidade\"] = self.converter_nivel_para_texto(nivel_de_veracidade_numerico)\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def verificar_url(self, url):\n",
        "        \"\"\"Função principal para verificar a credibilidade de uma notícia a partir da URL.\"\"\"\n",
        "        resultados = {\"url\": url}\n",
        "\n",
        "        # 1. Baixar o artigo\n",
        "        artigo = self.baixar_artigo(url)\n",
        "        if not artigo:\n",
        "            resultados[\"erro\"] = \"Não foi possível baixar o artigo.\"\n",
        "            return resultados\n",
        "\n",
        "        # 2. Analisar a credibilidade do artigo\n",
        "        analise_credibilidade = self.analisar_credibilidade(artigo)\n",
        "        resultados.update(analise_credibilidade)\n",
        "\n",
        "        return resultados\n",
        "\n",
        "\n",
        "def analisar_url(url):\n",
        "    \"\"\"Função para analisar uma URL específica.\"\"\"\n",
        "    verificador = VerificadorNoticiasAvancado()\n",
        "    resultados = verificador.verificar_url(url)\n",
        "    return resultados\n",
        "\n",
        "\n",
        "# Exemplo de uso (para testar no mesmo arquivo)\n",
        "def verificar_url(url_para):\n",
        "    # Aqui, insira o código da função `verificar_url` ou chame o método da sua classe\n",
        "    print(f\"Analisando a URL: {url_para}\")\n",
        "    # Você pode utilizar o método da classe `VerificadorNoticiasAvancado`\n",
        "    verificador = VerificadorNoticiasAvancado()\n",
        "    resultado = verificador.verificar_noticia(url_para)\n",
        "    print(resultado)  # Mostre o relatório de análise ou como você preferir formatar a saída\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    url_para_analisar = \"https://ultimosegundo.ig.com.br/mundo/2025-04-19/casa-branca--site-defende-teoria-de-que-coronavirus-vazou-de-laboratorio-na-china.html\"\n",
        "    verificar_url(url_para_analisar)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XueASsXPhFmc",
        "outputId": "9df44641-f6bb-40e1-8bae-30aa06fa2c83"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting verificacao_simples.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run verificacao_simples.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "xOJxbrg4ez-v",
        "outputId": "3bc75b5d-bda1-45ca-8e81-b3b224323189"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unindent does not match any outer indentation level (verificacao_simples.py, line 48)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/content/verificacao_simples.py\"\u001b[0;36m, line \u001b[0;32m48\u001b[0m\n\u001b[0;31m    def extrair_dominio(self, url):\u001b[0m\n\u001b[0m                                   ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile verificacao_simples.py\n",
        "# verificacao_simples.py\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import ssl\n",
        "import urllib3\n",
        "import json\n",
        "from urllib.parse import urlparse\n",
        "import hashlib\n",
        "from collections import Counter\n",
        "import time\n",
        "# Desabilitar avisos SSL\n",
        "urllib3.disable_warnings()\n",
        "class VerificadorNoticiasAvancado:\n",
        "    def __init__(self):\n",
        "        print(\"Verificador de Notícias Avançado inicializado!\")\n",
        "           # Carregar listas de fontes confiáveis e não confiáveis\n",
        "        self.fontes_confiaveis = [\n",
        "            'g1.globo.com', 'bbc.com', 'bbc.co.uk', 'reuters.com', 'apnews.com',\n",
        "            'nytimes.com', 'washingtonpost.com', 'estadao.com.br', 'folha.uol.com.br',\n",
        "            'cnn.com', 'valor.globo.com', 'info.abril.com.br', 'oglobo.globo.com',\n",
        "            'dw.com', 'npr.org', 'tecmundo.com.br', 'canaltech.com.br', 'olhardigital.com.br'\n",
        "        ]\n",
        "        self.fontes_suspeitas = [\n",
        "            'theonion.com', 'sensacionalista.com.br', 'breitbart.com', 'infowars.com',\n",
        "            'clickhole.com', 'nationalreport.net', 'worldnewsdailyreport.com',\n",
        "            'dailycurrant.com', 'empirenews.net'\n",
        "        ]\n",
        "        # Palavras para análise linguística\n",
        "        self.palavras_sensacionalistas = {\n",
        "            \"chocante\": 2, \"inacreditável\": 2, \"surpreendente\": 1, \"sensacional\": 2,\n",
        "            \"impressionante\": 1, \"alarmante\": 1.5, \"exclusivo\": 1, \"urgente\": 1.5,\n",
        "            \"shocking\": 2, \"unbelievable\": 2, \"amazing\": 1, \"surprising\": 1,\n",
        "            \"imperdível\": 1.5, \"incrível\": 1.5, \"absurdo\": 1.5, \"escandaloso\": 2,\n",
        "            \"bomba\": 1.5, \"inédito\": 1, \"revelado\": 1, \"segredo\": 1.5,\n",
        "            \"polêmico\": 1.5, \"controverso\": 1, \"escândalo\": 1.5\n",
        "        }\n",
        "        self.palavras_incerteza = [\n",
        "            \"talvez\", \"possivelmente\", \"provavelmente\", \"pode ser\", \"especula-se\",\n",
        "            \"acredita-se\", \"sugere\", \"alega\", \"supostamente\", \"rumores\",\n",
        "            \"maybe\", \"possibly\", \"probably\", \"might be\", \"it is speculated\",\n",
        "            \"it is believed\", \"suggests\", \"allegedly\", \"supposedly\", \"rumors\"\n",
        "        ]\n",
        "        self.expressoes_opinativas = [\n",
        "            \"eu acho\", \"na minha opinião\", \"parece que\", \"acredito que\",\n",
        "            \"i think\", \"in my opinion\", \"it seems that\", \"i believe\"\n",
        "        ]\n",
        " def extrair_dominio(self, url):\n",
        "        \"\"\"Extrai o domínio base da URL.\"\"\"\n",
        "        parsed_url = urlparse(url)\n",
        "        return parsed_url.netloc\n",
        "\n",
        "\n",
        "\n",
        " def verificar_fonte(self, url):\n",
        "        \"\"\"Verifica o domínio da URL contra as listas de fontes.\"\"\"\n",
        "        dominio = self.extrair_dominio(url)\n",
        "        if any(dominio.endswith(fonte) for fonte in self.fontes_confiaveis):\n",
        "            return \"Fonte confiável\", 20\n",
        "        elif any(dominio.endswith(fonte) for fonte in self.fontes_suspeitas):\n",
        "            return \"Fonte suspeita\", -30\n",
        "        else:\n",
        "            return \"Fonte não categorizada\", 0\n",
        "        # Verificar se o domínio está nas listas\n",
        "        if any(dominio.endswith(fonte) for fonte in self.fontes_confiaveis):\n",
        "            resultado[\"status_fonte\"] = \"confiável\"\n",
        "            resultado[\"pontos_fonte\"] = 20\n",
        "        elif any(dominio.endswith(fonte) for fonte in self.fontes_suspeitas):\n",
        "            resultado[\"status_fonte\"] = \"suspeita\"\n",
        "            resultado[\"pontos_fonte\"] = -30\n",
        "            resultado[\"alerta_fonte\"] = \"Fonte conhecida por conteúdo satírico ou de baixa credibilidade\"\n",
        "        else:\n",
        "            resultado[\"status_fonte\"] = \"não categorizada\"\n",
        "            resultado[\"pontos_fonte\"] = 0\n",
        "        resultado[\"dominio\"] = dominio\n",
        "        return resultado\n",
        "   def baixar_artigo(self, url):\n",
        "        \"\"\"Baixa o conteúdo do artigo a partir da URL.\"\"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers, verify=False, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            titulo = soup.title.text.strip() if soup.title else \"Título não encontrado\"\n",
        "            conteudo = ' '.join([p.get_text(strip=True) for p in soup.find_all('p')])\n",
        "            return {\"titulo\": titulo, \"conteudo\": conteudo}\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao baixar artigo: {e}\")\n",
        "            return None\n",
        "            # Tenta extrair o título\n",
        "            titulo = soup.title.text.strip() if soup.title else \"Título não encontrado\"\n",
        "            # Tenta extrair o conteúdo do artigo (abordagem mais abrangente)\n",
        "            content = \"\"\n",
        "            # Procura por conteúdo em elementos comuns\n",
        "            for tag in ['article', 'main', '.content', '.post', '.entry', '#content', '.article-body', '.news-text']:\n",
        "                if tag.startswith('.') or tag.startswith('#'):\n",
        "                    elements = soup.select(tag)\n",
        "                else:\n",
        "                    elements = soup.find_all(tag)\n",
        "                if elements:\n",
        "                    for element in elements:\n",
        "                        content += element.get_text(separator=' ', strip=True) + \" \"\n",
        "                    break\n",
        "\n",
        "            # Se não encontrou conteúdo nos elementos específicos, pega todos os parágrafos\n",
        "            if not content:\n",
        "                paragraphs = soup.find_all('p')\n",
        "                content = ' '.join([p.get_text(strip=True) for p in paragraphs])\n",
        "            # Tenta encontrar a data\n",
        "            data = None\n",
        "            # Procura por metadados comuns\n",
        "            for meta in soup.find_all('meta'):\n",
        "                if meta.get('property') in ['article:published_time', 'og:published_time', 'publication_date']:\n",
        "                    data = meta.get('content')\n",
        "                    break\n",
        "            # Procura por elementos que possam conter autores\n",
        "            autores = []\n",
        "            for author_tag in ['author', 'byline', '.author', '.byline', '[rel=author]']:\n",
        "                if author_tag.startswith('.') or author_tag.startswith('['):\n",
        "                    elements = soup.select(tag)\n",
        "                else:\n",
        "                    elements = soup.find_all(tag)\n",
        "\n",
        "                if elements:\n",
        "                    for element in elements:\n",
        "                        autor = element.get_text(strip=True)\n",
        "                        if autor and len(autor) < 100:  # Evitar pegar conteúdo grande por engano\n",
        "                            autores.append(autor)\n",
        "            # Remover duplicações nos autores\n",
        "            autores = list(dict.fromkeys(autores))\n",
        "\n",
        "            # Extrair imagens\n",
        "            imagens = []\n",
        "            for img in soup.find_all('img'):\n",
        "                src = img.get('src')\n",
        "                alt = img.get('alt', '')\n",
        "                if src and not src.startswith('data:'):\n",
        "                    # Garantir URL completa\n",
        "                    if src.startswith('/'):\n",
        "                        parsed_url = urlparse(url)\n",
        "                        base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
        "                        src = base_url + src\n",
        "                    imagens.append({\n",
        "                        'url': src,\n",
        "                        'alt': alt\n",
        "                    })\n",
        "            # Extrair palavras-chave\n",
        "            keywords = []\n",
        "            meta_keywords = soup.find('meta', attrs={'name': 'keywords'})\n",
        "            if meta_keywords:\n",
        "                keywords = meta_keywords.get('content', '').split(',')\n",
        "                keywords = [k.strip() for k in keywords]\n",
        "            return {\n",
        "                'titulo': titulo,\n",
        "                'conteudo': content,\n",
        "                'data': data,\n",
        "                'autores': autores,\n",
        "                'url': response.url,  # Use a URL final após redirecionamentos\n",
        "                'imagens': imagens,\n",
        "                'keywords': keywords\n",
        "            }\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Erro ao baixar artigo ({url}): {e}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"Erro inesperado ao processar artigo ({url}): {e}\")\n",
        "            return None\n",
        "\n",
        "def analisar_linguagem(self, texto):\n",
        "        \"\"\"Analisa o conteúdo em busca de sensacionalismo, incerteza e opinatividade.\"\"\"\n",
        "        indice_sensacionalismo = sum(\n",
        "            len(re.findall(r'\\b' + palavra + r'\\b', texto.lower())) * peso\n",
        "            for palavra, peso in self.palavras_sensacionalistas.items()\n",
        "        )\n",
        "        indice_incerteza = sum(\n",
        "            len(re.findall(r'\\b' + palavra + r'\\b', texto.lower())) for palavra in self.palavras_incerteza\n",
        "        )\n",
        "        indice_opiniao = sum(\n",
        "            len(re.findall(r'\\b' + expressao + r'\\b', texto.lower())) for expressao in self.expressoes_opinativas\n",
        "        )\n",
        "        return {\n",
        "            \"sensacionalismo\": indice_sensacionalismo,\n",
        "            \"incerteza\": indice_incerteza,\n",
        "            \"opinativo\": indice_opiniao\n",
        "        }\n",
        "\n",
        "\n",
        "    def verificar_imagens(self, imagens):\n",
        "        \"\"\"Analisa as imagens associadas ao artigo.\"\"\"\n",
        "        resultados = {}\n",
        "        if not imagens:\n",
        "            resultados[\"alerta_imagens\"] = \"Nenhuma imagem encontrada no artigo\"\n",
        "            return resultados\n",
        "        resultados[\"num_imagens\"] = len(imagens)\n",
        "        # Verificar descrições das imagens (alt text)\n",
        "        imagens_sem_descricao = sum(1 for img in imagens if not img.get('alt'))\n",
        "        resultados[\"imagens_sem_descricao\"] = imagens_sem_descricao\n",
        "        if imagens_sem_descricao == len(imagens) and len(imagens) > 0:\n",
        "            resultados[\"alerta_imagens_descricao\"] = \"Nenhuma das imagens possui descrição adequada\"\n",
        "        # Calcular hash básico das URLs de imagens para potencial comparação futura\n",
        "        hashes_urls = [hashlib.md5(img['url'].encode()).hexdigest()[:8] for img in imagens]\n",
        "        resultados[\"hashes_imagens\"] = hashes_urls\n",
        "        return resultados\n",
        "def verificar_noticia(self, url):\n",
        "        \"\"\"Realiza a verificação completa da notícia.\"\"\"\n",
        "        artigo = self.baixar_artigo(url)\n",
        "        if not artigo:\n",
        "            return {\"erro\": \"Não foi possível processar o artigo.\"}\n",
        "                fonte_status, pontos_fonte = self.verificar_fonte(url)\n",
        "        analise_linguagem = self.analisar_linguagem(artigo[\"conteudo\"])\n",
        "        pontuacao = 50 + pontos_fonte - analise_linguagem[\"sensacionalismo\"] - analise_linguagem[\"incerteza\"]\n",
        "        return {\n",
        "            \"titulo\": artigo[\"titulo\"],\n",
        "            \"fonte_status\": fonte_status,\n",
        "            \"analise_linguagem\": analise_linguagem,\n",
        "            \"pontuacao\": max(0, min(100, pontuacao))\n",
        "        }\n",
        "    def buscar_noticias_similares(self, titulo, keywords):\n",
        "        \"\"\"Busca por notícias similares na web usando palavras-chave do artigo.\"\"\"\n",
        "        resultados = {}\n",
        "        try:\n",
        "            # Preparar termos de busca\n",
        "            termos_busca = titulo\n",
        "            if keywords:\n",
        "                termos_busca += \" \" + \" \".join(keywords[:3])  # Limitar a 3 keywords\n",
        "\n",
        "            # Simular resultados para demonstração\n",
        "            # Em uma implementação real, você usaria uma API como Google News ou similar\n",
        "            resultados[\"artigos_similares\"] = 5  # Simulando 5 artigos similares\n",
        "            resultados[\"similaridade\"] = \"média\"  # baixa, média, alta\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao buscar notícias similares: {e}\")\n",
        "            resultados[\"erro_busca\"] = str(e)\n",
        "        return resultados\n",
        "    def converter_nivel_para_texto(self, nivel):\n",
        "        \"\"\"Converte um nível numérico de veracidade para texto.\"\"\"\n",
        "        nivel_texto = {\n",
        "            1: \"CONFIRMADO\",\n",
        "            2: \"PROVAVELMENTE VERDADEIRO\",\n",
        "            3: \"INCONCLUSIVO\",\n",
        "            4: \"PROVAVELMENTE FALSO\",\n",
        "            5: \"FALSO\"\n",
        "        }.get(nivel, \"INCONCLUSIVO\")\n",
        "        return nivel_texto\n",
        "    def analisar_credibilidade(self, artigo):\n",
        "        \"\"\"Análise combinada de credibilidade baseada em características do texto e fonte.\"\"\"\n",
        "        if not artigo or not artigo.get('conteudo'):\n",
        "            return {\"erro\": \"Artigo não disponível para análise\"}\n",
        "        resultados = {}\n",
        "        conteudo = artigo['conteudo']\n",
        "        # 1. Verificação de fonte/domínio\n",
        "        info_fonte = self.verificar_fonte(artigo['url'])\n",
        "        resultados.update(info_fonte)\n",
        "        # 2. Copiar informações básicas\n",
        "        resultados[\"titulo\"] = artigo['titulo']\n",
        "        resultados[\"data_publicacao\"] = artigo['data']\n",
        "        resultados[\"autores\"] = artigo['autores']\n",
        "        resultados[\"url\"] = artigo['url']\n",
        "        # 3. Análise básica do conteúdo\n",
        "        palavras = len(conteudo.split())\n",
        "        resultados[\"num_palavras\"] = palavras\n",
        "        if palavras < 100:\n",
        "            resultados[\"alerta_tamanho\"] = \"Texto muito curto, pode não ser um artigo completo\"\n",
        "        # 4. Análise detalhada da linguagem\n",
        "        analise_linguagem = self.analisar_linguagem(conteudo)\n",
        "        resultados.update(analise_linguagem)\n",
        "        # 5. Verificação de citações\n",
        "        citacoes = re.findall(r'\"([^\"]+)\"', conteudo)\n",
        "        resultados[\"num_citacoes\"] = len(citacoes)\n",
        "        if len(citacoes) == 0:\n",
        "            resultados[\"alerta_fontes\"] = \"Nenhuma citação direta encontrada\"\n",
        "        # 6. Verificação de URLs externos (referências)\n",
        "        urls = re.findall(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', conteudo)\n",
        "        resultados[\"num_urls\"] = len(urls)\n",
        "        # 7. Análise de imagens\n",
        "        resultados_imagens = self.verificar_imagens(artigo.get('imagens', []))\n",
        "        resultados.update(resultados_imagens)\n",
        "        # 8. Verificação cruzada com outras notícias\n",
        "        resultados_similares = self.buscar_noticias_similares(\n",
        "            artigo['titulo'],\n",
        "            artigo.get('keywords', [])\n",
        "        )\n",
        "        resultados.update(resultados_similares)\n",
        "        # 9. Determinar um nível de veracidade (ESTA LÓGICA PRECISA SER IMPLEMENTADA)\n",
        "        nivel_de_veracidade_numerico = 3  # Exemplo: Artigo considerado inconclusivo\n",
        "        resultados[\"nivel_veracidade_numerico\"] = nivel_de_veracidade_numerico\n",
        "        # Converter o nível numérico para texto\n",
        "        resultados[\"nivel_veracidade\"] = self.converter_nivel_para_texto(nivel_de_veracidade_numerico)\n",
        "        return resultados\n",
        "    def verificar_url(self, url):\n",
        "        \"\"\"Função principal para verificar a credibilidade de uma notícia a partir da URL.\"\"\"\n",
        "        resultados = {\"url\": url}\n",
        "        # 1. Baixar o artigo\n",
        "        artigo = self.baixar_artigo(url)\n",
        "        if not artigo:\n",
        "            resultados[\"erro\"] = \"Não foi possível baixar o artigo.\"\n",
        "            return resultados\n",
        "        # 2. Analisar a credibilidade do artigo\n",
        "        analise_credibilidade = self.analisar_credibilidade(artigo)\n",
        "        resultados.update(analise_credibilidade)\n",
        "        return resultados\n",
        "def analisar_url(url):\n",
        "    \"\"\"Função para analisar uma URL específica.\"\"\"\n",
        "    verificador = VerificadorNoticiasAvancado()\n",
        "    resultados = verificador.verificar_url(url)\n",
        "    return resultados\n",
        "# Exemplo de uso (para testar no mesmo arquivo)\n",
        "def verificar_url(url_para):\n",
        "    # Aqui, insira o código da função `verificar_url` ou chame o método da sua classe\n",
        "    print(f\"Analisando a URL: {url_para}\")\n",
        "    # Você pode utilizar o método da classe `VerificadorNoticiasAvancado`\n",
        "    verificador = VerificadorNoticiasAvancado()\n",
        "    resultado = verificador.verificar_noticia(url_para)\n",
        "    print(resultado)  # Mostre o relatório de análise ou como você preferir formatar a saída\n",
        "if __name__ == \"__main__\":\n",
        "    url_para_analisar = \"https://ultimosegundo.ig.com.br/mundo/2025-04-19/casa-branca--site-defende-teoria-de-que-coronavirus-vazou-de-laboratorio-na-china.html\"\n",
        "    verificar_url(url_para_analisar)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1TJ8Bzxervn",
        "outputId": "08f83a9a-1bbf-453b-da8f-3a8fdbd8bb96"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting verificacao_simples.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run verificacao_simples.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "qEf7p1cUWU2-",
        "outputId": "3df70a5a-93dc-49f1-aebe-bb445d3b701b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analisando a URL: https://ultimosegundo.ig.com.br/mundo/2025-04-19/casa-branca--site-defende-teoria-de-que-coronavirus-vazou-de-laboratorio-na-china.html\n",
            "Verificador de Notícias Avançado inicializado!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'VerificadorNoticiasAvancado' object has no attribute 'verificar_noticia'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/content/verificacao_simples.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0murl_para_analisar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://ultimosegundo.ig.com.br/mundo/2025-04-19/casa-branca--site-defende-teoria-de-que-coronavirus-vazou-de-laboratorio-na-china.html\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m     \u001b[0mverificar_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_para_analisar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/verificacao_simples.py\u001b[0m in \u001b[0;36mverificar_url\u001b[0;34m(url_para)\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Você pode utilizar o método da classe `VerificadorNoticiasAvancado`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0mverificador\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVerificadorNoticiasAvancado\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m     \u001b[0mresultado\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverificador\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverificar_noticia\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_para\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresultado\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Mostre o relatório de análise ou como você preferir formatar a saída\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'VerificadorNoticiasAvancado' object has no attribute 'verificar_noticia'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile verificacao_simples.py\n",
        "# verificacao_simples.py\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import ssl\n",
        "import urllib3\n",
        "import json\n",
        "from urllib.parse import urlparse\n",
        "import hashlib\n",
        "from collections import Counter\n",
        "import time\n",
        "\n",
        "# Desabilitar avisos SSL\n",
        "urllib3.disable_warnings()\n",
        "\n",
        "class VerificadorNoticiasAvancado:\n",
        "    def __init__(self):\n",
        "        print(\"Verificador de Notícias Avançado inicializado!\")\n",
        "        # Carregar listas de fontes confiáveis e não confiáveis\n",
        "        self.fontes_confiaveis = [\n",
        "            'g1.globo.com', 'bbc.com', 'bbc.co.uk', 'reuters.com', 'apnews.com',\n",
        "            'nytimes.com', 'washingtonpost.com', 'estadao.com.br', 'folha.uol.com.br',\n",
        "            'cnn.com', 'valor.globo.com', 'info.abril.com.br', 'oglobo.globo.com',\n",
        "            'dw.com', 'npr.org', 'tecmundo.com.br', 'canaltech.com.br', 'olhardigital.com.br'\n",
        "        ]\n",
        "\n",
        "        self.fontes_suspeitas = [\n",
        "            'theonion.com', 'sensacionalista.com.br', 'breitbart.com', 'infowars.com',\n",
        "            'clickhole.com', 'nationalreport.net', 'worldnewsdailyreport.com',\n",
        "            'dailycurrant.com', 'empirenews.net'\n",
        "        ]\n",
        "\n",
        "        # Palavras para análise linguística\n",
        "        self.palavras_sensacionalistas = {\n",
        "            \"chocante\": 2, \"inacreditável\": 2, \"surpreendente\": 1, \"sensacional\": 2,\n",
        "            \"impressionante\": 1, \"alarmante\": 1.5, \"exclusivo\": 1, \"urgente\": 1.5,\n",
        "            \"shocking\": 2, \"unbelievable\": 2, \"amazing\": 1, \"surprising\": 1,\n",
        "            \"imperdível\": 1.5, \"incrível\": 1.5, \"absurdo\": 1.5, \"escandaloso\": 2,\n",
        "            \"bomba\": 1.5, \"inédito\": 1, \"revelado\": 1, \"segredo\": 1.5,\n",
        "            \"polêmico\": 1.5, \"controverso\": 1, \"escândalo\": 1.5\n",
        "        }\n",
        "\n",
        "        self.palavras_incerteza = [\n",
        "            \"talvez\", \"possivelmente\", \"provavelmente\", \"pode ser\", \"especula-se\",\n",
        "            \"acredita-se\", \"sugere\", \"alega\", \"supostamente\", \"rumores\",\n",
        "            \"maybe\", \"possibly\", \"probably\", \"might be\", \"it is speculated\",\n",
        "            \"it is believed\", \"suggests\", \"allegedly\", \"supposedly\", \"rumors\"\n",
        "        ]\n",
        "\n",
        "        self.expressoes_opinativas = [\n",
        "            \"eu acho\", \"na minha opinião\", \"parece que\", \"acredito que\",\n",
        "            \"i think\", \"in my opinion\", \"it seems that\", \"i believe\"\n",
        "        ]\n",
        "\n",
        "    def extrair_dominio(self, url):\n",
        "        \"\"\"Extrai o domínio base da URL.\"\"\"\n",
        "        parsed_url = urlparse(url)\n",
        "        domain = parsed_url.netloc\n",
        "        return domain\n",
        "\n",
        "    def verificar_fonte(self, url):\n",
        "        \"\"\"Verifica o domínio da URL contra as listas de fontes.\"\"\"\n",
        "        dominio = self.extrair_dominio(url)\n",
        "        resultado = {}\n",
        "\n",
        "        # Verificar se o domínio está nas listas\n",
        "        if any(dominio.endswith(fonte) for fonte in self.fontes_confiaveis):\n",
        "            resultado[\"status_fonte\"] = \"confiável\"\n",
        "            resultado[\"pontos_fonte\"] = 20\n",
        "        elif any(dominio.endswith(fonte) for fonte in self.fontes_suspeitas):\n",
        "            resultado[\"status_fonte\"] = \"suspeita\"\n",
        "            resultado[\"pontos_fonte\"] = -30\n",
        "            resultado[\"alerta_fonte\"] = \"Fonte conhecida por conteúdo satírico ou de baixa credibilidade\"\n",
        "        else:\n",
        "            resultado[\"status_fonte\"] = \"não categorizada\"\n",
        "            resultado[\"pontos_fonte\"] = 0\n",
        "\n",
        "        resultado[\"dominio\"] = dominio\n",
        "        return resultado\n",
        "\n",
        "    def baixar_artigo(self, url):\n",
        "        \"\"\"Baixa e extrai o conteúdo básico de um artigo a partir da URL\"\"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers, verify=False, timeout=10, allow_redirects=True)\n",
        "            response.raise_for_status()  # Lança uma exceção para códigos de status HTTP ruins\n",
        "\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Tenta extrair o título\n",
        "            titulo = soup.title.text.strip() if soup.title else \"Título não encontrado\"\n",
        "\n",
        "            # Tenta extrair o conteúdo do artigo (abordagem mais abrangente)\n",
        "            content = \"\"\n",
        "\n",
        "            # Procura por conteúdo em elementos comuns\n",
        "            for tag in ['article', 'main', '.content', '.post', '.entry', '#content', '.article-body', '.news-text']:\n",
        "                if tag.startswith('.') or tag.startswith('#'):\n",
        "                    elements = soup.select(tag)\n",
        "                else:\n",
        "                    elements = soup.find_all(tag)\n",
        "\n",
        "                if elements:\n",
        "                    for element in elements:\n",
        "                        content += element.get_text(separator=' ', strip=True) + \" \"\n",
        "                    break\n",
        "\n",
        "            # Se não encontrou conteúdo nos elementos específicos, pega todos os parágrafos\n",
        "            if not content:\n",
        "                paragraphs = soup.find_all('p')\n",
        "                content = ' '.join([p.get_text(strip=True) for p in paragraphs])\n",
        "\n",
        "            # Tenta encontrar a data\n",
        "            data = None\n",
        "            # Procura por metadados comuns\n",
        "            for meta in soup.find_all('meta'):\n",
        "                if meta.get('property') in ['article:published_time', 'og:published_time', 'publication_date']:\n",
        "                    data = meta.get('content')\n",
        "                    break\n",
        "\n",
        "            # Procura por elementos que possam conter autores\n",
        "            autores = []\n",
        "            for author_tag in ['author', 'byline', '.author', '.byline', '[rel=author]']:\n",
        "                if author_tag.startswith('.') or author_tag.startswith('['):\n",
        "                    elements = soup.select(tag)\n",
        "                else:\n",
        "                    elements = soup.find_all(tag)\n",
        "\n",
        "                if elements:\n",
        "                    for element in elements:\n",
        "                        autor = element.get_text(strip=True)\n",
        "                        if autor and len(autor) < 100:  # Evitar pegar conteúdo grande por engano\n",
        "                            autores.append(autor)\n",
        "\n",
        "            # Remover duplicações nos autores\n",
        "            autores = list(dict.fromkeys(autores))\n",
        "\n",
        "            # Extrair imagens\n",
        "            imagens = []\n",
        "            for img in soup.find_all('img'):\n",
        "                src = img.get('src')\n",
        "                alt = img.get('alt', '')\n",
        "                if src and not src.startswith('data:'):\n",
        "                    # Garantir URL completa\n",
        "                    if src.startswith('/'):\n",
        "                        parsed_url = urlparse(url)\n",
        "                        base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
        "                        src = base_url + src\n",
        "                    imagens.append({\n",
        "                        'url': src,\n",
        "                        'alt': alt\n",
        "                    })\n",
        "\n",
        "            # Extrair palavras-chave\n",
        "            keywords = []\n",
        "            meta_keywords = soup.find('meta', attrs={'name': 'keywords'})\n",
        "            if meta_keywords:\n",
        "                keywords = meta_keywords.get('content', '').split(',')\n",
        "                keywords = [k.strip() for k in keywords]\n",
        "\n",
        "            return {\n",
        "                'titulo': titulo,\n",
        "                'conteudo': content,\n",
        "                'data': data,\n",
        "                'autores': autores,\n",
        "                'url': response.url,  # Use a URL final após redirecionamentos\n",
        "                'imagens': imagens,\n",
        "                'keywords': keywords\n",
        "            }\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Erro ao baixar artigo ({url}): {e}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"Erro inesperado ao processar artigo ({url}): {e}\")\n",
        "            return None\n",
        "\n",
        "    def analisar_linguagem(self, texto):\n",
        "        \"\"\"Analisa o tom e a linguagem usada no texto.\"\"\"\n",
        "        resultados = {}\n",
        "\n",
        "        # Análise de sensacionalismo com pesos\n",
        "        count_sensacionalismo = 0\n",
        "        palavras_encontradas = []\n",
        "\n",
        "        for palavra, peso in self.palavras_sensacionalistas.items():\n",
        "            ocorrencias = len(re.findall(r'\\b' + palavra + r'\\b', texto.lower()))\n",
        "            if ocorrencias > 0:\n",
        "                count_sensacionalismo += ocorrencias * peso\n",
        "                palavras_encontradas.append(f\"{palavra} ({ocorrencias}x)\")\n",
        "\n",
        "        resultados[\"indice_sensacionalismo\"] = count_sensacionalismo\n",
        "        resultados[\"palavras_sensacionalistas\"] = palavras_encontradas\n",
        "\n",
        "        if count_sensacionalismo > 3:\n",
        "            resultados[\"alerta_sensacionalismo\"] = \"Alto índice de linguagem sensacionalista detectado\"\n",
        "        elif count_sensacionalismo > 1.5:\n",
        "            resultados[\"alerta_sensacionalismo\"] = \"Linguagem potencialmente sensacionalista detectada\"\n",
        "\n",
        "        # Análise de incerteza\n",
        "        count_incerteza = 0\n",
        "        palavras_incerteza = []\n",
        "\n",
        "        for palavra in self.palavras_incerteza:\n",
        "            ocorrencias = len(re.findall(r'\\b' + palavra + r'\\b', texto.lower()))\n",
        "            if ocorrencias > 0:\n",
        "                count_incerteza += ocorrencias\n",
        "                palavras_incerteza.append(palavra)\n",
        "\n",
        "        resultados[\"indice_incerteza\"] = count_incerteza\n",
        "        resultados[\"palavras_incerteza\"] = palavras_incerteza\n",
        "\n",
        "        if count_incerteza > 3:\n",
        "            resultados[\"alerta_incerteza\"] = \"Alto uso de termos de incerteza detectado\"\n",
        "\n",
        "        # Análise de opinião\n",
        "        count_opiniao = 0\n",
        "        expressoes_opiniao = []\n",
        "\n",
        "        for expressao in self.expressoes_opinativas:\n",
        "            ocorrencias = len(re.findall(r'\\b' + expressao + r'\\b', texto.lower()))\n",
        "            if ocorrencias > 0:\n",
        "                count_opiniao += ocorrencias\n",
        "                expressoes_opiniao.append(expressao)\n",
        "\n",
        "        resultados[\"indice_opiniao\"] = count_opiniao\n",
        "        resultados[\"expressoes_opiniao\"] = expressoes_opiniao\n",
        "\n",
        "        if count_opiniao > 2:\n",
        "            resultados[\"alerta_opiniao\"] = \"Texto com forte caráter opinativo detectado\"\n",
        "\n",
        "        # Análise de exclamações (indicativo de sensacionalismo)\n",
        "        exclamacoes = len(re.findall(r'!', texto))\n",
        "        resultados[\"num_exclamacoes\"] = exclamacoes\n",
        "\n",
        "        if exclamacoes > 5:\n",
        "            resultados[\"alerta_exclamacoes\"] = \"Uso excessivo de pontos de exclamação\"\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def verificar_imagens(self, imagens):\n",
        "        \"\"\"Analisa as imagens associadas ao artigo.\"\"\"\n",
        "        resultados = {}\n",
        "\n",
        "        if not imagens:\n",
        "            resultados[\"alerta_imagens\"] = \"Nenhuma imagem encontrada no artigo\"\n",
        "            return resultados\n",
        "\n",
        "        resultados[\"num_imagens\"] = len(imagens)\n",
        "\n",
        "        # Verificar descrições das imagens (alt text)\n",
        "        imagens_sem_descricao = sum(1 for img in imagens if not img.get('alt'))\n",
        "        resultados[\"imagens_sem_descricao\"] = imagens_sem_descricao\n",
        "\n",
        "        if imagens_sem_descricao == len(imagens) and len(imagens) > 0:\n",
        "            resultados[\"alerta_imagens_descricao\"] = \"Nenhuma das imagens possui descrição adequada\"\n",
        "\n",
        "        # Calcular hash básico das URLs de imagens para potencial comparação futura\n",
        "        hashes_urls = [hashlib.md5(img['url'].encode()).hexdigest()[:8] for img in imagens]\n",
        "        resultados[\"hashes_imagens\"] = hashes_urls\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def buscar_noticias_similares(self, titulo, keywords):\n",
        "        \"\"\"Busca por notícias similares na web usando palavras-chave do artigo.\"\"\"\n",
        "        resultados = {}\n",
        "\n",
        "        try:\n",
        "            # Preparar termos de busca\n",
        "            termos_busca = titulo\n",
        "            if keywords:\n",
        "                termos_busca += \" \" + \" \".join(keywords[:3])  # Limitar a 3 keywords\n",
        "\n",
        "            # Simular resultados para demonstração\n",
        "            # Em uma implementação real, você usaria uma API como Google News ou similar\n",
        "            resultados[\"artigos_similares\"] = 5  # Simulando 5 artigos similares\n",
        "            resultados[\"similaridade\"] = \"média\"  # baixa, média, alta\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao buscar notícias similares: {e}\")\n",
        "            resultados[\"erro_busca\"] = str(e)\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def converter_nivel_para_texto(self, nivel):\n",
        "        \"\"\"Converte um nível numérico de veracidade para texto.\"\"\"\n",
        "        nivel_texto = {\n",
        "            1: \"CONFIRMADO\",\n",
        "            2: \"PROVAVELMENTE VERDADEIRO\",\n",
        "            3: \"INCONCLUSIVO\",\n",
        "            4: \"PROVAVELMENTE FALSO\",\n",
        "            5: \"FALSO\"\n",
        "        }.get(nivel, \"INCONCLUSIVO\")\n",
        "        return nivel_texto\n",
        "\n",
        "    def analisar_credibilidade(self, artigo):\n",
        "        \"\"\"Análise combinada de credibilidade baseada em características do texto e fonte.\"\"\"\n",
        "        if not artigo or not artigo.get('conteudo'):\n",
        "            return {\"erro\": \"Artigo não disponível para análise\"}\n",
        "\n",
        "        resultados = {}\n",
        "        conteudo = artigo['conteudo']\n",
        "\n",
        "        # 1. Verificação de fonte/domínio\n",
        "        info_fonte = self.verificar_fonte(artigo['url'])\n",
        "        resultados.update(info_fonte)\n",
        "\n",
        "        # 2. Copiar informações básicas\n",
        "        resultados[\"titulo\"] = artigo['titulo']\n",
        "        resultados[\"data_publicacao\"] = artigo['data']\n",
        "        resultados[\"autores\"] = artigo['autores']\n",
        "        resultados[\"url\"] = artigo['url']\n",
        "\n",
        "        # 3. Análise básica do conteúdo\n",
        "        palavras = len(conteudo.split())\n",
        "        resultados[\"num_palavras\"] = palavras\n",
        "        if palavras < 100:\n",
        "            resultados[\"alerta_tamanho\"] = \"Texto muito curto, pode não ser um artigo completo\"\n",
        "\n",
        "        # 4. Análise detalhada da linguagem\n",
        "        analise_linguagem = self.analisar_linguagem(conteudo)\n",
        "        resultados.update(analise_linguagem)\n",
        "\n",
        "        # 5. Verificação de citações\n",
        "        citacoes = re.findall(r'\"([^\"]+)\"', conteudo)\n",
        "        resultados[\"num_citacoes\"] = len(citacoes)\n",
        "\n",
        "        if len(citacoes) == 0:\n",
        "            resultados[\"alerta_fontes\"] = \"Nenhuma citação direta encontrada\"\n",
        "\n",
        "        # 6. Verificação de URLs externos (referências)\n",
        "        urls = re.findall(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', conteudo)\n",
        "        resultados[\"num_urls\"] = len(urls)\n",
        "\n",
        "        # 7. Análise de imagens\n",
        "        resultados_imagens = self.verificar_imagens(artigo.get('imagens', []))\n",
        "        resultados.update(resultados_imagens)\n",
        "\n",
        "        # 8. Verificação cruzada com outras notícias\n",
        "        resultados_similares = self.buscar_noticias_similares(\n",
        "            artigo['titulo'],\n",
        "            artigo.get('keywords', [])\n",
        "        )\n",
        "        resultados.update(resultados_similares)\n",
        "\n",
        "        # 9. Determinar um nível de veracidade (ESTA LÓGICA PRECISA SER IMPLEMENTADA)\n",
        "        nivel_de_veracidade_numerico = 3  # Exemplo: Artigo considerado inconclusivo\n",
        "        resultados[\"nivel_veracidade_numerico\"] = nivel_de_veracidade_numerico\n",
        "\n",
        "        # Converter o nível numérico para texto\n",
        "        resultados[\"nivel_veracidade\"] = self.converter_nivel_para_texto(nivel_de_veracidade_numerico)\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def verificar_url(self, url):\n",
        "        \"\"\"Função principal para verificar a credibilidade de uma notícia a partir da URL.\"\"\"\n",
        "        resultados = {\"url\": url}\n",
        "\n",
        "        # 1. Baixar o artigo\n",
        "        artigo = self.baixar_artigo(url)\n",
        "        if not artigo:\n",
        "            resultados[\"erro\"] = \"Não foi possível baixar o artigo.\"\n",
        "            return resultados\n",
        "\n",
        "        # 2. Analisar a credibilidade do artigo\n",
        "        analise_credibilidade = self.analisar_credibilidade(artigo)\n",
        "        resultados.update(analise_credibilidade)\n",
        "\n",
        "        return resultados\n",
        "\n",
        "def analisar_url(url):\n",
        "    \"\"\"Função para analisar uma URL específica.\"\"\"\n",
        "\n",
        "    verificador = VerificadorNoticiasAvancado()\n",
        "    resultados = verificador.verificar_url(url)\n",
        "\n",
        "    return resultados\n",
        "\n",
        "# Exemplo de uso (para testar no mesmo arquivo)\n",
        "def verificar_url(url_para):\n",
        "    # Aqui, insira o código da função `verificar_url` ou chame o método da sua classe\n",
        "    print(f\"Analisando a URL: {url_para}\")\n",
        "    # Você pode utilizar o método da classe `VerificadorNoticiasAvancado`\n",
        "    verificador = VerificadorNoticiasAvancado()\n",
        "    resultado = verificador.verificar_noticia(url_para)\n",
        "    print(resultado)  # Mostre o relatório de análise ou como você preferir formatar a saída\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    url_para_analisar = \"https://ultimosegundo.ig.com.br/mundo/2025-04-19/casa-branca--site-defende-teoria-de-que-coronavirus-vazou-de-laboratorio-na-china.html\"\n",
        "    verificar_url(url_para_analisar)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeEJJgpgWK7-",
        "outputId": "16e16688-5d0e-46a5-c054-af840eb85e56"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting verificacao_simples.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run verificacao_simples.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "PQAzXYuyQkOo",
        "outputId": "3ee2feb0-c222-48e5-f268-c236fea1a6f9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'url_para' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/content/verificacao_simples.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;31m# Exemplo de uso (para testar no mesmo arquivo)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m     \u001b[0murl_para\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'url_para' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile verificacao_simples.py\n",
        "# verificacao_simples.py\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import ssl\n",
        "import urllib3\n",
        "import json\n",
        "from urllib.parse import urlparse\n",
        "import hashlib\n",
        "from collections import Counter\n",
        "import time\n",
        "\n",
        "# Desabilitar avisos SSL\n",
        "urllib3.disable_warnings()\n",
        "\n",
        "class VerificadorNoticiasAvancado:\n",
        "    def __init__(self):\n",
        "        print(\"Verificador de Notícias Avançado inicializado!\")\n",
        "        # Carregar listas de fontes confiáveis e não confiáveis\n",
        "        self.fontes_confiaveis = [\n",
        "            'g1.globo.com', 'bbc.com', 'bbc.co.uk', 'reuters.com', 'apnews.com',\n",
        "            'nytimes.com', 'washingtonpost.com', 'estadao.com.br', 'folha.uol.com.br',\n",
        "            'cnn.com', 'valor.globo.com', 'info.abril.com.br', 'oglobo.globo.com',\n",
        "            'dw.com', 'npr.org', 'tecmundo.com.br', 'canaltech.com.br', 'olhardigital.com.br'\n",
        "        ]\n",
        "\n",
        "        self.fontes_suspeitas = [\n",
        "            'theonion.com', 'sensacionalista.com.br', 'breitbart.com', 'infowars.com',\n",
        "            'clickhole.com', 'nationalreport.net', 'worldnewsdailyreport.com',\n",
        "            'dailycurrant.com', 'empirenews.net'\n",
        "        ]\n",
        "\n",
        "        # Palavras para análise linguística\n",
        "        self.palavras_sensacionalistas = {\n",
        "            \"chocante\": 2, \"inacreditável\": 2, \"surpreendente\": 1, \"sensacional\": 2,\n",
        "            \"impressionante\": 1, \"alarmante\": 1.5, \"exclusivo\": 1, \"urgente\": 1.5,\n",
        "            \"shocking\": 2, \"unbelievable\": 2, \"amazing\": 1, \"surprising\": 1,\n",
        "            \"imperdível\": 1.5, \"incrível\": 1.5, \"absurdo\": 1.5, \"escandaloso\": 2,\n",
        "            \"bomba\": 1.5, \"inédito\": 1, \"revelado\": 1, \"segredo\": 1.5,\n",
        "            \"polêmico\": 1.5, \"controverso\": 1, \"escândalo\": 1.5\n",
        "        }\n",
        "\n",
        "        self.palavras_incerteza = [\n",
        "            \"talvez\", \"possivelmente\", \"provavelmente\", \"pode ser\", \"especula-se\",\n",
        "            \"acredita-se\", \"sugere\", \"alega\", \"supostamente\", \"rumores\",\n",
        "            \"maybe\", \"possibly\", \"probably\", \"might be\", \"it is speculated\",\n",
        "            \"it is believed\", \"suggests\", \"allegedly\", \"supposedly\", \"rumors\"\n",
        "        ]\n",
        "\n",
        "        self.expressoes_opinativas = [\n",
        "            \"eu acho\", \"na minha opinião\", \"parece que\", \"acredito que\",\n",
        "            \"i think\", \"in my opinion\", \"it seems that\", \"i believe\"\n",
        "        ]\n",
        "\n",
        "    def extrair_dominio(self, url):\n",
        "        \"\"\"Extrai o domínio base da URL.\"\"\"\n",
        "        parsed_url = urlparse(url)\n",
        "        domain = parsed_url.netloc\n",
        "        return domain\n",
        "\n",
        "    def verificar_fonte(self, url):\n",
        "        \"\"\"Verifica o domínio da URL contra as listas de fontes.\"\"\"\n",
        "        dominio = self.extrair_dominio(url)\n",
        "        resultado = {}\n",
        "\n",
        "        # Verificar se o domínio está nas listas\n",
        "        if any(dominio.endswith(fonte) for fonte in self.fontes_confiaveis):\n",
        "            resultado[\"status_fonte\"] = \"confiável\"\n",
        "            resultado[\"pontos_fonte\"] = 20\n",
        "        elif any(dominio.endswith(fonte) for fonte in self.fontes_suspeitas):\n",
        "            resultado[\"status_fonte\"] = \"suspeita\"\n",
        "            resultado[\"pontos_fonte\"] = -30\n",
        "            resultado[\"alerta_fonte\"] = \"Fonte conhecida por conteúdo satírico ou de baixa credibilidade\"\n",
        "        else:\n",
        "            resultado[\"status_fonte\"] = \"não categorizada\"\n",
        "            resultado[\"pontos_fonte\"] = 0\n",
        "\n",
        "        resultado[\"dominio\"] = dominio\n",
        "        return resultado\n",
        "\n",
        "    def baixar_artigo(self, url):\n",
        "        \"\"\"Baixa e extrai o conteúdo básico de um artigo a partir da URL\"\"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers, verify=False, timeout=10, allow_redirects=True)\n",
        "            response.raise_for_status()  # Lança uma exceção para códigos de status HTTP ruins\n",
        "\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Tenta extrair o título\n",
        "            titulo = soup.title.text.strip() if soup.title else \"Título não encontrado\"\n",
        "\n",
        "            # Tenta extrair o conteúdo do artigo (abordagem mais abrangente)\n",
        "            content = \"\"\n",
        "\n",
        "            # Procura por conteúdo em elementos comuns\n",
        "            for tag in ['article', 'main', '.content', '.post', '.entry', '#content', '.article-body', '.news-text']:\n",
        "                if tag.startswith('.') or tag.startswith('#'):\n",
        "                    elements = soup.select(tag)\n",
        "                else:\n",
        "                    elements = soup.find_all(tag)\n",
        "\n",
        "                if elements:\n",
        "                    for element in elements:\n",
        "                        content += element.get_text(separator=' ', strip=True) + \" \"\n",
        "                    break\n",
        "\n",
        "            # Se não encontrou conteúdo nos elementos específicos, pega todos os parágrafos\n",
        "            if not content:\n",
        "                paragraphs = soup.find_all('p')\n",
        "                content = ' '.join([p.get_text(strip=True) for p in paragraphs])\n",
        "\n",
        "            # Tenta encontrar a data\n",
        "            data = None\n",
        "            # Procura por metadados comuns\n",
        "            for meta in soup.find_all('meta'):\n",
        "                if meta.get('property') in ['article:published_time', 'og:published_time', 'publication_date']:\n",
        "                    data = meta.get('content')\n",
        "                    break\n",
        "\n",
        "            # Procura por elementos que possam conter autores\n",
        "            autores = []\n",
        "            for author_tag in ['author', 'byline', '.author', '.byline', '[rel=author]']:\n",
        "                if author_tag.startswith('.') or author_tag.startswith('['):\n",
        "                    elements = soup.select(tag)\n",
        "                else:\n",
        "                    elements = soup.find_all(tag)\n",
        "\n",
        "                if elements:\n",
        "                    for element in elements:\n",
        "                        autor = element.get_text(strip=True)\n",
        "                        if autor and len(autor) < 100:  # Evitar pegar conteúdo grande por engano\n",
        "                            autores.append(autor)\n",
        "\n",
        "            # Remover duplicações nos autores\n",
        "            autores = list(dict.fromkeys(autores))\n",
        "\n",
        "            # Extrair imagens\n",
        "            imagens = []\n",
        "            for img in soup.find_all('img'):\n",
        "                src = img.get('src')\n",
        "                alt = img.get('alt', '')\n",
        "                if src and not src.startswith('data:'):\n",
        "                    # Garantir URL completa\n",
        "                    if src.startswith('/'):\n",
        "                        parsed_url = urlparse(url)\n",
        "                        base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
        "                        src = base_url + src\n",
        "                    imagens.append({\n",
        "                        'url': src,\n",
        "                        'alt': alt\n",
        "                    })\n",
        "\n",
        "            # Extrair palavras-chave\n",
        "            keywords = []\n",
        "            meta_keywords = soup.find('meta', attrs={'name': 'keywords'})\n",
        "            if meta_keywords:\n",
        "                keywords = meta_keywords.get('content', '').split(',')\n",
        "                keywords = [k.strip() for k in keywords]\n",
        "\n",
        "            return {\n",
        "                'titulo': titulo,\n",
        "                'conteudo': content,\n",
        "                'data': data,\n",
        "                'autores': autores,\n",
        "                'url': response.url,  # Use a URL final após redirecionamentos\n",
        "                'imagens': imagens,\n",
        "                'keywords': keywords\n",
        "            }\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Erro ao baixar artigo ({url}): {e}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"Erro inesperado ao processar artigo ({url}): {e}\")\n",
        "            return None\n",
        "\n",
        "    def analisar_linguagem(self, texto):\n",
        "        \"\"\"Analisa o tom e a linguagem usada no texto.\"\"\"\n",
        "        resultados = {}\n",
        "\n",
        "        # Análise de sensacionalismo com pesos\n",
        "        count_sensacionalismo = 0\n",
        "        palavras_encontradas = []\n",
        "\n",
        "        for palavra, peso in self.palavras_sensacionalistas.items():\n",
        "            ocorrencias = len(re.findall(r'\\b' + palavra + r'\\b', texto.lower()))\n",
        "            if ocorrencias > 0:\n",
        "                count_sensacionalismo += ocorrencias * peso\n",
        "                palavras_encontradas.append(f\"{palavra} ({ocorrencias}x)\")\n",
        "\n",
        "        resultados[\"indice_sensacionalismo\"] = count_sensacionalismo\n",
        "        resultados[\"palavras_sensacionalistas\"] = palavras_encontradas\n",
        "\n",
        "        if count_sensacionalismo > 3:\n",
        "            resultados[\"alerta_sensacionalismo\"] = \"Alto índice de linguagem sensacionalista detectado\"\n",
        "        elif count_sensacionalismo > 1.5:\n",
        "            resultados[\"alerta_sensacionalismo\"] = \"Linguagem potencialmente sensacionalista detectada\"\n",
        "\n",
        "        # Análise de incerteza\n",
        "        count_incerteza = 0\n",
        "        palavras_incerteza = []\n",
        "\n",
        "        for palavra in self.palavras_incerteza:\n",
        "            ocorrencias = len(re.findall(r'\\b' + palavra + r'\\b', texto.lower()))\n",
        "            if ocorrencias > 0:\n",
        "                count_incerteza += ocorrencias\n",
        "                palavras_incerteza.append(palavra)\n",
        "\n",
        "        resultados[\"indice_incerteza\"] = count_incerteza\n",
        "        resultados[\"palavras_incerteza\"] = palavras_incerteza\n",
        "\n",
        "        if count_incerteza > 3:\n",
        "            resultados[\"alerta_incerteza\"] = \"Alto uso de termos de incerteza detectado\"\n",
        "\n",
        "        # Análise de opinião\n",
        "        count_opiniao = 0\n",
        "        expressoes_opiniao = []\n",
        "\n",
        "        for expressao in self.expressoes_opinativas:\n",
        "            ocorrencias = len(re.findall(r'\\b' + expressao + r'\\b', texto.lower()))\n",
        "            if ocorrencias > 0:\n",
        "                count_opiniao += ocorrencias\n",
        "                expressoes_opiniao.append(expressao)\n",
        "\n",
        "        resultados[\"indice_opiniao\"] = count_opiniao\n",
        "        resultados[\"expressoes_opiniao\"] = expressoes_opiniao\n",
        "\n",
        "        if count_opiniao > 2:\n",
        "            resultados[\"alerta_opiniao\"] = \"Texto com forte caráter opinativo detectado\"\n",
        "\n",
        "        # Análise de exclamações (indicativo de sensacionalismo)\n",
        "        exclamacoes = len(re.findall(r'!', texto))\n",
        "        resultados[\"num_exclamacoes\"] = exclamacoes\n",
        "\n",
        "        if exclamacoes > 5:\n",
        "            resultados[\"alerta_exclamacoes\"] = \"Uso excessivo de pontos de exclamação\"\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def verificar_imagens(self, imagens):\n",
        "        \"\"\"Analisa as imagens associadas ao artigo.\"\"\"\n",
        "        resultados = {}\n",
        "\n",
        "        if not imagens:\n",
        "            resultados[\"alerta_imagens\"] = \"Nenhuma imagem encontrada no artigo\"\n",
        "            return resultados\n",
        "\n",
        "        resultados[\"num_imagens\"] = len(imagens)\n",
        "\n",
        "        # Verificar descrições das imagens (alt text)\n",
        "        imagens_sem_descricao = sum(1 for img in imagens if not img.get('alt'))\n",
        "        resultados[\"imagens_sem_descricao\"] = imagens_sem_descricao\n",
        "\n",
        "        if imagens_sem_descricao == len(imagens) and len(imagens) > 0:\n",
        "            resultados[\"alerta_imagens_descricao\"] = \"Nenhuma das imagens possui descrição adequada\"\n",
        "\n",
        "        # Calcular hash básico das URLs de imagens para potencial comparação futura\n",
        "        hashes_urls = [hashlib.md5(img['url'].encode()).hexdigest()[:8] for img in imagens]\n",
        "        resultados[\"hashes_imagens\"] = hashes_urls\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def buscar_noticias_similares(self, titulo, keywords):\n",
        "        \"\"\"Busca por notícias similares na web usando palavras-chave do artigo.\"\"\"\n",
        "        resultados = {}\n",
        "\n",
        "        try:\n",
        "            # Preparar termos de busca\n",
        "            termos_busca = titulo\n",
        "            if keywords:\n",
        "                termos_busca += \" \" + \" \".join(keywords[:3])  # Limitar a 3 keywords\n",
        "\n",
        "            # Simular resultados para demonstração\n",
        "            # Em uma implementação real, você usaria uma API como Google News ou similar\n",
        "            resultados[\"artigos_similares\"] = 5  # Simulando 5 artigos similares\n",
        "            resultados[\"similaridade\"] = \"média\"  # baixa, média, alta\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao buscar notícias similares: {e}\")\n",
        "            resultados[\"erro_busca\"] = str(e)\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def converter_nivel_para_texto(self, nivel):\n",
        "        \"\"\"Converte um nível numérico de veracidade para texto.\"\"\"\n",
        "        nivel_texto = {\n",
        "            1: \"CONFIRMADO\",\n",
        "            2: \"PROVAVELMENTE VERDADEIRO\",\n",
        "            3: \"INCONCLUSIVO\",\n",
        "            4: \"PROVAVELMENTE FALSO\",\n",
        "            5: \"FALSO\"\n",
        "        }.get(nivel, \"INCONCLUSIVO\")\n",
        "        return nivel_texto\n",
        "\n",
        "    def analisar_credibilidade(self, artigo):\n",
        "        \"\"\"Análise combinada de credibilidade baseada em características do texto e fonte.\"\"\"\n",
        "        if not artigo or not artigo.get('conteudo'):\n",
        "            return {\"erro\": \"Artigo não disponível para análise\"}\n",
        "\n",
        "        resultados = {}\n",
        "        conteudo = artigo['conteudo']\n",
        "\n",
        "        # 1. Verificação de fonte/domínio\n",
        "        info_fonte = self.verificar_fonte(artigo['url'])\n",
        "        resultados.update(info_fonte)\n",
        "\n",
        "        # 2. Copiar informações básicas\n",
        "        resultados[\"titulo\"] = artigo['titulo']\n",
        "        resultados[\"data_publicacao\"] = artigo['data']\n",
        "        resultados[\"autores\"] = artigo['autores']\n",
        "        resultados[\"url\"] = artigo['url']\n",
        "\n",
        "        # 3. Análise básica do conteúdo\n",
        "        palavras = len(conteudo.split())\n",
        "        resultados[\"num_palavras\"] = palavras\n",
        "        if palavras < 100:\n",
        "            resultados[\"alerta_tamanho\"] = \"Texto muito curto, pode não ser um artigo completo\"\n",
        "\n",
        "        # 4. Análise detalhada da linguagem\n",
        "        analise_linguagem = self.analisar_linguagem(conteudo)\n",
        "        resultados.update(analise_linguagem)\n",
        "\n",
        "        # 5. Verificação de citações\n",
        "        citacoes = re.findall(r'\"([^\"]+)\"', conteudo)\n",
        "        resultados[\"num_citacoes\"] = len(citacoes)\n",
        "\n",
        "        if len(citacoes) == 0:\n",
        "            resultados[\"alerta_fontes\"] = \"Nenhuma citação direta encontrada\"\n",
        "\n",
        "        # 6. Verificação de URLs externos (referências)\n",
        "        urls = re.findall(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', conteudo)\n",
        "        resultados[\"num_urls\"] = len(urls)\n",
        "\n",
        "        # 7. Análise de imagens\n",
        "        resultados_imagens = self.verificar_imagens(artigo.get('imagens', []))\n",
        "        resultados.update(resultados_imagens)\n",
        "\n",
        "        # 8. Verificação cruzada com outras notícias\n",
        "        resultados_similares = self.buscar_noticias_similares(\n",
        "            artigo['titulo'],\n",
        "            artigo.get('keywords', [])\n",
        "        )\n",
        "        resultados.update(resultados_similares)\n",
        "\n",
        "        # 9. Determinar um nível de veracidade (ESTA LÓGICA PRECISA SER IMPLEMENTADA)\n",
        "        nivel_de_veracidade_numerico = 3  # Exemplo: Artigo considerado inconclusivo\n",
        "        resultados[\"nivel_veracidade_numerico\"] = nivel_de_veracidade_numerico\n",
        "\n",
        "        # Converter o nível numérico para texto\n",
        "        resultados[\"nivel_veracidade\"] = self.converter_nivel_para_texto(nivel_de_veracidade_numerico)\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def verificar_url(self, url):\n",
        "        \"\"\"Função principal para verificar a credibilidade de uma notícia a partir da URL.\"\"\"\n",
        "        resultados = {\"url\": url}\n",
        "\n",
        "        # 1. Baixar o artigo\n",
        "        artigo = self.baixar_artigo(url)\n",
        "        if not artigo:\n",
        "            resultados[\"erro\"] = \"Não foi possível baixar o artigo.\"\n",
        "            return resultados\n",
        "\n",
        "        # 2. Analisar a credibilidade do artigo\n",
        "        analise_credibilidade = self.analisar_credibilidade(artigo)\n",
        "        resultados.update(analise_credibilidade)\n",
        "\n",
        "        return resultados\n",
        "\n",
        "def analisar_url(url):\n",
        "    \"\"\"Função para analisar uma URL específica.\"\"\"\n",
        "    verificador = VerificadorNoticiasAvancado()\n",
        "    resultados = verificador.verificar_url(url)\n",
        "\n",
        "    return resultados\n",
        "\n",
        "# Exemplo de uso (para testar no mesmo arquivo)\n",
        "if __name__ == \"__main__\":\n",
        "    url_para"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLhuI7lbQY7u",
        "outputId": "c6cb3d5e-5af5-4e31-9cba-d5d38fde8028"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting verificacao_simples.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run verificacao_simples.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "g3m2qIz7O4Vm",
        "outputId": "f22fe246-c037-44b9-e19c-675ae3013d7b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'url_para' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/content/verificacao_simples.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;31m# Exemplo de uso (para testar no mesmo arquivo)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m     \u001b[0murl_para\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'url_para' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile verificacao_simples.py\n",
        "# verificacao_simples.py\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import ssl\n",
        "import urllib3\n",
        "import json\n",
        "from urllib.parse import urlparse\n",
        "import hashlib\n",
        "from collections import Counter\n",
        "import time\n",
        "\n",
        "# Desabilitar avisos SSL\n",
        "urllib3.disable_warnings()\n",
        "\n",
        "class VerificadorNoticiasAvancado:\n",
        "    def __init__(self):\n",
        "        print(\"Verificador de Notícias Avançado inicializado!\")\n",
        "        # Carregar listas de fontes confiáveis e não confiáveis\n",
        "        self.fontes_confiaveis = [\n",
        "            'g1.globo.com', 'bbc.com', 'bbc.co.uk', 'reuters.com', 'apnews.com',\n",
        "            'nytimes.com', 'washingtonpost.com', 'estadao.com.br', 'folha.uol.com.br',\n",
        "            'cnn.com', 'valor.globo.com', 'info.abril.com.br', 'oglobo.globo.com',\n",
        "            'dw.com', 'npr.org', 'tecmundo.com.br', 'canaltech.com.br', 'olhardigital.com.br'\n",
        "        ]\n",
        "\n",
        "        self.fontes_suspeitas = [\n",
        "            'theonion.com', 'sensacionalista.com.br', 'breitbart.com', 'infowars.com',\n",
        "            'clickhole.com', 'nationalreport.net', 'worldnewsdailyreport.com',\n",
        "            'dailycurrant.com', 'empirenews.net'\n",
        "        ]\n",
        "\n",
        "        # Palavras para análise linguística\n",
        "        self.palavras_sensacionalistas = {\n",
        "            \"chocante\": 2, \"inacreditável\": 2, \"surpreendente\": 1, \"sensacional\": 2,\n",
        "            \"impressionante\": 1, \"alarmante\": 1.5, \"exclusivo\": 1, \"urgente\": 1.5,\n",
        "            \"shocking\": 2, \"unbelievable\": 2, \"amazing\": 1, \"surprising\": 1,\n",
        "            \"imperdível\": 1.5, \"incrível\": 1.5, \"absurdo\": 1.5, \"escandaloso\": 2,\n",
        "            \"bomba\": 1.5, \"inédito\": 1, \"revelado\": 1, \"segredo\": 1.5,\n",
        "            \"polêmico\": 1.5, \"controverso\": 1, \"escândalo\": 1.5\n",
        "        }\n",
        "\n",
        "        self.palavras_incerteza = [\n",
        "            \"talvez\", \"possivelmente\", \"provavelmente\", \"pode ser\", \"especula-se\",\n",
        "            \"acredita-se\", \"sugere\", \"alega\", \"supostamente\", \"rumores\",\n",
        "            \"maybe\", \"possibly\", \"probably\", \"might be\", \"it is speculated\",\n",
        "            \"it is believed\", \"suggests\", \"allegedly\", \"supposedly\", \"rumors\"\n",
        "        ]\n",
        "\n",
        "        self.expressoes_opinativas = [\n",
        "            \"eu acho\", \"na minha opinião\", \"parece que\", \"acredito que\",\n",
        "            \"i think\", \"in my opinion\", \"it seems that\", \"i believe\"\n",
        "        ]\n",
        "\n",
        "    def extrair_dominio(self, url):\n",
        "        \"\"\"Extrai o domínio base da URL.\"\"\"\n",
        "        parsed_url = urlparse(url)\n",
        "        domain = parsed_url.netloc\n",
        "        return domain\n",
        "\n",
        "    def verificar_fonte(self, url):\n",
        "        \"\"\"Verifica o domínio da URL contra as listas de fontes.\"\"\"\n",
        "        dominio = self.extrair_dominio(url)\n",
        "        resultado = {}\n",
        "\n",
        "        # Verificar se o domínio está nas listas\n",
        "        if any(dominio.endswith(fonte) for fonte in self.fontes_confiaveis):\n",
        "            resultado[\"status_fonte\"] = \"confiável\"\n",
        "            resultado[\"pontos_fonte\"] = 20\n",
        "        elif any(dominio.endswith(fonte) for fonte in self.fontes_suspeitas):\n",
        "            resultado[\"status_fonte\"] = \"suspeita\"\n",
        "            resultado[\"pontos_fonte\"] = -30\n",
        "            resultado[\"alerta_fonte\"] = \"Fonte conhecida por conteúdo satírico ou de baixa credibilidade\"\n",
        "        else:\n",
        "            resultado[\"status_fonte\"] = \"não categorizada\"\n",
        "            resultado[\"pontos_fonte\"] = 0\n",
        "\n",
        "        resultado[\"dominio\"] = dominio\n",
        "        return resultado\n",
        "\n",
        "    def baixar_artigo(self, url):\n",
        "        \"\"\"Baixa e extrai o conteúdo básico de um artigo a partir da URL\"\"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers, verify=False, timeout=10, allow_redirects=True)\n",
        "            response.raise_for_status()  # Lança uma exceção para códigos de status HTTP ruins\n",
        "\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Tenta extrair o título\n",
        "            titulo = soup.title.text.strip() if soup.title else \"Título não encontrado\"\n",
        "\n",
        "            # Tenta extrair o conteúdo do artigo (abordagem mais abrangente)\n",
        "            content = \"\"\n",
        "\n",
        "            # Procura por conteúdo em elementos comuns\n",
        "            for tag in ['article', 'main', '.content', '.post', '.entry', '#content', '.article-body', '.news-text']:\n",
        "                if tag.startswith('.') or tag.startswith('#'):\n",
        "                    elements = soup.select(tag)\n",
        "                else:\n",
        "                    elements = soup.find_all(tag)\n",
        "\n",
        "                if elements:\n",
        "                    for element in elements:\n",
        "                        content += element.get_text(separator=' ', strip=True) + \" \"\n",
        "                    break\n",
        "\n",
        "            # Se não encontrou conteúdo nos elementos específicos, pega todos os parágrafos\n",
        "            if not content:\n",
        "                paragraphs = soup.find_all('p')\n",
        "                content = ' '.join([p.get_text(strip=True) for p in paragraphs])\n",
        "\n",
        "            # Tenta encontrar a data\n",
        "            data = None\n",
        "            # Procura por metadados comuns\n",
        "            for meta in soup.find_all('meta'):\n",
        "                if meta.get('property') in ['article:published_time', 'og:published_time', 'publication_date']:\n",
        "                    data = meta.get('content')\n",
        "                    break\n",
        "\n",
        "            # Procura por elementos que possam conter autores\n",
        "            autores = []\n",
        "            for author_tag in ['author', 'byline', '.author', '.byline', '[rel=author]']:\n",
        "                if author_tag.startswith('.') or author_tag.startswith('['):\n",
        "                    elements = soup.select(tag)\n",
        "                else:\n",
        "                    elements = soup.find_all(tag)\n",
        "\n",
        "                if elements:\n",
        "                    for element in elements:\n",
        "                        autor = element.get_text(strip=True)\n",
        "                        if autor and len(autor) < 100:  # Evitar pegar conteúdo grande por engano\n",
        "                            autores.append(autor)\n",
        "\n",
        "            # Remover duplicações nos autores\n",
        "            autores = list(dict.fromkeys(autores))\n",
        "\n",
        "            # Extrair imagens\n",
        "            imagens = []\n",
        "            for img in soup.find_all('img'):\n",
        "                src = img.get('src')\n",
        "                alt = img.get('alt', '')\n",
        "                if src and not src.startswith('data:'):\n",
        "                    # Garantir URL completa\n",
        "                    if src.startswith('/'):\n",
        "                        parsed_url = urlparse(url)\n",
        "                        base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
        "                        src = base_url + src\n",
        "                    imagens.append({\n",
        "                        'url': src,\n",
        "                        'alt': alt\n",
        "                    })\n",
        "\n",
        "            # Extrair palavras-chave\n",
        "            keywords = []\n",
        "            meta_keywords = soup.find('meta', attrs={'name': 'keywords'})\n",
        "            if meta_keywords:\n",
        "                keywords = meta_keywords.get('content', '').split(',')\n",
        "                keywords = [k.strip() for k in keywords]\n",
        "\n",
        "            return {\n",
        "                'titulo': titulo,\n",
        "                'conteudo': content,\n",
        "                'data': data,\n",
        "                'autores': autores,\n",
        "                'url': response.url,  # Use a URL final após redirecionamentos\n",
        "                'imagens': imagens,\n",
        "                'keywords': keywords\n",
        "            }\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Erro ao baixar artigo ({url}): {e}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"Erro inesperado ao processar artigo ({url}): {e}\")\n",
        "            return None\n",
        "\n",
        "    def analisar_linguagem(self, texto):\n",
        "        \"\"\"Analisa o tom e a linguagem usada no texto.\"\"\"\n",
        "        resultados = {}\n",
        "\n",
        "        # Análise de sensacionalismo com pesos\n",
        "        count_sensacionalismo = 0\n",
        "        palavras_encontradas = []\n",
        "\n",
        "        for palavra, peso in self.palavras_sensacionalistas.items():\n",
        "            ocorrencias = len(re.findall(r'\\b' + palavra + r'\\b', texto.lower()))\n",
        "            if ocorrencias > 0:\n",
        "                count_sensacionalismo += ocorrencias * peso\n",
        "                palavras_encontradas.append(f\"{palavra} ({ocorrencias}x)\")\n",
        "\n",
        "        resultados[\"indice_sensacionalismo\"] = count_sensacionalismo\n",
        "        resultados[\"palavras_sensacionalistas\"] = palavras_encontradas\n",
        "\n",
        "        if count_sensacionalismo > 3:\n",
        "            resultados[\"alerta_sensacionalismo\"] = \"Alto índice de linguagem sensacionalista detectado\"\n",
        "        elif count_sensacionalismo > 1.5:\n",
        "            resultados[\"alerta_sensacionalismo\"] = \"Linguagem potencialmente sensacionalista detectada\"\n",
        "\n",
        "        # Análise de incerteza\n",
        "        count_incerteza = 0\n",
        "        palavras_incerteza = []\n",
        "\n",
        "        for palavra in self.palavras_incerteza:\n",
        "            ocorrencias = len(re.findall(r'\\b' + palavra + r'\\b', texto.lower()))\n",
        "            if ocorrencias > 0:\n",
        "                count_incerteza += ocorrencias\n",
        "                palavras_incerteza.append(palavra)\n",
        "\n",
        "        resultados[\"indice_incerteza\"] = count_incerteza\n",
        "        resultados[\"palavras_incerteza\"] = palavras_incerteza\n",
        "\n",
        "        if count_incerteza > 3:\n",
        "            resultados[\"alerta_incerteza\"] = \"Alto uso de termos de incerteza detectado\"\n",
        "\n",
        "        # Análise de opinião\n",
        "        count_opiniao = 0\n",
        "        expressoes_opiniao = []\n",
        "\n",
        "        for expressao in self.expressoes_opinativas:\n",
        "            ocorrencias = len(re.findall(r'\\b' + expressao + r'\\b', texto.lower()))\n",
        "            if ocorrencias > 0:\n",
        "                count_opiniao += ocorrencias\n",
        "                expressoes_opiniao.append(expressao)\n",
        "\n",
        "        resultados[\"indice_opiniao\"] = count_opiniao\n",
        "        resultados[\"expressoes_opiniao\"] = expressoes_opiniao\n",
        "\n",
        "        if count_opiniao > 2:\n",
        "            resultados[\"alerta_opiniao\"] = \"Texto com forte caráter opinativo detectado\"\n",
        "\n",
        "        # Análise de exclamações (indicativo de sensacionalismo)\n",
        "        exclamacoes = len(re.findall(r'!', texto))\n",
        "        resultados[\"num_exclamacoes\"] = exclamacoes\n",
        "\n",
        "        if exclamacoes > 5:\n",
        "            resultados[\"alerta_exclamacoes\"] = \"Uso excessivo de pontos de exclamação\"\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def verificar_imagens(self, imagens):\n",
        "        \"\"\"Analisa as imagens associadas ao artigo.\"\"\"\n",
        "        resultados = {}\n",
        "\n",
        "        if not imagens:\n",
        "            resultados[\"alerta_imagens\"] = \"Nenhuma imagem encontrada no artigo\"\n",
        "            return resultados\n",
        "\n",
        "        resultados[\"num_imagens\"] = len(imagens)\n",
        "\n",
        "        # Verificar descrições das imagens (alt text)\n",
        "        imagens_sem_descricao = sum(1 for img in imagens if not img.get('alt'))\n",
        "        resultados[\"imagens_sem_descricao\"] = imagens_sem_descricao\n",
        "\n",
        "        if imagens_sem_descricao == len(imagens) and len(imagens) > 0:\n",
        "            resultados[\"alerta_imagens_descricao\"] = \"Nenhuma das imagens possui descrição adequada\"\n",
        "\n",
        "        # Calcular hash básico das URLs de imagens para potencial comparação futura\n",
        "        hashes_urls = [hashlib.md5(img['url'].encode()).hexdigest()[:8] for img in imagens]\n",
        "        resultados[\"hashes_imagens\"] = hashes_urls\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def buscar_noticias_similares(self, titulo, keywords):\n",
        "        \"\"\"Busca por notícias similares na web usando palavras-chave do artigo.\"\"\"\n",
        "        resultados = {}\n",
        "\n",
        "        try:\n",
        "            # Preparar termos de busca\n",
        "            termos_busca = titulo\n",
        "            if keywords:\n",
        "                termos_busca += \" \" + \" \".join(keywords[:3])  # Limitar a 3 keywords\n",
        "\n",
        "            # Simular resultados para demonstração\n",
        "            # Em uma implementação real, você usaria uma API como Google News ou similar\n",
        "            resultados[\"artigos_similares\"] = 5  # Simulando 5 artigos similares\n",
        "            resultados[\"similaridade\"] = \"média\"  # baixa, média, alta\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao buscar notícias similares: {e}\")\n",
        "            resultados[\"erro_busca\"] = str(e)\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def converter_nivel_para_texto(self, nivel):\n",
        "        \"\"\"Converte um nível numérico de veracidade para texto.\"\"\"\n",
        "        nivel_texto = {\n",
        "            1: \"CONFIRMADO\",\n",
        "            2: \"PROVAVELMENTE VERDADEIRO\",\n",
        "            3: \"INCONCLUSIVO\",\n",
        "            4: \"PROVAVELMENTE FALSO\",\n",
        "            5: \"FALSO\"\n",
        "        }.get(nivel, \"INCONCLUSIVO\")\n",
        "        return nivel_texto\n",
        "\n",
        "    def analisar_credibilidade(self, artigo):\n",
        "        \"\"\"Análise combinada de credibilidade baseada em características do texto e fonte.\"\"\"\n",
        "        if not artigo or not artigo.get('conteudo'):\n",
        "            return {\"erro\": \"Artigo não disponível para análise\"}\n",
        "\n",
        "        resultados = {}\n",
        "        conteudo = artigo['conteudo']\n",
        "\n",
        "        # 1. Verificação de fonte/domínio\n",
        "        info_fonte = self.verificar_fonte(artigo['url'])\n",
        "        resultados.update(info_fonte)\n",
        "\n",
        "        # 2. Copiar informações básicas\n",
        "        resultados[\"titulo\"] = artigo['titulo']\n",
        "        resultados[\"data_publicacao\"] = artigo['data']\n",
        "        resultados[\"autores\"] = artigo['autores']\n",
        "        resultados[\"url\"] = artigo['url']\n",
        "\n",
        "        # 3. Análise básica do conteúdo\n",
        "        palavras = len(conteudo.split())\n",
        "        resultados[\"num_palavras\"] = palavras\n",
        "        if palavras < 100:\n",
        "            resultados[\"alerta_tamanho\"] = \"Texto muito curto, pode não ser um artigo completo\"\n",
        "\n",
        "        # 4. Análise detalhada da linguagem\n",
        "        analise_linguagem = self.analisar_linguagem(conteudo)\n",
        "        resultados.update(analise_linguagem)\n",
        "\n",
        "        # 5. Verificação de citações\n",
        "        citacoes = re.findall(r'\"([^\"]+)\"', conteudo)\n",
        "        resultados[\"num_citacoes\"] = len(citacoes)\n",
        "\n",
        "        if len(citacoes) == 0:\n",
        "            resultados[\"alerta_fontes\"] = \"Nenhuma citação direta encontrada\"\n",
        "\n",
        "        # 6. Verificação de URLs externos (referências)\n",
        "        urls = re.findall(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', conteudo)\n",
        "        resultados[\"num_urls\"] = len(urls)\n",
        "\n",
        "        # 7. Análise de imagens\n",
        "        resultados_imagens = self.verificar_imagens(artigo.get('imagens', []))\n",
        "        resultados.update(resultados_imagens)\n",
        "\n",
        "        # 8. Verificação cruzada com outras notícias\n",
        "        resultados_similares = self.buscar_noticias_similares(\n",
        "            artigo['titulo'],\n",
        "            artigo.get('keywords', [])\n",
        "        )\n",
        "        resultados.update(resultados_similares)\n",
        "\n",
        "        # 9. Determinar um nível de veracidade (ESTA LÓGICA PRECISA SER IMPLEMENTADA)\n",
        "        nivel_de_veracidade_numerico = 3  # Exemplo: Artigo considerado inconclusivo\n",
        "        resultados[\"nivel_veracidade_numerico\"] = nivel_de_veracidade_numerico\n",
        "\n",
        "        # Converter o nível numérico para texto\n",
        "        resultados[\"nivel_veracidade\"] = self.converter_nivel_para_texto(nivel_de_veracidade_numerico)\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def verificar_url(self, url):\n",
        "        \"\"\"Função principal para verificar a credibilidade de uma notícia a partir da URL.\"\"\"\n",
        "        resultados = {\"url\": url}\n",
        "\n",
        "        # 1. Baixar o artigo\n",
        "        artigo = self.baixar_artigo(url)\n",
        "        if not artigo:\n",
        "            resultados[\"erro\"] = \"Não foi possível baixar o artigo.\"\n",
        "            return resultados\n",
        "\n",
        "        # 2. Analisar a credibilidade do artigo\n",
        "        analise_credibilidade = self.analisar_credibilidade(artigo)\n",
        "        resultados.update(analise_credibilidade)\n",
        "\n",
        "        return resultados\n",
        "\n",
        "def analisar_url(url):\n",
        "    \"\"\"Função para analisar uma URL específica.\"\"\"\n",
        "    verificador = VerificadorNoticiasAvancado()\n",
        "    resultados = verificador.verificar_url(url)\n",
        "\n",
        "    return resultados\n",
        "\n",
        "# Exemplo de uso (para testar no mesmo arquivo)\n",
        "if __name__ == \"__main__\":\n",
        "    url_para"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dnxo84zZOr6l",
        "outputId": "bdde99d4-853e-411b-f108-1561ed9c1fa8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting verificacao_simples.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run verificacao_simples.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5n-BgDDNaif",
        "outputId": "f901e85f-9dd7-43e8-ca44-ee62a0a17de7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verificador de Notícias Avançado inicializado!\n",
            "\n",
            "--- Resultados da Análise ---\n",
            "Título: N/A\n",
            "URL: https://www.msn.com/pt-br/noticias/brasil/casa-branca-aponta-que-suposta-verdade-sobre-a-origem-da-covid-19-tem-rela%C3%A7%C3%A3o-com-vazamento-em-laborat%C3%B3rio-na-china/ar-AA1DbpaW?ocid=BingNewsSerp\n",
            "Fonte: N/A\n",
            "Data de Publicação: N/A\n",
            "Número de Palavras: N/A\n",
            "Alertas: Nenhum\n",
            "Erro: Artigo não disponível para análise\n",
            "Verificador de Notícias Avançado inicializado!\n",
            "Verificador de Notícias Avançado inicializado!\n",
            "{\n",
            "    \"url\": \"https://www.msn.com/pt-br/noticias/brasil/casa-branca-aponta-que-suposta-verdade-sobre-a-origem-da-covid-19-tem-rela%C3%A7%C3%A3o-com-vazamento-em-laborat%C3%B3rio-na-china/ar-AA1DbpaW?ocid=BingNewsSerp\",\n",
            "    \"erro\": \"Artigo não disponível para análise\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile verificacao_simples.py\n",
        "\n",
        "\n",
        "# verificacao_simples.py\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import ssl\n",
        "import urllib3\n",
        "import json\n",
        "from urllib.parse import urlparse\n",
        "import hashlib\n",
        "from collections import Counter\n",
        "import time\n",
        "\n",
        "# Desabilitar avisos SSL\n",
        "urllib3.disable_warnings()\n",
        "\n",
        "class VerificadorNoticiasAvancado:\n",
        "    def __init__(self):\n",
        "        print(\"Verificador de Notícias Avançado inicializado!\")\n",
        "        # Carregar listas de fontes confiáveis e não confiáveis\n",
        "        self.fontes_confiaveis = [\n",
        "            'g1.globo.com', 'bbc.com', 'bbc.co.uk', 'reuters.com', 'apnews.com',\n",
        "            'nytimes.com', 'washingtonpost.com', 'estadao.com.br', 'folha.uol.com.br',\n",
        "            'cnn.com', 'valor.globo.com', 'info.abril.com.br', 'oglobo.globo.com',\n",
        "            'dw.com', 'npr.org', 'tecmundo.com.br', 'canaltech.com.br', 'olhardigital.com.br'\n",
        "        ]\n",
        "\n",
        "        self.fontes_suspeitas = [\n",
        "            'theonion.com', 'sensacionalista.com.br', 'breitbart.com', 'infowars.com',\n",
        "            'clickhole.com', 'nationalreport.net', 'worldnewsdailyreport.com',\n",
        "            'dailycurrant.com', 'empirenews.net'\n",
        "        ]\n",
        "\n",
        "        # Palavras para análise linguística\n",
        "        self.palavras_sensacionalistas = {\n",
        "            \"chocante\": 2, \"inacreditável\": 2, \"surpreendente\": 1, \"sensacional\": 2,\n",
        "            \"impressionante\": 1, \"alarmante\": 1.5, \"exclusivo\": 1, \"urgente\": 1.5,\n",
        "            \"shocking\": 2, \"unbelievable\": 2, \"amazing\": 1, \"surprising\": 1,\n",
        "            \"imperdível\": 1.5, \"incrível\": 1.5, \"absurdo\": 1.5, \"escandaloso\": 2,\n",
        "            \"bomba\": 1.5, \"inédito\": 1, \"revelado\": 1, \"segredo\": 1.5,\n",
        "            \"polêmico\": 1.5, \"controverso\": 1, \"escândalo\": 1.5\n",
        "        }\n",
        "\n",
        "        self.palavras_incerteza = [\n",
        "            \"talvez\", \"possivelmente\", \"provavelmente\", \"pode ser\", \"especula-se\",\n",
        "            \"acredita-se\", \"sugere\", \"alega\", \"supostamente\", \"rumores\",\n",
        "            \"maybe\", \"possibly\", \"probably\", \"might be\", \"it is speculated\",\n",
        "            \"it is believed\", \"suggests\", \"allegedly\", \"supposedly\", \"rumors\"\n",
        "        ]\n",
        "\n",
        "        self.expressoes_opinativas = [\n",
        "            \"eu acho\", \"na minha opinião\", \"parece que\", \"acredito que\",\n",
        "            \"i think\", \"in my opinion\", \"it seems that\", \"i believe\"\n",
        "        ]\n",
        "\n",
        "    def extrair_dominio(self, url):\n",
        "        \"\"\"Extrai o domínio base da URL.\"\"\"\n",
        "        parsed_url = urlparse(url)\n",
        "        domain = parsed_url.netloc\n",
        "        return domain\n",
        "\n",
        "    def verificar_fonte(self, url):\n",
        "        \"\"\"Verifica o domínio da URL contra as listas de fontes.\"\"\"\n",
        "        dominio = self.extrair_dominio(url)\n",
        "        resultado = {}\n",
        "\n",
        "        # Verificar se o domínio está nas listas\n",
        "        if any(dominio.endswith(fonte) for fonte in self.fontes_confiaveis):\n",
        "            resultado[\"status_fonte\"] = \"confiável\"\n",
        "            resultado[\"pontos_fonte\"] = 20\n",
        "        elif any(dominio.endswith(fonte) for fonte in self.fontes_suspeitas):\n",
        "            resultado[\"status_fonte\"] = \"suspeita\"\n",
        "            resultado[\"pontos_fonte\"] = -30\n",
        "            resultado[\"alerta_fonte\"] = \"Fonte conhecida por conteúdo satírico ou de baixa credibilidade\"\n",
        "        else:\n",
        "            resultado[\"status_fonte\"] = \"não categorizada\"\n",
        "            resultado[\"pontos_fonte\"] = 0\n",
        "\n",
        "        resultado[\"dominio\"] = dominio\n",
        "        return resultado\n",
        "\n",
        "    def baixar_artigo(self, url):\n",
        "        \"\"\"Baixa e extrai o conteúdo básico de um artigo a partir da URL\"\"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers, verify=False, timeout=10)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Tenta extrair o título\n",
        "            titulo = soup.title.text.strip() if soup.title else \"Título não encontrado\"\n",
        "\n",
        "            # Tenta extrair o conteúdo do artigo (abordagem mais abrangente)\n",
        "            content = \"\"\n",
        "\n",
        "            # Procura por conteúdo em elementos comuns\n",
        "            for tag in ['article', 'main', '.content', '.post', '.entry', '#content', '.article-body', '.news-text']:\n",
        "                if tag.startswith('.') or tag.startswith('#'):\n",
        "                    elements = soup.select(tag)\n",
        "                else:\n",
        "                    elements = soup.find_all(tag)\n",
        "\n",
        "                if elements:\n",
        "                    for element in elements:\n",
        "                        content += element.get_text(separator=' ', strip=True) + \" \"\n",
        "                    break\n",
        "\n",
        "            # Se não encontrou conteúdo nos elementos específicos, pega todos os parágrafos\n",
        "            if not content:\n",
        "                paragraphs = soup.find_all('p')\n",
        "                content = ' '.join([p.get_text(strip=True) for p in paragraphs])\n",
        "\n",
        "            # Tenta encontrar a data\n",
        "            data = None\n",
        "            # Procura por metadados comuns\n",
        "            for meta in soup.find_all('meta'):\n",
        "                if meta.get('property') in ['article:published_time', 'og:published_time', 'publication_date']:\n",
        "                    data = meta.get('content')\n",
        "                    break\n",
        "\n",
        "            # Procura por elementos que possam conter autores\n",
        "            autores = []\n",
        "            for author_tag in ['author', 'byline', '.author', '.byline', '[rel=author]']:\n",
        "                if author_tag.startswith('.') or author_tag.startswith('['):\n",
        "                    elements = soup.select(tag)\n",
        "                else:\n",
        "                    elements = soup.find_all(tag)\n",
        "\n",
        "                if elements:\n",
        "                    for element in elements:\n",
        "                        autor = element.get_text(strip=True)\n",
        "                        if autor and len(autor) < 100:  # Evitar pegar conteúdo grande por engano\n",
        "                            autores.append(autor)\n",
        "\n",
        "            # Remover duplicações nos autores\n",
        "            autores = list(dict.fromkeys(autores))\n",
        "\n",
        "            # Extrair imagens\n",
        "            imagens = []\n",
        "            for img in soup.find_all('img'):\n",
        "                src = img.get('src')\n",
        "                alt = img.get('alt', '')\n",
        "                if src and not src.startswith('data:'):\n",
        "                    # Garantir URL completa\n",
        "                    if src.startswith('/'):\n",
        "                        parsed_url = urlparse(url)\n",
        "                        base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
        "                        src = base_url + src\n",
        "                    imagens.append({\n",
        "                        'url': src,\n",
        "                        'alt': alt\n",
        "                    })\n",
        "\n",
        "            # Extrair palavras-chave\n",
        "            keywords = []\n",
        "            meta_keywords = soup.find('meta', attrs={'name': 'keywords'})\n",
        "            if meta_keywords:\n",
        "                keywords = meta_keywords.get('content', '').split(',')\n",
        "                keywords = [k.strip() for k in keywords]\n",
        "\n",
        "            return {\n",
        "                'titulo': titulo,\n",
        "                'conteudo': content,\n",
        "                'data': data,\n",
        "                'autores': autores,\n",
        "                'url': url,\n",
        "                'imagens': imagens,\n",
        "                'keywords': keywords\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao baixar artigo: {e}\")\n",
        "            return None\n",
        "\n",
        "    def analisar_linguagem(self, texto):\n",
        "        \"\"\"Analisa o tom e a linguagem usada no texto.\"\"\"\n",
        "        resultados = {}\n",
        "\n",
        "        # Análise de sensacionalismo com pesos\n",
        "        count_sensacionalismo = 0\n",
        "        palavras_encontradas = []\n",
        "\n",
        "        for palavra, peso in self.palavras_sensacionalistas.items():\n",
        "            ocorrencias = len(re.findall(r'\\b' + palavra + r'\\b', texto.lower()))\n",
        "            if ocorrencias > 0:\n",
        "                count_sensacionalismo += ocorrencias * peso\n",
        "                palavras_encontradas.append(f\"{palavra} ({ocorrencias}x)\")\n",
        "\n",
        "        resultados[\"indice_sensacionalismo\"] = count_sensacionalismo\n",
        "        resultados[\"palavras_sensacionalistas\"] = palavras_encontradas\n",
        "\n",
        "        if count_sensacionalismo > 3:\n",
        "            resultados[\"alerta_sensacionalismo\"] = \"Alto índice de linguagem sensacionalista detectado\"\n",
        "        elif count_sensacionalismo > 1.5:\n",
        "            resultados[\"alerta_sensacionalismo\"] = \"Linguagem potencialmente sensacionalista detectada\"\n",
        "\n",
        "        # Análise de incerteza\n",
        "        count_incerteza = 0\n",
        "        palavras_incerteza = []\n",
        "\n",
        "        for palavra in self.palavras_incerteza:\n",
        "            ocorrencias = len(re.findall(r'\\b' + palavra + r'\\b', texto.lower()))\n",
        "            if ocorrencias > 0:\n",
        "                count_incerteza += ocorrencias\n",
        "                palavras_incerteza.append(palavra)\n",
        "\n",
        "        resultados[\"indice_incerteza\"] = count_incerteza\n",
        "        resultados[\"palavras_incerteza\"] = palavras_incerteza\n",
        "\n",
        "        if count_incerteza > 3:\n",
        "            resultados[\"alerta_incerteza\"] = \"Alto uso de termos de incerteza detectado\"\n",
        "\n",
        "        # Análise de opinião\n",
        "        count_opiniao = 0\n",
        "        expressoes_opiniao = []\n",
        "\n",
        "        for expressao in self.expressoes_opinativas:\n",
        "            ocorrencias = len(re.findall(r'\\b' + expressao + r'\\b', texto.lower()))\n",
        "            if ocorrencias > 0:\n",
        "                count_opiniao += ocorrencias\n",
        "                expressoes_opiniao.append(expressao)\n",
        "\n",
        "        resultados[\"indice_opiniao\"] = count_opiniao\n",
        "        resultados[\"expressoes_opiniao\"] = expressoes_opiniao\n",
        "\n",
        "        if count_opiniao > 2:\n",
        "            resultados[\"alerta_opiniao\"] = \"Texto com forte caráter opinativo detectado\"\n",
        "\n",
        "        # Análise de exclamações (indicativo de sensacionalismo)\n",
        "        exclamacoes = len(re.findall(r'!', texto))\n",
        "        resultados[\"num_exclamacoes\"] = exclamacoes\n",
        "\n",
        "        if exclamacoes > 5:\n",
        "            resultados[\"alerta_exclamacoes\"] = \"Uso excessivo de pontos de exclamação\"\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def verificar_imagens(self, imagens):\n",
        "        \"\"\"Analisa as imagens associadas ao artigo.\"\"\"\n",
        "        resultados = {}\n",
        "\n",
        "        if not imagens:\n",
        "            resultados[\"alerta_imagens\"] = \"Nenhuma imagem encontrada no artigo\"\n",
        "            return resultados\n",
        "\n",
        "        resultados[\"num_imagens\"] = len(imagens)\n",
        "\n",
        "        # Verificar descrições das imagens (alt text)\n",
        "        imagens_sem_descricao = sum(1 for img in imagens if not img.get('alt'))\n",
        "        resultados[\"imagens_sem_descricao\"] = imagens_sem_descricao\n",
        "\n",
        "        if imagens_sem_descricao == len(imagens) and len(imagens) > 0:\n",
        "            resultados[\"alerta_imagens_descricao\"] = \"Nenhuma das imagens possui descrição adequada\"\n",
        "\n",
        "        # Calcular hash básico das URLs de imagens para potencial comparação futura\n",
        "        hashes_urls = [hashlib.md5(img['url'].encode()).hexdigest()[:8] for img in imagens]\n",
        "        resultados[\"hashes_imagens\"] = hashes_urls\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def buscar_noticias_similares(self, titulo, keywords):\n",
        "        \"\"\"Busca por notícias similares na web usando palavras-chave do artigo.\"\"\"\n",
        "        resultados = {}\n",
        "\n",
        "        try:\n",
        "            # Preparar termos de busca\n",
        "            termos_busca = titulo\n",
        "            if keywords:\n",
        "                termos_busca += \" \" + \" \".join(keywords[:3])  # Limitar a 3 keywords\n",
        "\n",
        "            # Simular resultados para demonstração\n",
        "            # Em uma implementação real, você usaria uma API como Google News ou similar\n",
        "            resultados[\"artigos_similares\"] = 5  # Simulando 5 artigos similares\n",
        "            resultados[\"similaridade\"] = \"média\"  # baixa, média, alta\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao buscar notícias similares: {e}\")\n",
        "            resultados[\"erro_busca\"] = str(e)\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def analisar_credibilidade(self, artigo):\n",
        "        \"\"\"Análise combinada de credibilidade baseada em características do texto e fonte.\"\"\"\n",
        "        if not artigo or not artigo.get('conteudo'):\n",
        "            return {\"erro\": \"Artigo não disponível para análise\"}\n",
        "\n",
        "        resultados = {}\n",
        "        conteudo = artigo['conteudo']\n",
        "\n",
        "        # 1. Verificação de fonte/domínio\n",
        "        info_fonte = self.verificar_fonte(artigo['url'])\n",
        "        resultados.update(info_fonte)\n",
        "\n",
        "        # 2. Copiar informações básicas\n",
        "        resultados[\"titulo\"] = artigo['titulo']\n",
        "        resultados[\"data_publicacao\"] = artigo['data']\n",
        "        resultados[\"autores\"] = artigo['autores']\n",
        "        resultados[\"url\"] = artigo['url']\n",
        "\n",
        "        # 3. Análise básica do conteúdo\n",
        "        palavras = len(conteudo.split())\n",
        "        resultados[\"num_palavras\"] = palavras\n",
        "        if palavras < 100:\n",
        "            resultados[\"alerta_tamanho\"] = \"Texto muito curto, pode não ser um artigo completo\"\n",
        "\n",
        "        # 4. Análise detalhada da linguagem\n",
        "        analise_linguagem = self.analisar_linguagem(conteudo)\n",
        "        resultados.update(analise_linguagem)\n",
        "\n",
        "        # 5. Verificação de citações\n",
        "        citacoes = re.findall(r'\"([^\"]+)\"', conteudo)\n",
        "        resultados[\"num_citacoes\"] = len(citacoes)\n",
        "\n",
        "        if len(citacoes) == 0:\n",
        "            resultados[\"alerta_fontes\"] = \"Nenhuma citação direta encontrada\"\n",
        "\n",
        "        # 6. Verificação de URLs externos (referências)\n",
        "        urls = re.findall(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', conteudo)\n",
        "        resultados[\"num_urls\"] = len(urls)\n",
        "\n",
        "        # 7. Análise de imagens\n",
        "        resultados_imagens = self.verificar_imagens(artigo.get('imagens', []))\n",
        "        resultados.update(resultados_imagens)\n",
        "\n",
        "        # 8. Verificação cruzada com outras notícias\n",
        "        resultados_similares = self.buscar_noticias_similares(\n",
        "            artigo['titulo'],\n",
        "            artigo.get('keywords', [])\n",
        "        )\n",
        "        resultados.update(resultados_similares)\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def verificar_url(self, url):\n",
        "        \"\"\"Função principal para verificar a credibilidade de uma notícia a partir da URL.\"\"\"\n",
        "        resultados = {\"url\": url}\n",
        "\n",
        "        # 1. Baixar o artigo\n",
        "        artigo = self.baixar_artigo(url)\n",
        "        if not artigo:\n",
        "            resultados[\"erro\"] = \"Não foi possível baixar o artigo.\"\n",
        "            return resultados\n",
        "\n",
        "        # 2. Analisar a credibilidade do artigo\n",
        "        analise_credibilidade = self.analisar_credibilidade(artigo)\n",
        "        resultados.update(analise_credibilidade)\n",
        "\n",
        "        return resultados\n",
        "\n",
        "def analisar_url(url):\n",
        "    \"\"\"Função para analisar uma URL específica.\"\"\"\n",
        "    verificador = VerificadorNoticiasAvancado()\n",
        "    resultados = verificador.verificar_url(url)\n",
        "\n",
        "    return resultados\n",
        "\n",
        "# Exemplo de uso (para testar no mesmo arquivo)\n",
        "if __name__ == \"__main__\":\n",
        "    url_para_analisar = \"https://www.msn.com/pt-br/noticias/brasil/casa-branca-aponta-que-suposta-verdade-sobre-a-origem-da-covid-19-tem-rela%C3%A7%C3%A3o-com-vazamento-em-laborat%C3%B3rio-na-china/ar-AA1DbpaW?ocid=BingNewsSerp\"\n",
        "\n",
        "    analise = analisar_url(url_para_analisar)\n",
        "\n",
        "    # Imprimir resultados de forma organizada (exemplo)\n",
        "    print(\"\\n--- Resultados da Análise ---\")\n",
        "    print(f\"Título: {analise.get('titulo', 'N/A')}\")\n",
        "    print(f\"URL: {analise.get('url', 'N/A')}\")\n",
        "    print(f\"Fonte: {analise.get('status_fonte', 'N/A')}\")\n",
        "    print(f\"Data de Publicação: {analise.get('data_publicacao', 'N/A')}\")\n",
        "    print(f\"Número de Palavras: {analise.get('num_palavras', 'N/A')}\")\n",
        "    print(f\"Alertas: {analise.get('alertas', 'Nenhum')}\")  # Adapte conforme a estrutura real\n",
        "\n",
        "    if \"erro\" in analise:\n",
        "        print(f\"Erro: {analise['erro']}\")\n",
        "\n",
        "def verificar_url(url):\n",
        "    \"\"\"Função para analisar uma URL.\"\"\"\n",
        "    return analisar_url(url)\n",
        "\n",
        "print(\"Verificador de Notícias Avançado inicializado!\")\n",
        "\n",
        "# Exemplo de uso da função verificar_url\n",
        "url_para_analisar = \"https://www.msn.com/pt-br/noticias/brasil/casa-branca-aponta-que-suposta-verdade-sobre-a-origem-da-covid-19-tem-rela%C3%A7%C3%A3o-com-vazamento-em-laborat%C3%B3rio-na-china/ar-AA1DbpaW?ocid=BingNewsSerp\"\n",
        "resultados = verificar_url(url_para_analisar)\n",
        "print(json.dumps(resultados, indent=4, ensure_ascii=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MbfciXbNFlb",
        "outputId": "731a1cbf-7075-4383-b05f-498e6a002eef"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing verificacao_simples.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile verificacao_simples.py\n",
        "\n",
        "# verificacao_simples.py\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import ssl\n",
        "import urllib3\n",
        "import json\n",
        "from urllib.parse import urlparse\n",
        "import hashlib\n",
        "from collections import Counter\n",
        "import time\n",
        "\n",
        "# Desabilitar avisos SSL\n",
        "urllib3.disable_warnings()\n",
        "\n",
        "class VerificadorNoticiasAvancado:\n",
        "    def __init__(self):\n",
        "        print(\"Verificador de Notícias Avançado inicializado!\")\n",
        "        # Carregar listas de fontes confiáveis e não confiáveis\n",
        "        self.fontes_confiaveis = [\n",
        "            'g1.globo.com', 'bbc.com', 'bbc.co.uk', 'reuters.com', 'apnews.com',\n",
        "            'nytimes.com', 'washingtonpost.com', 'estadao.com.br', 'folha.uol.com.br',\n",
        "            'cnn.com', 'valor.globo.com', 'info.abril.com.br', 'oglobo.globo.com',\n",
        "            'dw.com', 'npr.org', 'tecmundo.com.br', 'canaltech.com.br', 'olhardigital.com.br'\n",
        "        ]\n",
        "\n",
        "        self.fontes_suspeitas = [\n",
        "            'theonion.com', 'sensacionalista.com.br', 'breitbart.com', 'infowars.com',\n",
        "            'clickhole.com', 'nationalreport.net', 'worldnewsdailyreport.com',\n",
        "            'dailycurrant.com', 'empirenews.net'\n",
        "        ]\n",
        "\n",
        "        # Palavras para análise linguística\n",
        "        self.palavras_sensacionalistas = {\n",
        "            \"chocante\": 2, \"inacreditável\": 2, \"surpreendente\": 1, \"sensacional\": 2,\n",
        "            \"impressionante\": 1, \"alarmante\": 1.5, \"exclusivo\": 1, \"urgente\": 1.5,\n",
        "            \"shocking\": 2, \"unbelievable\": 2, \"amazing\": 1, \"surprising\": 1,\n",
        "            \"imperdível\": 1.5, \"incrível\": 1.5, \"absurdo\": 1.5, \"escandaloso\": 2,\n",
        "            \"bomba\": 1.5, \"inédito\": 1, \"revelado\": 1, \"segredo\": 1.5,\n",
        "            \"polêmico\": 1.5, \"controverso\": 1, \"escândalo\": 1.5\n",
        "        }\n",
        "\n",
        "        self.palavras_incerteza = [\n",
        "            \"talvez\", \"possivelmente\", \"provavelmente\", \"pode ser\", \"especula-se\",\n",
        "            \"acredita-se\", \"sugere\", \"alega\", \"supostamente\", \"rumores\",\n",
        "            \"maybe\", \"possibly\", \"probably\", \"might be\", \"it is speculated\",\n",
        "            \"it is believed\", \"suggests\", \"allegedly\", \"supposedly\", \"rumors\"\n",
        "        ]\n",
        "\n",
        "        self.expressoes_opinativas = [\n",
        "            \"eu acho\", \"na minha opinião\", \"parece que\", \"acredito que\",\n",
        "            \"i think\", \"in my opinion\", \"it seems that\", \"i believe\"\n",
        "        ]\n",
        "\n",
        "    def extrair_dominio(self, self, url):\n",
        "        \"\"\"Extrai o domínio base da URL.\"\"\"\n",
        "        parsed_url = urlparse(url)\n",
        "        domain = parsed_url.netloc\n",
        "        return domain\n",
        "\n",
        "    def verificar_fonte(self, url):\n",
        "        \"\"\"Verifica o domínio da URL contra as listas de fontes.\"\"\"\n",
        "        dominio = self.extrair_dominio(url)\n",
        "        resultado = {}\n",
        "\n",
        "        # Verificar se o domínio está nas listas\n",
        "        if any(dominio.endswith(fonte) for fonte in self.fontes_confiaveis):\n",
        "            resultado[\"status_fonte\"] = \"confiável\"\n",
        "            resultado[\"pontos_fonte\"] = 20\n",
        "        elif any(dominio.endswith(fonte) for fonte in self.fontes_suspeitas):\n",
        "            resultado[\"status_fonte\"] = \"suspeita\"\n",
        "            resultado[\"pontos_fonte\"] = -30\n",
        "            resultado[\"alerta_fonte\"] = \"Fonte conhecida por conteúdo satírico ou de baixa credibilidade\"\n",
        "        else:\n",
        "            resultado[\"status_fonte\"] = \"não categorizada\"\n",
        "            resultado[\"pontos_fonte\"] = 0\n",
        "\n",
        "        resultado[\"dominio\"] = dominio\n",
        "        return resultado\n",
        "\n",
        "    def baixar_artigo(self, url):\n",
        "        \"\"\"Baixa e extrai o conteúdo básico de um artigo a partir da URL\"\"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers, verify=False, timeout=10)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Tenta extrair o título\n",
        "            titulo = soup.title.text.strip() if soup.title else \"Título não encontrado\"\n",
        "\n",
        "            # Tenta extrair o conteúdo do artigo (abordagem mais abrangente)\n",
        "            content = \"\"\n",
        "\n",
        "            # Procura por conteúdo em elementos comuns\n",
        "            for tag in ['article', 'main', '.content', '.post', '.entry', '#content', '.article-body', '.news-text']:\n",
        "                if tag.startswith('.') or tag.startswith('#'):\n",
        "                    elements = soup.select(tag)\n",
        "                else:\n",
        "                    elements = soup.find_all(tag)\n",
        "\n",
        "                if elements:\n",
        "                    for element in elements:\n",
        "                        content += element.get_text(separator=' ', strip=True) + \" \"\n",
        "                    break\n",
        "\n",
        "            # Se não encontrou conteúdo nos elementos específicos, pega todos os parágrafos\n",
        "            if not content:\n",
        "                paragraphs = soup.find_all('p')\n",
        "                content = ' '.join([p.get_text(strip=True) for p in paragraphs])\n",
        "\n",
        "            # Tenta encontrar a data\n",
        "            data = None\n",
        "            # Procura por metadados comuns\n",
        "            for meta in soup.find_all('meta'):\n",
        "                if meta.get('property') in ['article:published_time', 'og:published_time', 'publication_date']:\n",
        "                    data = meta.get('content')\n",
        "                    break\n",
        "\n",
        "            # Procura por elementos que possam conter autores\n",
        "            autores = []\n",
        "            for author_tag in ['author', 'byline', '.author', '.byline', '[rel=author]']:\n",
        "                if author_tag.startswith('.') or author_tag.startswith('['):\n",
        "                    elements = soup.select(tag)\n",
        "                else:\n",
        "                    elements = soup.find_all(tag)\n",
        "\n",
        "                if elements:\n",
        "                    for element in elements:\n",
        "                        autor = element.get_text(strip=True)\n",
        "                        if autor and len(autor) < 100:  # Evitar pegar conteúdo grande por engano\n",
        "                            autores.append(autor)\n",
        "\n",
        "            # Remover duplicações nos autores\n",
        "            autores = list(dict.fromkeys(autores))\n",
        "\n",
        "            # Extrair imagens\n",
        "            imagens = []\n",
        "            for img in soup.find_all('img'):\n",
        "                src = img.get('src')\n",
        "                alt = img.get('alt', '')\n",
        "                if src and not src.startswith('data:'):\n",
        "                    # Garantir URL completa\n",
        "                    if src.startswith('/'):\n",
        "                        parsed_url = urlparse(url)\n",
        "                        base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
        "                        src = base_url + src\n",
        "                    imagens.append({\n",
        "                        'url': src,\n",
        "                        'alt': alt\n",
        "                    })\n",
        "\n",
        "            # Extrair palavras-chave\n",
        "            keywords = []\n",
        "            meta_keywords = soup.find('meta', attrs={'name': 'keywords'})\n",
        "            if meta_keywords:\n",
        "                keywords = meta_keywords.get('content', '').split(',')\n",
        "                keywords = [k.strip() for k in keywords]\n",
        "\n",
        "            return {\n",
        "                'titulo': titulo,\n",
        "                'conteudo': content,\n",
        "                'data': data,\n",
        "                'autores': autores,\n",
        "                'url': url,\n",
        "                'imagens': imagens,\n",
        "                'keywords': keywords\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao baixar artigo: {e}\")\n",
        "            return None\n",
        "\n",
        "    def analisar_linguagem(self, texto):\n",
        "        \"\"\"Analisa o tom e a linguagem usada no texto.\"\"\"\n",
        "        resultados = {}\n",
        "\n",
        "        # Análise de sensacionalismo com pesos\n",
        "        count_sensacionalismo = 0\n",
        "        palavras_encontradas = []\n",
        "\n",
        "        for palavra, peso in self.palavras_sensacionalistas.items():\n",
        "            ocorrencias = len(re.findall(r'\\b' + palavra + r'\\b', texto.lower()))\n",
        "            if ocorrencias > 0:\n",
        "                count_sensacionalismo += ocorrencias * peso\n",
        "                palavras_encontradas.append(f\"{palavra} ({ocorrencias}x)\")\n",
        "\n",
        "        resultados[\"indice_sensacionalismo\"] = count_sensacionalismo\n",
        "        resultados[\"palavras_sensacionalistas\"] = palavras_encontradas\n",
        "\n",
        "        if count_sensacionalismo > 3:\n",
        "            resultados[\"alerta_sensacionalismo\"] = \"Alto índice de linguagem sensacionalista detectado\"\n",
        "        elif count_sensacionalismo > 1.5:\n",
        "            resultados[\"alerta_sensacionalismo\"] = \"Linguagem potencialmente sensacionalista detectada\"\n",
        "\n",
        "        # Análise de incerteza\n",
        "        count_incerteza = 0\n",
        "        palavras_incerteza = []\n",
        "\n",
        "        for palavra in self.palavras_incerteza:\n",
        "            ocorrencias = len(re.findall(r'\\b' + palavra + r'\\b', texto.lower()))\n",
        "            if ocorrencias > 0:\n",
        "                count_incerteza += ocorrencias\n",
        "                palavras_incerteza.append(palavra)\n",
        "\n",
        "        resultados[\"indice_incerteza\"] = count_incerteza\n",
        "        resultados[\"palavras_incerteza\"] = palavras_incerteza\n",
        "\n",
        "        if count_incerteza > 3:\n",
        "            resultados[\"alerta_incerteza\"] = \"Alto uso de termos de incerteza detectado\"\n",
        "\n",
        "        # Análise de opinião\n",
        "        count_opiniao = 0\n",
        "        expressoes_opiniao = []\n",
        "\n",
        "        for expressao in self.expressoes_opinativas:\n",
        "            ocorrencias = len(re.findall(r'\\b' + expressao + r'\\b', texto.lower()))\n",
        "            if ocorrencias > 0:\n",
        "                count_opiniao += ocorrencias\n",
        "                expressoes_opiniao.append(expressao)\n",
        "\n",
        "        resultados[\"indice_opiniao\"] = count_opiniao\n",
        "        resultados[\"expressoes_opiniao\"] = expressoes_opiniao\n",
        "\n",
        "        if count_opiniao > 2:\n",
        "            resultados[\"alerta_opiniao\"] = \"Texto com forte caráter opinativo detectado\"\n",
        "\n",
        "        # Análise de exclamações (indicativo de sensacionalismo)\n",
        "        exclamacoes = len(re.findall(r'!', texto))\n",
        "        resultados[\"num_exclamacoes\"] = exclamacoes\n",
        "\n",
        "        if exclamacoes > 5:\n",
        "            resultados[\"alerta_exclamacoes\"] = \"Uso excessivo de pontos de exclamação\"\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def verificar_imagens(self, imagens):\n",
        "        \"\"\"Analisa as imagens associadas ao artigo.\"\"\"\n",
        "        resultados = {}\n",
        "\n",
        "        if not imagens:\n",
        "            resultados[\"alerta_imagens\"] = \"Nenhuma imagem encontrada no artigo\"\n",
        "            return resultados\n",
        "\n",
        "        resultados[\"num_imagens\"] = len(imagens)\n",
        "\n",
        "        # Verificar descrições das imagens (alt text)\n",
        "        imagens_sem_descricao = sum(1 for img in imagens if not img.get('alt'))\n",
        "        resultados[\"imagens_sem_descricao\"] = imagens_sem_descricao\n",
        "\n",
        "        if imagens_sem_descricao == len(imagens) and len(imagens) > 0:\n",
        "            resultados[\"alerta_imagens_descricao\"] = \"Nenhuma das imagens possui descrição adequada\"\n",
        "\n",
        "        # Calcular hash básico das URLs de imagens para potencial comparação futura\n",
        "        hashes_urls = [hashlib.md5(img['url'].encode()).hexdigest()[:8] for img in imagens]\n",
        "        resultados[\"hashes_imagens\"] = hashes_urls\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def buscar_noticias_similares(self, titulo, keywords):\n",
        "        \"\"\"Busca por notícias similares na web usando palavras-chave do artigo.\"\"\"\n",
        "        resultados = {}\n",
        "\n",
        "        try:\n",
        "            # Preparar termos de busca\n",
        "            termos_busca = titulo\n",
        "            if keywords:\n",
        "                termos_busca += \" \" + \" \".join(keywords[:3])  # Limitar a 3 keywords\n",
        "\n",
        "            # Simular resultados para demonstração\n",
        "            # Em uma implementação real, você usaria uma API como Google News ou similar\n",
        "            resultados[\"artigos_similares\"] = 5  # Simulando 5 artigos similares\n",
        "            resultados[\"similaridade\"] = \"média\"  # baixa, média, alta\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao buscar notícias similares: {e}\")\n",
        "            resultados[\"erro_busca\"] = str(e)\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def analisar_credibilidade(self, artigo):\n",
        "        \"\"\"Análise combinada de credibilidade baseada em características do texto e fonte.\"\"\"\n",
        "        if not artigo or not artigo.get('conteudo'):\n",
        "            return {\"erro\": \"Artigo não disponível para análise\"}\n",
        "\n",
        "        resultados = {}\n",
        "        conteudo = artigo['conteudo']\n",
        "\n",
        "        # 1. Verificação de fonte/domínio\n",
        "        info_fonte = self.verificar_fonte(artigo['url'])\n",
        "        resultados.update(info_fonte)\n",
        "\n",
        "        # 2. Copiar informações básicas\n",
        "        resultados[\"titulo\"] = artigo['titulo']\n",
        "        resultados[\"data_publicacao\"] = artigo['data']\n",
        "        resultados[\"autores\"] = artigo['autores']\n",
        "        resultados[\"url\"] = artigo['url']\n",
        "\n",
        "        # 3. Análise básica do conteúdo\n",
        "        palavras = len(conteudo.split())\n",
        "        resultados[\"num_palavras\"] = palavras\n",
        "        if palavras < 100:\n",
        "            resultados[\"alerta_tamanho\"] = \"Texto muito curto, pode não ser um artigo completo\"\n",
        "\n",
        "        # 4. Análise detalhada da linguagem\n",
        "        analise_linguagem = self.analisar_linguagem(conteudo)\n",
        "        resultados.update(analise_linguagem)\n",
        "\n",
        "        # 5. Verificação de citações\n",
        "        citacoes = re.findall(r'\"([^\"]+)\"', conteudo)\n",
        "        resultados[\"num_citacoes\"] = len(citacoes)\n",
        "\n",
        "        if len(citacoes) == 0:\n",
        "            resultados[\"alerta_fontes\"] = \"Nenhuma citação direta encontrada\"\n",
        "\n",
        "        # 6. Verificação de URLs externos (referências)\n",
        "        urls = re.findall(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', conteudo)\n",
        "        resultados[\"num_urls\"] = len(urls)\n",
        "\n",
        "        # 7. Análise de imagens\n",
        "        resultados_imagens = self.verificar_imagens(artigo.get('imagens', []))\n",
        "        resultados.update(resultados_imagens)\n",
        "\n",
        "        # 8. Verificação cruzada com outras notícias\n",
        "        resultados_similares = self.buscar_noticias_similares(\n",
        "            artigo['titulo'],\n",
        "            artigo.get('keywords', [])\n",
        "        )\n",
        "        resultados.update(resultados_similares)\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def verificar_url(self, url):\n",
        "        \"\"\"Função principal para verificar a credibilidade de uma notícia a partir da URL.\"\"\"\n",
        "        resultados = {\"url\": url}\n",
        "\n",
        "        # 1. Baixar o artigo\n",
        "        artigo = self.baixar_artigo(url)\n",
        "        if not artigo:\n",
        "            resultados[\"erro\"] = \"Não foi possível baixar o artigo.\"\n",
        "            return resultados\n",
        "\n",
        "        # 2. Analisar a credibilidade do artigo\n",
        "        analise_credibilidade = self.analisar_credibilidade(artigo)\n",
        "        resultados.update(analise_credibilidade)\n",
        "\n",
        "        return resultados\n",
        "\n",
        "def analisar_url(url):\n",
        "    \"\"\"Função para analisar uma URL específica.\"\"\"\n",
        "    verificador = VerificadorNoticiasAvancado()\n",
        "    resultados = verificador.verificar_url(url)\n",
        "\n",
        "    return resultados\n",
        "\n",
        "# Exemplo de uso (para testar no mesmo arquivo)\n",
        "if __name__ == \"__main__\":\n",
        "    url_para_analisar = \"https://www.msn.com/pt-br/noticias/brasil/casa-branca-aponta-que-suposta-verdade-sobre-a-origem-da-covid-19-tem-rela%C3%A7%C3%A3o-com-vazamento-em-laborat%C3%B3rio-na-china/ar-AA1DbpaW?ocid=BingNewsSerp\"\n",
        "\n",
        "    analise = analisar_url(url_para_analisar)\n",
        "\n",
        "    # Imprimir resultados de forma organizada (exemplo)\n",
        "    print(\"\\n--- Resultados da Análise ---\")\n",
        "    print(f\"Título: {analise.get('titulo', 'N/A')}\")\n",
        "    print(f\"URL: {analise.get('url', 'N/A')}\")\n",
        "    print(f\"Fonte: {analise.get('status_fonte', 'N/A')}\")\n",
        "    print(f\"Data de Publicação: {analise.get('data_publicacao', 'N/A')}\")\n",
        "    print(f\"Número de Palavras: {analise.get('num_palavras', 'N/A')}\")\n",
        "    print(f\"Alertas: {analise.get('alertas', 'Nenhum')}\")  # Adapte conforme a estrutura real\n",
        "\n",
        "    if \"erro\" in analise:\n",
        "        print(f\"Erro: {analise['erro']}\")\n",
        "\n",
        "def verificar_url(url):\n",
        "    \"\"\"Função para analisar uma URL.\"\"\"\n",
        "    return analisar_url(url)\n",
        "\n",
        "print(\"Verificador de Notícias Avançado inicializado!\")\n",
        "\n",
        "# Exemplo de uso da função verificar_url\n",
        "url_para_analisar = \"https://www.msn.com/pt-br/noticias/brasil/casa-branca-aponta-que-suposta-verdade-sobre-a-origem-da-covid-19-tem-rela%C3%A7%C3%A3o-com-vazamento-em-laborat%C3%B3rio-na-china/ar-AA1DbpaW?ocid=BingNewsSerp\"\n",
        "resultados = verificar_url(url_para_analisar)\n",
        "print(json.dumps(resultados, indent=4, ensure_ascii=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrRBqVyto4ix",
        "outputId": "6cd7801e-9297-4243-8020-31dd7f7b91ff"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting verificacao_simples.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "python verificacao_simples.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "kO7Gpy26ooDE",
        "outputId": "96981adf-c026-4ad8-85f7-a6fcf24673be"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-11-212b1e80bbf7>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-212b1e80bbf7>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    python verificacao_simples.py\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run verificacao_simples.py\n",
        "\n",
        "# verificacao_simples.py\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import ssl\n",
        "import urllib3\n",
        "import json\n",
        "from urllib.parse import urlparse\n",
        "import hashlib\n",
        "from collections import Counter\n",
        "import time\n",
        "\n",
        "# Desabilitar avisos SSL\n",
        "urllib3.disable_warnings()\n",
        "\n",
        "class VerificadorNoticiasAvancado:\n",
        "    def __init__(self):\n",
        "        print(\"Verificador de Notícias Avançado inicializado!\")\n",
        "        # Carregar listas de fontes confiáveis e não confiáveis\n",
        "        self.fontes_confiaveis = [\n",
        "            'g1.globo.com', 'bbc.com', 'bbc.co.uk', 'reuters.com', 'apnews.com',\n",
        "            'nytimes.com', 'washingtonpost.com', 'estadao.com.br', 'folha.uol.com.br',\n",
        "            'cnn.com', 'valor.globo.com', 'info.abril.com.br', 'oglobo.globo.com',\n",
        "            'dw.com', 'npr.org', 'tecmundo.com.br', 'canaltech.com.br', 'olhardigital.com.br'\n",
        "        ]\n",
        "\n",
        "        self.fontes_suspeitas = [\n",
        "            'theonion.com', 'sensacionalista.com.br', 'breitbart.com', 'infowars.com',\n",
        "            'clickhole.com', 'nationalreport.net', 'worldnewsdailyreport.com',\n",
        "            'dailycurrant.com', 'empirenews.net'\n",
        "        ]\n",
        "\n",
        "        # Palavras para análise linguística\n",
        "        self.palavras_sensacionalistas = {\n",
        "            \"chocante\": 2, \"inacreditável\": 2, \"surpreendente\": 1, \"sensacional\": 2,\n",
        "            \"impressionante\": 1, \"alarmante\": 1.5, \"exclusivo\": 1, \"urgente\": 1.5,\n",
        "            \"shocking\": 2, \"unbelievable\": 2, \"amazing\": 1, \"surprising\": 1,\n",
        "            \"imperdível\": 1.5, \"incrível\": 1.5, \"absurdo\": 1.5, \"escandaloso\": 2,\n",
        "            \"bomba\": 1.5, \"inédito\": 1, \"revelado\": 1, \"segredo\": 1.5,\n",
        "            \"polêmico\": 1.5, \"controverso\": 1, \"escândalo\": 1.5\n",
        "        }\n",
        "\n",
        "        self.palavras_incerteza = [\n",
        "            \"talvez\", \"possivelmente\", \"provavelmente\", \"pode ser\", \"especula-se\",\n",
        "            \"acredita-se\", \"sugere\", \"alega\", \"supostamente\", \"rumores\",\n",
        "            \"maybe\", \"possibly\", \"probably\", \"might be\", \"it is speculated\",\n",
        "            \"it is believed\", \"suggests\", \"allegedly\", \"supposedly\", \"rumors\"\n",
        "        ]\n",
        "\n",
        "        self.expressoes_opinativas = [\n",
        "            \"eu acho\", \"na minha opinião\", \"parece que\", \"acredito que\",\n",
        "            \"i think\", \"in my opinion\", \"it seems that\", \"i believe\"\n",
        "        ]\n",
        "\n",
        "    def extrair_dominio(self, url):\n",
        "        \"\"\"Extrai o domínio base da URL.\"\"\"\n",
        "        parsed_url = urlparse(url)\n",
        "        domain = parsed_url.netloc\n",
        "        return domain\n",
        "\n",
        "    def verificar_fonte(self, url):\n",
        "        \"\"\"Verifica o domínio da URL contra as listas de fontes.\"\"\"\n",
        "        dominio = self.extrair_dominio(url)\n",
        "        resultado = {}\n",
        "\n",
        "        # Verificar se o domínio está nas listas\n",
        "        if any(dominio.endswith(fonte) for fonte in self.fontes_confiaveis):\n",
        "            resultado[\"status_fonte\"] = \"confiável\"\n",
        "            resultado[\"pontos_fonte\"] = 20\n",
        "        elif any(dominio.endswith(fonte) for fonte in self.fontes_suspeitas):\n",
        "            resultado[\"status_fonte\"] = \"suspeita\"\n",
        "            resultado[\"pontos_fonte\"] = -30\n",
        "            resultado[\"alerta_fonte\"] = \"Fonte conhecida por conteúdo satírico ou de baixa credibilidade\"\n",
        "        else:\n",
        "            resultado[\"status_fonte\"] = \"não categorizada\"\n",
        "            resultado[\"pontos_fonte\"] = 0\n",
        "\n",
        "        resultado[\"dominio\"] = dominio\n",
        "        return resultado\n",
        "\n",
        "    def baixar_artigo(self, url):\n",
        "        \"\"\"Baixa e extrai o conteúdo básico de um artigo a partir da URL\"\"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers, verify=False, timeout=10)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Tenta extrair o título\n",
        "            titulo = soup.title.text.strip() if soup.title else \"Título não encontrado\"\n",
        "\n",
        "            # Tenta extrair o conteúdo do artigo (abordagem mais abrangente)\n",
        "            content = \"\"\n",
        "\n",
        "            # Procura por conteúdo em elementos comuns\n",
        "            for tag in ['article', 'main', '.content', '.post', '.entry', '#content', '.article-body', '.news-text']:\n",
        "                if tag.startswith('.') or tag.startswith('#'):\n",
        "                    elements = soup.select(tag)\n",
        "                else:\n",
        "                    elements = soup.find_all(tag)\n",
        "\n",
        "                if elements:\n",
        "                    for element in elements:\n",
        "                        content += element.get_text(separator=' ', strip=True) + \" \"\n",
        "                    break\n",
        "\n",
        "            # Se não encontrou conteúdo nos elementos específicos, pega todos os parágrafos\n",
        "            if not content:\n",
        "                paragraphs = soup.find_all('p')\n",
        "                content = ' '.join([p.get_text(strip=True) for p in paragraphs])\n",
        "\n",
        "            # Tenta encontrar a data\n",
        "            data = None\n",
        "            # Procura por metadados comuns\n",
        "            for meta in soup.find_all('meta'):\n",
        "                if meta.get('property') in ['article:published_time', 'og:published_time', 'publication_date']:\n",
        "                    data = meta.get('content')\n",
        "                    break\n",
        "\n",
        "            # Procura por elementos que possam conter autores\n",
        "            autores = []\n",
        "            for author_tag in ['author', 'byline', '.author', '.byline', '[rel=author]']:\n",
        "                if author_tag.startswith('.') or author_tag.startswith('['):\n",
        "                    elements = soup.select(author_tag)\n",
        "                else:\n",
        "                    elements = soup.find_all(author_tag)\n",
        "\n",
        "                if elements:\n",
        "                    for element in elements:\n",
        "                        autor = element.get_text(strip=True)\n",
        "                        if autor and len(autor) < 100:  # Evitar pegar conteúdo grande por engano\n",
        "                            autores.append(autor)\n",
        "\n",
        "            # Remover duplicações nos autores\n",
        "            autores = list(dict.fromkeys(autores))\n",
        "\n",
        "            # Extrair imagens\n",
        "            imagens = []\n",
        "            for img in soup.find_all('img'):\n",
        "                src = img.get('src')\n",
        "                alt = img.get('alt', '')\n",
        "                if src and not src.startswith('data:'):\n",
        "                    # Garantir URL completa\n",
        "                    if src.startswith('/'):\n",
        "                        parsed_url = urlparse(url)\n",
        "                        base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
        "                        src = base_url + src\n",
        "                    imagens.append({\n",
        "                        'url': src,\n",
        "                        'alt': alt\n",
        "                    })\n",
        "\n",
        "            # Extrair palavras-chave\n",
        "            keywords = []\n",
        "            meta_keywords = soup.find('meta', attrs={'name': 'keywords'})\n",
        "            if meta_keywords:\n",
        "                keywords = meta_keywords.get('content', '').split(',')\n",
        "                keywords = [k.strip() for k in keywords]\n",
        "\n",
        "            return {\n",
        "                'titulo': titulo,\n",
        "                'conteudo': content,\n",
        "                'data': data,\n",
        "                'autores': autores,\n",
        "                'url': url,\n",
        "                'imagens': imagens,\n",
        "                'keywords': keywords\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao baixar artigo: {e}\")\n",
        "            return None\n",
        "\n",
        "    def analisar_linguagem(self, texto):\n",
        "        \"\"\"Analisa o tom e a linguagem usada no texto.\"\"\"\n",
        "        resultados = {}\n",
        "\n",
        "        # Análise de sensacionalismo com pesos\n",
        "        count_sensacionalismo = 0\n",
        "        palavras_encontradas = []\n",
        "\n",
        "        for palavra, peso in self.palavras_sensacionalistas.items():\n",
        "            ocorrencias = len(re.findall(r'\\b' + palavra + r'\\b', texto.lower()))\n",
        "            if ocorrencias > 0:\n",
        "                count_sensacionalismo += ocorrencias * peso\n",
        "                palavras_encontradas.append(f\"{palavra} ({ocorrencias}x)\")\n",
        "\n",
        "        resultados[\"indice_sensacionalismo\"] = count_sensacionalismo\n",
        "        resultados[\"palavras_sensacionalistas\"] = palavras_encontradas\n",
        "\n",
        "        if count_sensacionalismo > 3:\n",
        "            resultados[\"alerta_sensacionalismo\"] = \"Alto índice de linguagem sensacionalista detectado\"\n",
        "        elif count_sensacionalismo > 1.5:\n",
        "            resultados[\"alerta_sensacionalismo\"] = \"Linguagem potencialmente sensacionalista detectada\"\n",
        "\n",
        "        # Análise de incerteza\n",
        "        count_incerteza = 0\n",
        "        palavras_incerteza = []\n",
        "\n",
        "        for palavra in self.palavras_incerteza:\n",
        "            ocorrencias = len(re.findall(r'\\b' + palavra + r'\\b', texto.lower()))\n",
        "            if ocorrencias > 0:\n",
        "                count_incerteza += ocorrencias\n",
        "                palavras_incerteza.append(palavra)\n",
        "\n",
        "        resultados[\"indice_incerteza\"] = count_incerteza\n",
        "        resultados[\"palavras_incerteza\"] = palavras_incerteza\n",
        "\n",
        "        if count_incerteza > 3:\n",
        "            resultados[\"alerta_incerteza\"] = \"Alto uso de termos de incerteza detectado\"\n",
        "\n",
        "        # Análise de opinião\n",
        "        count_opiniao = 0\n",
        "        expressoes_opiniao = []\n",
        "\n",
        "        for expressao in self.expressoes_opinativas:\n",
        "            ocorrencias = len(re.findall(r'\\b' + expressao + r'\\b', texto.lower()))\n",
        "            if ocorrencias > 0:\n",
        "                count_opiniao += ocorrencias\n",
        "                expressoes_opiniao.append(expressao)\n",
        "\n",
        "        resultados[\"indice_opiniao\"] = count_opiniao\n",
        "        resultados[\"expressoes_opiniao\"] = expressoes_opiniao\n",
        "\n",
        "        if count_opiniao > 2:\n",
        "            resultados[\"alerta_opiniao\"] = \"Texto com forte caráter opinativo detectado\"\n",
        "\n",
        "        # Análise de exclamações (indicativo de sensacionalismo)\n",
        "        exclamacoes = len(re.findall(r'!', texto))\n",
        "        resultados[\"num_exclamacoes\"] = exclamacoes\n",
        "\n",
        "        if exclamacoes > 5:\n",
        "            resultados[\"alerta_exclamacoes\"] = \"Uso excessivo de pontos de exclamação\"\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def verificar_imagens(self, imagens):\n",
        "        \"\"\"Analisa as imagens associadas ao artigo.\"\"\"\n",
        "        resultados = {}\n",
        "\n",
        "        if not imagens:\n",
        "            resultados[\"alerta_imagens\"] = \"Nenhuma imagem encontrada no artigo\"\n",
        "            return resultados\n",
        "\n",
        "        resultados[\"num_imagens\"] = len(imagens)\n",
        "\n",
        "        # Verificar descrições das imagens (alt text)\n",
        "        imagens_sem_descricao = sum(1 for img in imagens if not img.get('alt'))\n",
        "        resultados[\"imagens_sem_descricao\"] = imagens_sem_descricao\n",
        "\n",
        "        if imagens_sem_descricao == len(imagens) and len(imagens) > 0:\n",
        "            resultados[\"alerta_imagens_descricao\"] = \"Nenhuma das imagens possui descrição adequada\"\n",
        "\n",
        "        # Calcular hash básico das URLs de imagens para potencial comparação futura\n",
        "        hashes_urls = [hashlib.md5(img['url'].encode()).hexdigest()[:8] for img in imagens]\n",
        "        resultados[\"hashes_imagens\"] = hashes_urls\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def buscar_noticias_similares(self, titulo, keywords):\n",
        "        \"\"\"Busca por notícias similares na web usando palavras-chave do artigo.\"\"\"\n",
        "        resultados = {}\n",
        "\n",
        "        try:\n",
        "            # Preparar termos de busca\n",
        "            termos_busca = titulo\n",
        "            if keywords:\n",
        "                termos_busca += \" \" + \" \".join(keywords[:3])  # Limitar a 3 keywords\n",
        "\n",
        "            # Simular resultados para demonstração\n",
        "            # Em uma implementação real, você usaria uma API como Google News ou similar\n",
        "            resultados[\"artigos_similares\"] = 5  # Simulando 5 artigos similares\n",
        "            resultados[\"similaridade\"] = \"média\"  # baixa, média, alta\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao buscar notícias similares: {e}\")\n",
        "            resultados[\"erro_busca\"] = str(e)\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def analisar_credibilidade(self, artigo):\n",
        "        \"\"\"Análise combinada de credibilidade baseada em características do texto e fonte.\"\"\"\n",
        "        if not artigo or not artigo.get('conteudo'):\n",
        "            return {\"erro\": \"Artigo não disponível para análise\"}\n",
        "\n",
        "        resultados = {}\n",
        "        conteudo = artigo['conteudo']\n",
        "\n",
        "        # 1. Verificação de fonte/domínio\n",
        "        info_fonte = self.verificar_fonte(artigo['url'])\n",
        "        resultados.update(info_fonte)\n",
        "\n",
        "        # 2. Copiar informações básicas\n",
        "        resultados[\"titulo\"] = artigo['titulo']\n",
        "        resultados[\"data_publicacao\"] = artigo['data']\n",
        "        resultados[\"autores\"] = artigo['autores']\n",
        "        resultados[\"url\"] = artigo['url']\n",
        "\n",
        "        # 3. Análise básica do conteúdo\n",
        "        palavras = len(conteudo.split())\n",
        "        resultados[\"num_palavras\"] = palavras\n",
        "        if palavras < 100:\n",
        "            resultados[\"alerta_tamanho\"] = \"Texto muito curto, pode não ser um artigo completo\"\n",
        "\n",
        "        # 4. Análise detalhada da linguagem\n",
        "        analise_linguagem = self.analisar_linguagem(conteudo)\n",
        "        resultados.update(analise_linguagem)\n",
        "\n",
        "        # 5. Verificação de citações\n",
        "        citacoes = re.findall(r'\"([^\"]+)\"', conteudo)\n",
        "        resultados[\"num_citacoes\"] = len(citacoes)\n",
        "\n",
        "        if len(citacoes) == 0:\n",
        "            resultados[\"alerta_fontes\"] = \"Nenhuma citação direta encontrada\"\n",
        "\n",
        "        # 6. Verificação de URLs externos (referências)\n",
        "        urls = re.findall(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', conteudo)\n",
        "        resultados[\"num_urls\"] = len(urls)\n",
        "\n",
        "        # 7. Análise de imagens\n",
        "        resultados_imagens = self.verificar_imagens(artigo.get('imagens', []))\n",
        "        resultados.update(resultados_imagens)\n",
        "\n",
        "        # 8. Verificação cruzada com outras notícias\n",
        "        resultados_similares = self.buscar_noticias_similares(\n",
        "            artigo['titulo'],\n",
        "            artigo.get('keywords', [])\n",
        "        )\n",
        "        resultados.update(resultados_similares)\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def verificar_url(self, url):\n",
        "        \"\"\"Função principal para verificar a credibilidade de uma notícia a partir da URL.\"\"\"\n",
        "        resultados = {\"url\": url}\n",
        "\n",
        "        # 1. Baixar o artigo\n",
        "        artigo = self.baixar_artigo(url)\n",
        "        if not artigo:\n",
        "            resultados[\"erro\"] = \"Não foi possível baixar o artigo.\"\n",
        "            return resultados\n",
        "\n",
        "        # 2. Analisar a credibilidade do artigo\n",
        "        analise_credibilidade = self.analisar_credibilidade(artigo)\n",
        "        resultados.update(analise_credibilidade)\n",
        "\n",
        "        return resultados\n",
        "\n",
        "def analisar_url(url):\n",
        "    \"\"\"Função para analisar uma URL específica.\"\"\"\n",
        "    verificador = VerificadorNoticiasAvancado()\n",
        "    resultados = verificador.verificar_url(url)\n",
        "\n",
        "    return resultados\n",
        "\n",
        "# Exemplo de uso (para testar no mesmo arquivo)\n",
        "if __name__ == \"__main__\":\n",
        "    url_para_analisar = \"https://www.msn.com/pt-br/noticias/brasil/casa-branca-aponta-que-suposta-verdade-sobre-a-origem-da-covid-19-tem-rela%C3%A7%C3%A3o-com-vazamento-em-laborat%C3%B3rio-na-china/ar-AA1DbpaW?ocid=BingNewsSerp\"\n",
        "\n",
        "    analise = analisar_url(url_para_analisar)\n",
        "\n",
        "    # Imprimir resultados de forma organizada (exemplo)\n",
        "    print(\"\\n--- Resultados da Análise ---\")\n",
        "    print(f\"Título: {analise.get('titulo', 'N/A')}\")\n",
        "    print(f\"URL: {analise.get('url', 'N/A')}\")\n",
        "    print(f\"Fonte: {analise.get('status_fonte', 'N/A')}\")\n",
        "    print(f\"Data de Publicação: {analise.get('data_publicacao', 'N/A')}\")\n",
        "    print(f\"Número de Palavras: {analise.get('num_palavras', 'N/A')}\")\n",
        "    print(f\"Alertas: {analise.get('alertas', 'Nenhum')}\")  # Adapte conforme a estrutura real\n",
        "\n",
        "    if \"erro\" in analise:\n",
        "        print(f\"Erro: {analise['erro']}\")\n",
        "\n",
        "def verificar_url(url):\n",
        "    \"\"\"Função para analisar uma URL.\"\"\"\n",
        "    return analisar_url(url)\n",
        "\n",
        "print(\"Verificador de Notícias Avançado inicializado!\")\n",
        "\n",
        "# Exemplo de uso da função verificar_url\n",
        "url_para_analisar = \"https://www.msn.com/pt-br/noticias/brasil/casa-branca-aponta-que-suposta-verdade-sobre-a-origem-da-covid-19-tem-rela%C3%A7%C3%A3o-com-vazamento-em-laborat%C3%B3rio-na-china/ar-AA1DbpaW?ocid=BingNewsSerp\"\n",
        "resultados = verificar_url(url_para_analisar)\n",
        "print(json.dumps(resultados, indent=4, ensure_ascii=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bWFQQBxoRIh",
        "outputId": "3301f743-7a8d-4ea4-8ad0-45d5106e9438"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verificador de notícias avançado - use a função verificar_url(sua_url) para analisar uma notícia\n",
            "Verificador de Notícias Avançado inicializado!\n",
            "\n",
            "--- Resultados da Análise ---\n",
            "Título: N/A\n",
            "URL: https://www.msn.com/pt-br/noticias/brasil/casa-branca-aponta-que-suposta-verdade-sobre-a-origem-da-covid-19-tem-rela%C3%A7%C3%A3o-com-vazamento-em-laborat%C3%B3rio-na-china/ar-AA1DbpaW?ocid=BingNewsSerp\n",
            "Fonte: N/A\n",
            "Data de Publicação: N/A\n",
            "Número de Palavras: N/A\n",
            "Alertas: Nenhum\n",
            "Erro: Artigo não disponível para análise\n",
            "Verificador de Notícias Avançado inicializado!\n",
            "Verificador de Notícias Avançado inicializado!\n",
            "{\n",
            "    \"url\": \"https://www.msn.com/pt-br/noticias/brasil/casa-branca-aponta-que-suposta-verdade-sobre-a-origem-da-covid-19-tem-rela%C3%A7%C3%A3o-com-vazamento-em-laborat%C3%B3rio-na-china/ar-AA1DbpaW?ocid=BingNewsSerp\",\n",
            "    \"erro\": \"Artigo não disponível para análise\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "python verificar_noticias.py \"https://www.msn.com/pt-br/noticias/brasil/casa-branca-aponta-que-suposta-verdade-sobre-a-origem-da-covid-19-tem-rela%C3%A7%C3%A3o-com-vazamento-em-laborat%C3%B3rio-na-china/ar-AA1DbpaW?ocid=BingNewsSerp\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "bOQzz0JqnNIe",
        "outputId": "07dece41-8488-4bc3-bbd4-441dd529f89a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-9-2cd42356226d>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-2cd42356226d>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    python verificar_noticias.py \"https://www.msn.com/pt-br/noticias/brasil/casa-branca-aponta-que-suposta-verdade-sobre-a-origem-da-covid-19-tem-rela%C3%A7%C3%A3o-com-vazamento-em-laborat%C3%B3rio-na-china/ar-AA1DbpaW?ocid=BingNewsSerp\"\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers newspaper3k beautifulsoup4 requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOAabwjMnD0U",
        "outputId": "bda064b1-891f-4786-ab84-230f2be05dd8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: newspaper3k in /usr/local/lib/python3.11/dist-packages (0.2.8)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (11.1.0)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (1.3.0)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (5.3.2)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (3.9.1)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (6.0.11)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (5.2.0)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (0.0.4)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (0.35.1)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (2.8.2)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.11/dist-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (1.4.2)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.11/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run verificacao_simples.py\n",
        "verificar_url(\"https://www.msn.com/pt-br/noticias/brasil/casa-branca-aponta-que-suposta-verdade-sobre-a-origem-da-covid-19-tem-rela%C3%A7%C3%A3o-com-vazamento-em-laborat%C3%B3rio-na-china/ar-AA1DbpaW?ocid=BingNewsSerp\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7K9C_qHJlIco",
        "outputId": "e0c77252-3282-4554-fdcf-1223ef5485a3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verificador de notícias avançado - use a função verificar_url(sua_url) para analisar uma notícia\n",
            "Verificador de Notícias Avançado inicializado!\n",
            "Analisando URL: https://www.msn.com/pt-br/noticias/brasil/casa-branca-aponta-que-suposta-verdade-sobre-a-origem-da-covid-19-tem-rela%C3%A7%C3%A3o-com-vazamento-em-laborat%C3%B3rio-na-china/ar-AA1DbpaW?ocid=BingNewsSerp\n",
            "\n",
            "==================================================\n",
            "RELATÓRIO DE VERIFICAÇÃO DE NOTÍCIA\n",
            "==================================================\n",
            "URL: https://www.msn.com/pt-br/noticias/brasil/casa-branca-aponta-que-suposta-verdade-sobre-a-origem-da-covid-19-tem-rela%C3%A7%C3%A3o-com-vazamento-em-laborat%C3%B3rio-na-china/ar-AA1DbpaW?ocid=BingNewsSerp\n",
            "Domínio: N/A\n",
            "Status da fonte: N/A\n",
            "--------------------------------------------------\n",
            "Título: N/A\n",
            "Data de publicação: N/A\n",
            "Autores: N/A\n",
            "Número de palavras: 0\n",
            "--------------------------------------------------\n",
            "MÉTRICAS:\n",
            "• Citações: 0\n",
            "• URLs referenciadas: 0\n",
            "• Índice de sensacionalismo: 0\n",
            "• Índice de incerteza: 0\n",
            "• Índice opinativo: 0\n",
            "• Pontos de exclamação: 0\n",
            "• Imagens: 0\n",
            "• Artigos similares encontrados: 0\n",
            "--------------------------------------------------\n",
            "PONTUAÇÃO DE CREDIBILIDADE: 70/100\n",
            "AVALIAÇÃO: Boa credibilidade\n",
            "Tempo de análise: 0.12 segundos\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'erro': 'Artigo não disponível para análise',\n",
              " 'pontuacao_credibilidade': 70,\n",
              " 'avaliacao': 'Boa credibilidade'}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run verificacao_simples.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7Y5FMs3lBWg",
        "outputId": "9522c114-bcc7-4c39-8c86-7ffc33bf566c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verificador de notícias avançado - use a função verificar_url(sua_url) para analisar uma notícia\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERptorkIa3bH",
        "outputId": "a170aecf-fcc9-48cb-cc94-7beca90907e8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZxw0GQ-Y0oC",
        "outputId": "7d82766e-39b0-4d35-9fc1-0fda1e055b37"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtsSXjUmWL5U",
        "outputId": "dbf18505-20f7-4a92-f472-a17d98b9503e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install functions-framework"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        },
        "id": "EvUKXp5zV7aY",
        "outputId": "a1219029-fa45-40ef-8445-3f58000ef38b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting functions-framework\n",
            "  Downloading functions_framework-3.8.2-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: flask<4.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from functions-framework) (3.1.0)\n",
            "Requirement already satisfied: click<9.0,>=7.0 in /usr/local/lib/python3.11/dist-packages (from functions-framework) (8.1.8)\n",
            "Collecting watchdog>=1.0.0 (from functions-framework)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cloudevents<2.0.0,>=1.2.0 (from functions-framework)\n",
            "  Downloading cloudevents-1.11.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: Werkzeug<4.0.0,>=0.14 in /usr/local/lib/python3.11/dist-packages (from functions-framework) (3.1.3)\n",
            "Collecting gunicorn>=22.0.0 (from functions-framework)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting deprecation<3.0,>=2.0 (from cloudevents<2.0.0,>=1.2.0->functions-framework)\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask<4.0,>=1.0->functions-framework) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask<4.0,>=1.0->functions-framework) (2.2.0)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask<4.0,>=1.0->functions-framework) (1.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gunicorn>=22.0.0->functions-framework) (24.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Werkzeug<4.0.0,>=0.14->functions-framework) (3.0.2)\n",
            "Downloading functions_framework-3.8.2-py3-none-any.whl (35 kB)\n",
            "Downloading cloudevents-1.11.0-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: watchdog, gunicorn, deprecation, cloudevents, functions-framework\n",
            "Successfully installed cloudevents-1.11.0 deprecation-2.1.0 functions-framework-3.8.2 gunicorn-23.0.0 watchdog-6.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "898b8d4598ef44e1aacf97b87d62012a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnbnNq73Vs8O",
        "outputId": "32ec0483-bb7a-4394-a3d4-a0d8cc228e30"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-cloud-functions-framework"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMJN4dMkVei8",
        "outputId": "cdf72631-8fa6-4ee7-f274-51f27400b5d6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement google-cloud-functions-framework (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for google-cloud-functions-framework\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install google-cloud-functions-framework"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qB7rsqtVaJX",
        "outputId": "a9d61cca-fe65-406d-8937-d93101cd3bbe"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement google-cloud-functions-framework (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for google-cloud-functions-framework\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t__0lhvAVPYb",
        "outputId": "3368fd8d-33e2-4679-f1b0-75cebc284be9"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/main.py\", line 2, in <module>\n",
            "    import functions_framework\n",
            "ModuleNotFoundError: No module named 'functions_framework'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "python main.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "Yhaeog0aU7Tp",
        "outputId": "b0bdf61f-7368-4d3c-f1bd-595bbae15190"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-29-a5b85dd88f47>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-29-a5b85dd88f47>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    python main.py\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile verificacao_avancada.py\n",
        "# -*- coding: utf-8 -*-\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import ssl\n",
        "import urllib3\n",
        "import json\n",
        "from urllib.parse import urlparse, urljoin # Added urljoin\n",
        "import hashlib\n",
        "from collections import Counter\n",
        "import time\n",
        "import logging # Added logging\n",
        "\n",
        "# Desabilitar avisos SSL (Use com cautela em produção)\n",
        "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
        "\n",
        "# Configurar logging básico\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "class VerificadorNoticiasAvancado:\n",
        "    def __init__(self):\n",
        "        logging.info(\"Verificador de Notícias Avançado inicializado!\")\n",
        "        # Carregar listas de fontes confiáveis e não confiáveis\n",
        "        self.fontes_confiaveis = [\n",
        "            'g1.globo.com', 'bbc.com', 'bbc.co.uk', 'reuters.com', 'apnews.com',\n",
        "            'nytimes.com', 'washingtonpost.com', 'estadao.com.br', 'folha.uol.com.br',\n",
        "            'cnn.com', 'valor.globo.com', 'info.abril.com.br', 'oglobo.globo.com',\n",
        "            'dw.com', 'npr.org', 'tecmundo.com.br', 'canaltech.com.br', 'olhardigital.com.br'\n",
        "        ]\n",
        "\n",
        "        self.fontes_suspeitas = [\n",
        "            'theonion.com', 'sensacionalista.com.br', 'breitbart.com', 'infowars.com',\n",
        "            'clickhole.com', 'nationalreport.net', 'worldnewsdailyreport.com',\n",
        "            'dailycurrant.com', 'empirenews.net'\n",
        "        ]\n",
        "\n",
        "        # Palavras para análise linguística\n",
        "        self.palavras_sensacionalistas = {\n",
        "            \"chocante\": 2, \"inacreditável\": 2, \"surpreendente\": 1, \"sensacional\": 2,\n",
        "            \"impressionante\": 1, \"alarmante\": 1.5, \"exclusivo\": 1, \"urgente\": 1.5,\n",
        "            \"shocking\": 2, \"unbelievable\": 2, \"amazing\": 1, \"surprising\": 1,\n",
        "            \"imperdível\": 1.5, \"incrível\": 1.5, \"absurdo\": 1.5, \"escandaloso\": 2,\n",
        "            \"bomba\": 1.5, \"inédito\": 1, \"revelado\": 1, \"segredo\": 1.5,\n",
        "            \"polêmico\": 1.5, \"controverso\": 1, \"escândalo\": 1.5\n",
        "        }\n",
        "\n",
        "        self.palavras_incerteza = [\n",
        "            \"talvez\", \"possivelmente\", \"provavelmente\", \"pode ser\", \"especula-se\",\n",
        "            \"acredita-se\", \"sugere\", \"alega\", \"supostamente\", \"rumores\",\n",
        "            \"maybe\", \"possibly\", \"probably\", \"might be\", \"it is speculated\",\n",
        "            \"it is believed\", \"suggests\", \"allegedly\", \"supposedly\", \"rumors\"\n",
        "        ]\n",
        "\n",
        "        self.expressoes_opinativas = [\n",
        "            \"eu acho\", \"na minha opinião\", \"parece que\", \"acredito que\",\n",
        "            \"i think\", \"in my opinion\", \"it seems that\", \"i believe\"\n",
        "        ]\n",
        "\n",
        "    def extrair_dominio(self, url):\n",
        "        \"\"\"Extrai o domínio base da URL.\"\"\"\n",
        "        try:\n",
        "            parsed_url = urlparse(url)\n",
        "            domain = parsed_url.netloc.replace('www.', '') # Remove www. for consistency\n",
        "            return domain\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Erro ao extrair domínio de {url}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def verificar_fonte(self, url):\n",
        "        \"\"\"Verifica o domínio da URL contra as listas de fontes.\"\"\"\n",
        "        dominio = self.extrair_dominio(url)\n",
        "        resultado = {\"dominio\": dominio} # Initialize with domain\n",
        "\n",
        "        if not dominio:\n",
        "             resultado[\"status_fonte\"] = \"erro_dominio\"\n",
        "             resultado[\"pontos_fonte\"] = 0\n",
        "             resultado[\"alerta_fonte\"] = \"Não foi possível extrair o domínio da URL.\"\n",
        "             return resultado\n",
        "\n",
        "        # Verificar se o domínio está nas listas\n",
        "        if any(dominio == fonte for fonte in self.fontes_confiaveis):\n",
        "            resultado[\"status_fonte\"] = \"confiável\"\n",
        "            resultado[\"pontos_fonte\"] = 20\n",
        "        elif any(dominio == fonte for fonte in self.fontes_suspeitas):\n",
        "            resultado[\"status_fonte\"] = \"suspeita\"\n",
        "            resultado[\"pontos_fonte\"] = -30\n",
        "            resultado[\"alerta_fonte\"] = \"Fonte conhecida por conteúdo satírico ou de baixa credibilidade.\"\n",
        "        else:\n",
        "            # Check subdomains of reliable sources (e.g., news.google.com)\n",
        "            is_subdomain_confiavel = False\n",
        "            for fonte_confiavel in self.fontes_confiaveis:\n",
        "                 if dominio.endswith('.' + fonte_confiavel):\n",
        "                      is_subdomain_confiavel = True\n",
        "                      break\n",
        "            if is_subdomain_confiavel:\n",
        "                 resultado[\"status_fonte\"] = \"confiável (subdomínio)\"\n",
        "                 resultado[\"pontos_fonte\"] = 15 # Slightly less than main domain\n",
        "            else:\n",
        "                 resultado[\"status_fonte\"] = \"não categorizada\"\n",
        "                 resultado[\"pontos_fonte\"] = 0\n",
        "\n",
        "        return resultado\n",
        "\n",
        "    def baixar_artigo(self, url):\n",
        "        \"\"\"Baixa e extrai o conteúdo básico de um artigo a partir da URL\"\"\"\n",
        "        logging.info(f\"Tentando baixar artigo de: {url}\")\n",
        "        try:\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
        "                'Accept-Language': 'en-US,en;q=0.9,pt-BR;q=0.8,pt;q=0.7'\n",
        "            }\n",
        "            # Usar verify=False é um risco de segurança, mas necessário para alguns sites com SSL mal configurado.\n",
        "            # Idealmente, configurar certifi ou similar.\n",
        "            response = requests.get(url, headers=headers, verify=False, timeout=15) # Increased timeout\n",
        "            response.raise_for_status() # Levanta exceção para erros HTTP (4xx ou 5xx)\n",
        "\n",
        "            # Detectar codificação para evitar problemas com caracteres especiais\n",
        "            response.encoding = response.apparent_encoding\n",
        "            html_content = response.text\n",
        "\n",
        "            soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "            # Tenta extrair o título\n",
        "            titulo = soup.title.text.strip() if soup.title else \"Título não encontrado\"\n",
        "\n",
        "            # Tenta extrair o conteúdo do artigo (abordagem mais abrangente)\n",
        "            content_tags = ['article', 'main', '.content', '.post', '.entry', '#content', '.article-body', '.news-text', '.story-content', '.main-content']\n",
        "            content = \"\"\n",
        "            found_content = False\n",
        "\n",
        "            for tag_selector in content_tags:\n",
        "                elements = soup.select(tag_selector)\n",
        "                if elements:\n",
        "                    logging.info(f\"Encontrado conteúdo com seletor: {tag_selector}\")\n",
        "                    for element in elements:\n",
        "                        # Remove elementos indesejados como scripts, styles, menus, footers\n",
        "                        for unwanted in element.select('script, style, nav, footer, .menu, .footer, .sidebar, .ad, .advertisement, .related-links'):\n",
        "                            unwanted.decompose()\n",
        "                        content += element.get_text(separator=' ', strip=True) + \" \"\n",
        "                    if len(content.split()) > 50: # Consider content found if it has reasonable length\n",
        "                         found_content = True\n",
        "                         break # Stop searching if good content is found\n",
        "\n",
        "            # Fallback: Pega todos os parágrafos se a busca específica falhar\n",
        "            if not found_content or len(content.split()) < 50:\n",
        "                logging.info(\"Seletores específicos falharam ou conteúdo curto, tentando todos <p>.\")\n",
        "                paragraphs = soup.find_all('p')\n",
        "                content = ' '.join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 20]) # Filter short paragraphs\n",
        "\n",
        "            if not content:\n",
        "                 logging.warning(f\"Não foi possível extrair o conteúdo principal de {url}\")\n",
        "                 # Fallback ainda mais geral: pegar o body todo (pode ser ruidoso)\n",
        "                 body = soup.find('body')\n",
        "                 if body:\n",
        "                      for unwanted in body.select('script, style, nav, footer, header, .menu, .footer, .sidebar, .ad, .advertisement'):\n",
        "                           unwanted.decompose()\n",
        "                      content = body.get_text(separator=' ', strip=True)\n",
        "\n",
        "            # Tenta encontrar a data\n",
        "            data = None\n",
        "            date_selectors = [\n",
        "                'meta[property=\"article:published_time\"]',\n",
        "                'meta[property=\"og:published_time\"]',\n",
        "                'meta[name=\"publication_date\"]',\n",
        "                'meta[name=\"date\"]',\n",
        "                'time[datetime]'\n",
        "            ]\n",
        "            for selector in date_selectors:\n",
        "                 element = soup.select_one(selector)\n",
        "                 if element:\n",
        "                      data = element.get('content') or element.get('datetime') or element.text\n",
        "                      if data:\n",
        "                           logging.info(f\"Data encontrada: {data}\")\n",
        "                           break\n",
        "\n",
        "            # Procura por elementos que possam conter autores\n",
        "            autores = []\n",
        "            author_selectors = [\n",
        "                'meta[name=\"author\"]',\n",
        "                '.author', '.byline', '[rel=\"author\"]', '.autor', '.nome-autor'\n",
        "                'a[href*=\"/author/\"]', 'span[class*=\"author\"]'\n",
        "            ]\n",
        "            processed_authors = set() # Evitar duplicação e processamento excessivo\n",
        "\n",
        "            for selector in author_selectors:\n",
        "                 elements = soup.select(selector)\n",
        "                 for element in elements:\n",
        "                      autor_text = None\n",
        "                      if element.name == 'meta':\n",
        "                           autor_text = element.get('content')\n",
        "                      else:\n",
        "                           autor_text = element.get_text(strip=True)\n",
        "\n",
        "                      if autor_text and len(autor_text) > 2 and len(autor_text) < 100: # Filtros básicos\n",
        "                           # Limpeza adicional\n",
        "                           autor_text = re.sub(r'(?i)por\\s+|by\\s+', '', autor_text).strip()\n",
        "                           if autor_text and autor_text.lower() not in processed_authors:\n",
        "                                autores.append(autor_text)\n",
        "                                processed_authors.add(autor_text.lower())\n",
        "\n",
        "\n",
        "            # Extrair imagens com URLs absolutas\n",
        "            imagens = []\n",
        "            processed_img_urls = set()\n",
        "            for img in soup.find_all('img'):\n",
        "                src = img.get('src')\n",
        "                alt = img.get('alt', '')\n",
        "                if src and not src.startswith('data:'): # Ignorar imagens embutidas\n",
        "                    # Garantir URL completa\n",
        "                    absolute_src = urljoin(url, src) # Constrói URL absoluta\n",
        "                    if absolute_src not in processed_img_urls:\n",
        "                         imagens.append({\n",
        "                             'url': absolute_src,\n",
        "                             'alt': alt.strip() if alt else ''\n",
        "                         })\n",
        "                         processed_img_urls.add(absolute_src)\n",
        "\n",
        "            # Extrair palavras-chave\n",
        "            keywords = []\n",
        "            meta_keywords = soup.find('meta', attrs={'name': re.compile(r'keywords', re.I)})\n",
        "            if meta_keywords and meta_keywords.get('content'):\n",
        "                keywords = [k.strip() for k in meta_keywords['content'].split(',') if k.strip()]\n",
        "\n",
        "            logging.info(f\"Artigo baixado: Título='{titulo[:50]}...', Palavras={len(content.split())}, Imagens={len(imagens)}\")\n",
        "            return {\n",
        "                'titulo': titulo,\n",
        "                'conteudo': content,\n",
        "                'data': data,\n",
        "                'autores': list(autores), # Converter set de volta para lista se necessário\n",
        "                'url': url,\n",
        "                'imagens': imagens,\n",
        "                'keywords': keywords,\n",
        "                'status': 'sucesso' # Adicionar status\n",
        "            }\n",
        "\n",
        "        except requests.exceptions.HTTPError as http_err:\n",
        "            logging.error(f\"Erro HTTP ao baixar {url}: {http_err} - Status: {http_err.response.status_code}\")\n",
        "            return {'status': 'erro_http', 'mensagem': str(http_err), 'status_code': http_err.response.status_code}\n",
        "        except requests.exceptions.ConnectionError as conn_err:\n",
        "             logging.error(f\"Erro de conexão ao baixar {url}: {conn_err}\")\n",
        "             return {'status': 'erro_conexao', 'mensagem': str(conn_err)}\n",
        "        except requests.exceptions.Timeout as timeout_err:\n",
        "            logging.error(f\"Timeout ao baixar {url}: {timeout_err}\")\n",
        "            return {'status': 'erro_timeout', 'mensagem': str(timeout_err)}\n",
        "        except requests.exceptions.RequestException as req_err:\n",
        "            logging.error(f\"Erro genérico de request ao baixar {url}: {req_err}\")\n",
        "            return {'status': 'erro_request', 'mensagem': str(req_err)}\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Erro inesperado ao processar {url}: {e}\", exc_info=True) # Log traceback\n",
        "            return {'status': 'erro_inesperado', 'mensagem': str(e)}\n",
        "\n",
        "    def analisar_linguagem(self, texto):\n",
        "        \"\"\"Analisa o tom e a linguagem usada no texto.\"\"\"\n",
        "        resultados = {}\n",
        "        texto_lower = texto.lower() # Converter para minúsculas uma vez\n",
        "\n",
        "        # Análise de sensacionalismo com pesos\n",
        "        count_sensacionalismo = 0\n",
        "        palavras_encontradas_sensacionalismo = []\n",
        "        for palavra, peso in self.palavras_sensacionalistas.items():\n",
        "            # Usar regex para encontrar palavras inteiras (evitar substrings)\n",
        "            ocorrencias = len(re.findall(r'\\b' + re.escape(palavra) + r'\\b', texto_lower))\n",
        "            if ocorrencias > 0:\n",
        "                count_sensacionalismo += ocorrencias * peso\n",
        "                palavras_encontradas_sensacionalismo.append(f\"{palavra} ({ocorrencias}x)\")\n",
        "\n",
        "        resultados[\"indice_sensacionalismo\"] = round(count_sensacionalismo, 2)\n",
        "        resultados[\"palavras_sensacionalistas\"] = palavras_encontradas_sensacionalismo\n",
        "        if count_sensacionalismo > 5: # Ajustar limiar\n",
        "            resultados[\"alerta_sensacionalismo\"] = \"Alto índice de linguagem sensacionalista detectado.\"\n",
        "        elif count_sensacionalismo > 2: # Ajustar limiar\n",
        "            resultados[\"alerta_sensacionalismo\"] = \"Linguagem potencialmente sensacionalista detectada.\"\n",
        "\n",
        "        # Análise de incerteza\n",
        "        count_incerteza = 0\n",
        "        palavras_encontradas_incerteza = []\n",
        "        for palavra in self.palavras_incerteza:\n",
        "            ocorrencias = len(re.findall(r'\\b' + re.escape(palavra) + r'\\b', texto_lower))\n",
        "            if ocorrencias > 0:\n",
        "                count_incerteza += ocorrencias\n",
        "                palavras_encontradas_incerteza.append(palavra)\n",
        "\n",
        "        resultados[\"indice_incerteza\"] = count_incerteza\n",
        "        resultados[\"palavras_incerteza\"] = list(set(palavras_encontradas_incerteza)) # Lista única\n",
        "        if count_incerteza > 4: # Ajustar limiar\n",
        "            resultados[\"alerta_incerteza\"] = \"Alto uso de termos de incerteza detectado.\"\n",
        "\n",
        "        # Análise de opinião\n",
        "        count_opiniao = 0\n",
        "        expressoes_encontradas_opiniao = []\n",
        "        for expressao in self.expressoes_opinativas:\n",
        "            ocorrencias = len(re.findall(r'\\b' + re.escape(expressao) + r'\\b', texto_lower))\n",
        "            if ocorrencias > 0:\n",
        "                count_opiniao += ocorrencias\n",
        "                expressoes_encontradas_opiniao.append(expressao)\n",
        "\n",
        "        resultados[\"indice_opiniao\"] = count_opiniao\n",
        "        resultados[\"expressoes_opiniao\"] = list(set(expressoes_encontradas_opiniao)) # Lista única\n",
        "        if count_opiniao > 2: # Ajustar limiar\n",
        "            resultados[\"alerta_opiniao\"] = \"Texto com possível caráter opinativo detectado.\"\n",
        "\n",
        "        # Análise de exclamações\n",
        "        exclamacoes = texto.count('!')\n",
        "        resultados[\"num_exclamacoes\"] = exclamacoes\n",
        "        if exclamacoes > 5:\n",
        "            resultados[\"alerta_exclamacoes\"] = \"Uso excessivo de pontos de exclamação.\"\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def verificar_imagens(self, imagens):\n",
        "        \"\"\"Analisa as imagens associadas ao artigo.\"\"\"\n",
        "        resultados = {}\n",
        "        num_imagens = len(imagens)\n",
        "        resultados[\"num_imagens\"] = num_imagens\n",
        "\n",
        "        if num_imagens == 0:\n",
        "            # Não necessariamente um alerta negativo, alguns artigos não têm imagens.\n",
        "            # resultados[\"alerta_imagens\"] = \"Nenhuma imagem encontrada no artigo.\"\n",
        "            return resultados\n",
        "\n",
        "        # Verificar descrições das imagens (alt text)\n",
        "        imagens_sem_descricao = sum(1 for img in imagens if not img.get('alt'))\n",
        "        resultados[\"imagens_sem_descricao\"] = imagens_sem_descricao\n",
        "\n",
        "        if imagens_sem_descricao > 0 and imagens_sem_descricao == num_imagens:\n",
        "            resultados[\"alerta_imagens_descricao\"] = \"Nenhuma das imagens possui descrição (alt text).\"\n",
        "        elif imagens_sem_descricao > num_imagens / 2:\n",
        "             resultados[\"alerta_imagens_descricao\"] = \"Muitas imagens sem descrição (alt text).\"\n",
        "\n",
        "        # Calcular hash básico das URLs de imagens (pode ser usado para verificação reversa)\n",
        "        # hashes_urls = [hashlib.md5(img['url'].encode()).hexdigest()[:8] for img in imagens]\n",
        "        # resultados[\"hashes_imagens\"] = hashes_urls\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def buscar_noticias_similares(self, titulo, keywords):\n",
        "        \"\"\"Busca por notícias similares na web (SIMULAÇÃO).\"\"\"\n",
        "        # --- IMPLEMENTAÇÃO SIMULADA ---\n",
        "        # Em uma aplicação real, você usaria uma API de busca (Google News, Bing News, etc.)\n",
        "        # ou um motor de busca customizado.\n",
        "        logging.info(f\"Simulando busca por notícias similares para: '{titulo[:50]}...'\")\n",
        "        resultados = {}\n",
        "        try:\n",
        "            # Simular tempo de busca\n",
        "            time.sleep(0.5)\n",
        "\n",
        "            # Simular resultados baseados em keywords ou título\n",
        "            num_similares = 0\n",
        "            if keywords:\n",
        "                 num_similares = len(keywords) * 2 # Exemplo simples\n",
        "            else:\n",
        "                 num_similares = len(titulo.split()) // 5 # Exemplo simples\n",
        "\n",
        "            num_similares = min(num_similares, 10) # Limitar simulação\n",
        "\n",
        "            resultados[\"artigos_similares_encontrados\"] = num_similares # Nome mais claro\n",
        "            if num_similares > 5:\n",
        "                 resultados[\"similaridade_detectada\"] = \"alta\"\n",
        "            elif num_similares > 2:\n",
        "                 resultados[\"similaridade_detectada\"] = \"média\"\n",
        "            else:\n",
        "                 resultados[\"similaridade_detectada\"] = \"baixa\"\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Erro na simulação de busca de notícias similares: {e}\")\n",
        "            resultados[\"erro_busca_similar\"] = str(e)\n",
        "\n",
        "        return resultados\n",
        "        # --- FIM DA SIMULAÇÃO ---\n",
        "\n",
        "    def analisar_credibilidade(self, artigo):\n",
        "        \"\"\"Análise combinada de credibilidade baseada em características do texto e fonte.\"\"\"\n",
        "        # Verifica se o artigo foi baixado corretamente\n",
        "        if not artigo or artigo.get('status') != 'sucesso' or not artigo.get('conteudo'):\n",
        "            logging.warning(\"Artigo não disponível ou incompleto para análise de credibilidade.\")\n",
        "            # Retorna a informação de erro que veio de baixar_artigo\n",
        "            return artigo if artigo else {\"status\": \"erro\", \"mensagem\": \"Artigo não fornecido para análise.\"}\n",
        "\n",
        "        resultados = {}\n",
        "        conteudo = artigo['conteudo']\n",
        "        url = artigo['url']\n",
        "\n",
        "        # 1. Verificação de fonte/domínio\n",
        "        info_fonte = self.verificar_fonte(url)\n",
        "        resultados.update(info_fonte)\n",
        "\n",
        "        # 2. Copiar informações básicas do artigo\n",
        "        resultados[\"titulo\"] = artigo.get('titulo', 'N/A')\n",
        "        resultados[\"data_publicacao\"] = artigo.get('data', 'N/A')\n",
        "        resultados[\"autores\"] = artigo.get('autores', [])\n",
        "        resultados[\"url\"] = url\n",
        "\n",
        "        # 3. Análise básica do conteúdo\n",
        "        palavras = conteudo.split()\n",
        "        num_palavras = len(palavras)\n",
        "        resultados[\"num_palavras\"] = num_palavras\n",
        "        if num_palavras < 150: # Aumentar limiar para considerar notícia\n",
        "            resultados[\"alerta_tamanho\"] = f\"Texto curto ({num_palavras} palavras), pode ser apenas uma nota ou resumo.\"\n",
        "\n",
        "        # 4. Análise detalhada da linguagem\n",
        "        if num_palavras > 10: # Só analisa linguagem se houver texto suficiente\n",
        "            analise_linguagem = self.analisar_linguagem(conteudo)\n",
        "            resultados.update(analise_linguagem)\n",
        "\n",
        "        # 5. Verificação de citações diretas (entre aspas)\n",
        "        # Regex aprimorado para capturar aspas corretamente\n",
        "        citacoes = re.findall(r'[“\"]([^\"”]+)[”\"]', conteudo)\n",
        "        resultados[\"num_citacoes\"] = len(citacoes)\n",
        "        # Citações não são obrigatórias, remover alerta negativo por falta delas.\n",
        "        # if len(citacoes) == 0 and num_palavras > 300: # Só alerta em textos longos\n",
        "        #     resultados[\"info_fontes\"] = \"Nenhuma citação direta (entre aspas) encontrada.\"\n",
        "\n",
        "        # 6. Verificação de URLs externos (links)\n",
        "        # Regex mais robusto para URLs\n",
        "        urls_externos = re.findall(r'https?://[^\\s/$.?#].[^\\s]*', conteudo)\n",
        "        # Filtrar links para o mesmo domínio\n",
        "        dominio_artigo = self.extrair_dominio(url)\n",
        "        links_externos_validos = [link for link in urls_externos if dominio_artigo not in urlparse(link).netloc]\n",
        "        resultados[\"num_links_externos\"] = len(links_externos_validos)\n",
        "\n",
        "        # 7. Análise de imagens\n",
        "        resultados_imagens = self.verificar_imagens(artigo.get('imagens', []))\n",
        "        resultados.update(resultados_imagens)\n",
        "\n",
        "        # 8. Verificação cruzada com outras notícias (Simulação)\n",
        "        resultados_similares = self.buscar_noticias_similares(\n",
        "            artigo.get('titulo', ''),\n",
        "            artigo.get('keywords', [])\n",
        "        )\n",
        "        resultados.update(resultados_similares)\n",
        "\n",
        "        # Marcar que a análise de credibilidade foi feita\n",
        "        resultados['status_analise'] = 'concluida'\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def calcular_pontuacao_credibilidade(self, resultados_analise):\n",
        "        \"\"\"Calcula a pontuação final de credibilidade baseada nos resultados da análise.\"\"\"\n",
        "\n",
        "        # Verifica se a análise foi concluída com sucesso\n",
        "        if resultados_analise.get('status_analise') != 'concluida':\n",
        "             # Se houve erro antes (ex: download), a pontuação não é calculada\n",
        "             return {\"pontuacao_credibilidade\": 0, \"avaliacao\": \"Não foi possível analisar\"}\n",
        "\n",
        "        pontuacao = 50  # Base inicial neutra (escala 0-100)\n",
        "\n",
        "        # Ajustes baseados na fonte\n",
        "        pontuacao += resultados_analise.get(\"pontos_fonte\", 0)\n",
        "\n",
        "        # Fatores que reduzem a pontuação\n",
        "        if \"alerta_tamanho\" in resultados_analise:\n",
        "            pontuacao -= 10\n",
        "        if \"alerta_sensacionalismo\" in resultados_analise:\n",
        "            if \"Alto índice\" in resultados_analise[\"alerta_sensacionalismo\"]:\n",
        "                pontuacao -= 25 # Penalidade maior para alto sensacionalismo\n",
        "            else:\n",
        "                pontuacao -= 15\n",
        "        # if \"alerta_fontes\" in resultados_analise: # Removido alerta negativo de citações\n",
        "        #     pontuacao -= 5\n",
        "        if \"alerta_opiniao\" in resultados_analise:\n",
        "            pontuacao -= 15\n",
        "        if \"alerta_incerteza\" in resultados_analise:\n",
        "             # Penalidade proporcional ao índice de incerteza\n",
        "            pontuacao -= min(resultados_analise.get(\"indice_incerteza\", 0) * 2, 15) # Limita a penalidade\n",
        "        if \"alerta_exclamacoes\" in resultados_analise:\n",
        "            pontuacao -= 5\n",
        "        if \"alerta_imagens_descricao\" in resultados_analise:\n",
        "             # Penalidade menor se houver imagens, mas sem descrição\n",
        "             if \"Nenhuma das imagens\" in resultados_analise[\"alerta_imagens_descricao\"]:\n",
        "                  pontuacao -= 5\n",
        "             else:\n",
        "                  pontuacao -= 2\n",
        "\n",
        "        # Fatores que aumentam a pontuação\n",
        "        num_citacoes = resultados_analise.get(\"num_citacoes\", 0)\n",
        "        if num_citacoes > 3:\n",
        "            pontuacao += 10\n",
        "        elif num_citacoes > 0:\n",
        "            pontuacao += 5\n",
        "\n",
        "        num_links_externos = resultados_analise.get(\"num_links_externos\", 0)\n",
        "        if num_links_externos > 3:\n",
        "            pontuacao += 10\n",
        "        elif num_links_externos > 0:\n",
        "            pontuacao += 5\n",
        "\n",
        "        num_imagens = resultados_analise.get(\"num_imagens\", 0)\n",
        "        if num_imagens >= 1 and resultados_analise.get(\"imagens_sem_descricao\", num_imagens) < num_imagens:\n",
        "             # Bonus se tem imagens E pelo menos uma tem descrição\n",
        "             pontuacao += 5\n",
        "\n",
        "        # Bônus por artigos similares (indicativo que a notícia é coberta por outros)\n",
        "        artigos_similares = resultados_analise.get(\"artigos_similares_encontrados\", 0)\n",
        "        if artigos_similares > 5:\n",
        "             pontuacao += 10\n",
        "        elif artigos_similares > 2:\n",
        "             pontuacao += 5\n",
        "\n",
        "        # Bônus por ter autores identificados\n",
        "        if resultados_analise.get(\"autores\"):\n",
        "             pontuacao += 5\n",
        "\n",
        "        # Bônus por ter data de publicação\n",
        "        if resultados_analise.get(\"data_publicacao\") != 'N/A':\n",
        "             pontuacao += 3\n",
        "\n",
        "\n",
        "        # Ajustar pontuação final para ficar entre 0 e 100\n",
        "        pontuacao = max(0, min(100, pontuacao))\n",
        "        resultados_analise[\"pontuacao_credibilidade\"] = int(round(pontuacao)) # Arredonda para inteiro\n",
        "\n",
        "        # Avaliação qualitativa baseada na pontuação\n",
        "        if pontuacao >= 85:\n",
        "            resultados_analise[\"avaliacao\"] = \"Credibilidade Muito Alta\"\n",
        "        elif pontuacao >= 70:\n",
        "            resultados_analise[\"avaliacao\"] = \"Credibilidade Alta\"\n",
        "        elif pontuacao >= 50:\n",
        "            resultados_analise[\"avaliacao\"] = \"Credibilidade Moderada\"\n",
        "        elif pontuacao >= 30:\n",
        "            resultados_analise[\"avaliacao\"] = \"Credibilidade Baixa\"\n",
        "        else:\n",
        "            resultados_analise[\"avaliacao\"] = \"Credibilidade Muito Baixa\"\n",
        "\n",
        "        return resultados_analise\n",
        "\n",
        "\n",
        "    def verificar_noticia(self, url):\n",
        "        \"\"\"Função principal para verificar uma notícia: baixa, analisa e pontua.\"\"\"\n",
        "        logging.info(f\"Iniciando verificação completa para: {url}\")\n",
        "        tempo_inicio = time.time()\n",
        "\n",
        "        # 1. Baixar e extrair informações do artigo\n",
        "        artigo = self.baixar_artigo(url)\n",
        "\n",
        "        # Se baixar falhar, retorna o erro imediatamente\n",
        "        if not artigo or artigo.get('status') != 'sucesso':\n",
        "            logging.error(f\"Falha ao baixar ou processar artigo de {url}.\")\n",
        "            # Retornar um dicionário consistente mesmo em caso de erro\n",
        "            return {\n",
        "                \"url\": url,\n",
        "                \"status_geral\": \"erro_ao_baixar\",\n",
        "                \"mensagem\": artigo.get(\"mensagem\", \"Erro desconhecido ao baixar artigo.\"),\n",
        "                \"pontuacao_credibilidade\": 0,\n",
        "                \"avaliacao\": \"Não foi possível analisar\",\n",
        "                \"tempo_analise_seg\": round(time.time() - tempo_inicio, 2)\n",
        "            }\n",
        "\n",
        "        # 2. Analisar a credibilidade com base nas informações extraídas\n",
        "        resultados_analise = self.analisar_credibilidade(artigo)\n",
        "\n",
        "        # Se a análise de credibilidade falhar (improvável se o download funcionou, mas por segurança)\n",
        "        if resultados_analise.get('status_analise') != 'concluida':\n",
        "             logging.error(f\"Falha na etapa de análise de credibilidade para {url}.\")\n",
        "             return {\n",
        "                 **resultados_analise, # Retorna o que foi possível analisar\n",
        "                 \"url\": url,\n",
        "                 \"status_geral\": \"erro_na_analise\",\n",
        "                 \"pontuacao_credibilidade\": 0,\n",
        "                 \"avaliacao\": \"Não foi possível analisar completamente\",\n",
        "                 \"tempo_analise_seg\": round(time.time() - tempo_inicio, 2)\n",
        "             }\n",
        "\n",
        "        # 3. Calcular a pontuação e avaliação final\n",
        "        resultados_finais = self.calcular_pontuacao_credibilidade(resultados_analise)\n",
        "\n",
        "        # Adicionar tempo de execução e status geral\n",
        "        resultados_finais[\"tempo_analise_seg\"] = round(time.time() - tempo_inicio, 2)\n",
        "        resultados_finais[\"status_geral\"] = \"analise_completa\"\n",
        "\n",
        "        logging.info(f\"Análise de {url} concluída em {resultados_finais['tempo_analise_seg']:.2f}s. Pontuação: {resultados_finais['pontuacao_credibilidade']}/100\")\n",
        "\n",
        "        return resultados_finais\n",
        "\n",
        "# --- Helper para formatação de saída (usado no main.py para testes locais) ---\n",
        "def formatar_resultado_texto(resultado):\n",
        "    \"\"\"Formata o dicionário de resultados em um texto legível.\"\"\"\n",
        "    if not resultado:\n",
        "        return \"Erro: Resultado da análise está vazio.\"\n",
        "\n",
        "    output = []\n",
        "    output.append(\"=\"*50)\n",
        "    output.append(f\"RELATÓRIO DE VERIFICAÇÃO DE NOTÍCIA\")\n",
        "    output.append(\"=\"*50)\n",
        "    output.append(f\"URL: {resultado.get('url', 'N/A')}\")\n",
        "\n",
        "    if resultado.get('status_geral', '').startswith('erro'):\n",
        "        output.append(\"-\" * 50)\n",
        "        output.append(f\"⚠️ ERRO NA ANÁLISE: {resultado.get('status_geral')}\")\n",
        "        output.append(f\"Mensagem: {resultado.get('mensagem', 'Detalhe não disponível')}\")\n",
        "        if 'status_code' in resultado:\n",
        "             output.append(f\"Status HTTP: {resultado.get('status_code')}\")\n",
        "        output.append(\"=\"*50)\n",
        "        return \"\\n\".join(output)\n",
        "\n",
        "    # Se a análise foi completa\n",
        "    output.append(f\"Domínio: {resultado.get('dominio', 'N/A')} ({resultado.get('status_fonte', 'N/A')})\")\n",
        "    output.append(\"-\" * 50)\n",
        "\n",
        "    output.append(f\"Título: {resultado.get('titulo', 'N/A')}\")\n",
        "    output.append(f\"Data de publicação: {resultado.get('data_publicacao', 'N/A')}\")\n",
        "    autores = resultado.get('autores', [])\n",
        "    output.append(f\"Autores: {', '.join(autores) if autores else 'N/A'}\")\n",
        "    output.append(f\"Número de palavras: {resultado.get('num_palavras', 'N/A')}\")\n",
        "\n",
        "    # Métricas quantitativas\n",
        "    output.append(\"-\" * 50)\n",
        "    output.append(\"MÉTRICAS:\")\n",
        "    output.append(f\"• Citações diretas: {resultado.get('num_citacoes', 0)}\")\n",
        "    output.append(f\"• Links externos: {resultado.get('num_links_externos', 0)}\")\n",
        "    output.append(f\"• Índice de sensacionalismo: {resultado.get('indice_sensacionalismo', 0):.2f}\")\n",
        "    if resultado.get('palavras_sensacionalistas'):\n",
        "        output.append(f\"  - Termos: {', '.join(resultado.get('palavras_sensacionalistas', []))}\")\n",
        "    output.append(f\"• Índice de incerteza: {resultado.get('indice_incerteza', 0)}\")\n",
        "    if resultado.get('palavras_incerteza'):\n",
        "        output.append(f\"  - Termos: {', '.join(resultado.get('palavras_incerteza', []))}\")\n",
        "    output.append(f\"• Índice opinativo: {resultado.get('indice_opiniao', 0)}\")\n",
        "    output.append(f\"• Pontos de exclamação: {resultado.get('num_exclamacoes', 0)}\")\n",
        "    output.append(f\"• Imagens: {resultado.get('num_imagens', 0)} (Sem descrição: {resultado.get('imagens_sem_descricao', 0)})\")\n",
        "    output.append(f\"• Artigos similares (simulado): {resultado.get('artigos_similares_encontrados', 'N/A')}\")\n",
        "\n",
        "    # Alertas\n",
        "    alertas = {k:v for k, v in resultado.items() if k.startswith(\"alerta_\")}\n",
        "    if alertas:\n",
        "        output.append(\"-\" * 50)\n",
        "        output.append(\"ALERTAS:\")\n",
        "        for key, msg in alertas.items():\n",
        "            output.append(f\"⚠️ {msg}\")\n",
        "\n",
        "    # Pontuação final\n",
        "    output.append(\"-\" * 50)\n",
        "    output.append(f\"PONTUAÇÃO DE CREDIBILIDADE: {resultado.get('pontuacao_credibilidade', 'N/A')}/100\")\n",
        "    output.append(f\"AVALIAÇÃO: {resultado.get('avaliacao', 'N/A')}\")\n",
        "    output.append(f\"Tempo de análise: {resultado.get('tempo_analise_seg', 'N/A'):.2f} segundos\")\n",
        "    output.append(\"=\" * 50)\n",
        "\n",
        "    return \"\\n\".join(output)\n",
        "\n",
        "\n",
        "# Removido o bloco if __name__ == \"__main__\": daqui,\n",
        "# a execução local será feita pelo main.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vO9J-8GsTpEg",
        "outputId": "8dac03e9-bfc7-4354-c3e3-36a47a51e204"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting verificacao_avancada.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.py\n",
        "# -*- coding: utf-8 -*-\n",
        "import functions_framework\n",
        "import json\n",
        "import logging\n",
        "import os # Para obter variáveis de ambiente se necessário\n",
        "\n",
        "# Importar a classe e a função de formatação do outro arquivo\n",
        "from verificacao_avancada import VerificadorNoticiasAvancado, formatar_resultado_texto\n",
        "\n",
        "# Configura o logging para Cloud Functions\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Instanciar o verificador globalmente?\n",
        "# Pró: Reutiliza o objeto e listas carregadas entre invocações (se a instância for mantida \"quente\")\n",
        "# Contra: Pode manter estado indesejado (embora esta classe pareça stateless)\n",
        "# Alternativa: Instanciar dentro da função para garantir estado limpo a cada requisição.\n",
        "# Vamos instanciar dentro da função por simplicidade e garantia de limpeza.\n",
        "# verificador_global = VerificadorNoticiasAvancado()\n",
        "\n",
        "\n",
        "@functions_framework.http\n",
        "def handle_webhook_request(request):\n",
        "    \"\"\"\n",
        "    Responde a requisições HTTP vindas do Dialogflow CX/ES.\n",
        "    Espera um JSON com a estrutura do Dialogflow e extrai um parâmetro 'url'.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        request_json = request.get_json(silent=True)\n",
        "        logging.info(f\"Recebido request do Dialogflow: {json.dumps(request_json, indent=2)}\")\n",
        "\n",
        "        # Verificar se o JSON foi recebido e tem a estrutura esperada\n",
        "        if not request_json:\n",
        "            logging.error(\"Request sem corpo JSON.\")\n",
        "            return create_dialogflow_response(\"Erro: Não recebi dados na requisição.\"), 400\n",
        "\n",
        "        # --- Extrair a URL do parâmetro do Dialogflow ---\n",
        "        # A localização exata do parâmetro pode variar (Dialogflow ES vs CX)\n",
        "        # Tentativa comum para ES/CX: sessionInfo.parameters ou queryResult.parameters\n",
        "        url_noticia = None\n",
        "        if 'sessionInfo' in request_json and 'parameters' in request_json['sessionInfo']:\n",
        "            url_noticia = request_json['sessionInfo']['parameters'].get('url') # Ajuste 'url' se o nome do parâmetro for diferente\n",
        "        elif 'queryResult' in request_json and 'parameters' in request_json['queryResult']:\n",
        "             url_noticia = request_json['queryResult']['parameters'].get('url') # Ajuste 'url' se o nome do parâmetro for diferente\n",
        "\n",
        "        if not url_noticia:\n",
        "            logging.warning(\"Parâmetro 'url' não encontrado na requisição do Dialogflow.\")\n",
        "            # Tenta pegar de uma possível mensagem de texto direto (menos comum para parâmetros)\n",
        "            if 'text' in request_json.get('message', {}):\n",
        "                 possible_url = request_json['message']['text']\n",
        "                 # Validação básica se parece uma URL\n",
        "                 if isinstance(possible_url, str) and possible_url.startswith(('http://', 'https://')):\n",
        "                      url_noticia = possible_url\n",
        "                      logging.info(f\"URL encontrada no texto da mensagem: {url_noticia}\")\n",
        "\n",
        "        # Se ainda não encontrou a URL, retorna erro amigável\n",
        "        if not url_noticia or not isinstance(url_noticia, str):\n",
        "            logging.error(\"URL da notícia não fornecida ou inválida na requisição.\")\n",
        "            return create_dialogflow_response(\"Por favor, me envie a URL da notícia que você quer verificar.\"), 400\n",
        "\n",
        "        # --- Executar a Verificação ---\n",
        "        logging.info(f\"Iniciando verificação para a URL: {url_noticia}\")\n",
        "        # Instanciar o verificador a cada requisição\n",
        "        verificador = VerificadorNoticiasAvancado()\n",
        "        resultado_analise = verificador.verificar_noticia(url_noticia)\n",
        "\n",
        "        # --- Formatar a Resposta para o Dialogflow ---\n",
        "        texto_resposta = gerar_resposta_dialogflow(resultado_analise)\n",
        "        logging.info(f\"Enviando resposta para Dialogflow: {texto_resposta[:200]}...\") # Loga início da resposta\n",
        "        return create_dialogflow_response(texto_resposta)\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.exception(\"Erro inesperado ao processar a requisição do webhook.\") # Loga o traceback completo\n",
        "        # Retorna uma mensagem de erro genérica para o usuário\n",
        "        return create_dialogflow_response(\"Desculpe, ocorreu um erro interno ao tentar verificar a notícia. Tente novamente mais tarde.\"), 500\n",
        "\n",
        "def gerar_resposta_dialogflow(resultado):\n",
        "    \"\"\"Gera uma string de resposta amigável baseada no resultado da análise.\"\"\"\n",
        "\n",
        "    if not resultado:\n",
        "         return \"Desculpe, não consegui obter um resultado para a análise.\"\n",
        "\n",
        "    status = resultado.get('status_geral', 'erro')\n",
        "\n",
        "    if status.startswith('erro'):\n",
        "        msg_erro = resultado.get('mensagem', 'Não foi possível completar a análise.')\n",
        "        if status == 'erro_ao_baixar':\n",
        "             # Erros comuns de download\n",
        "             if '404' in msg_erro:\n",
        "                  return f\"Não consegui encontrar a página da notícia (Erro 404). Verifique se a URL está correta: {resultado.get('url')}\"\n",
        "             elif 'Timeout' in msg_erro:\n",
        "                  return f\"A página demorou muito para responder. O site pode estar fora do ar ou lento. Tente novamente mais tarde. URL: {resultado.get('url')}\"\n",
        "             elif 'Connection refused' in msg_erro or 'SSL' in msg_erro:\n",
        "                  return f\"Tive problemas para conectar ao site da notícia. Pode ser um problema temporário ou de segurança do site. URL: {resultado.get('url')}\"\n",
        "             else:\n",
        "                  return f\"Não consegui acessar a notícia para analisar. Verifique a URL ou tente mais tarde. Detalhe: {msg_erro[:100]}\" # Limita tamanho do erro técnico\n",
        "        else:\n",
        "             return f\"Ocorreu um problema durante a análise. Detalhe: {msg_erro[:100]}\"\n",
        "\n",
        "    # Análise completa\n",
        "    pontuacao = resultado.get('pontuacao_credibilidade', 0)\n",
        "    avaliacao = resultado.get('avaliacao', 'Indeterminada')\n",
        "    dominio = resultado.get('dominio', 'Domínio desconhecido')\n",
        "    status_fonte = resultado.get('status_fonte', 'não categorizada')\n",
        "\n",
        "    resposta = []\n",
        "    resposta.append(f\"Análise da notícia em '{dominio}':\")\n",
        "    resposta.append(f\"Pontuação de Credibilidade: {pontuacao}/100 ({avaliacao}).\")\n",
        "\n",
        "    # Adiciona detalhes sobre a fonte\n",
        "    if status_fonte == 'confiável' or status_fonte == 'confiável (subdomínio)':\n",
        "        resposta.append(\"✅ A fonte é considerada confiável.\")\n",
        "    elif status_fonte == 'suspeita':\n",
        "        resposta.append(f\"⚠️ Atenção: Esta fonte ('{dominio}') é conhecida por notícias satíricas ou de baixa credibilidade.\")\n",
        "    else: # não categorizada ou erro\n",
        "        resposta.append(\"ℹ️ A reputação desta fonte não está na minha lista principal.\")\n",
        "\n",
        "    # Adiciona os principais alertas, se houver\n",
        "    alertas_importantes = []\n",
        "    if \"alerta_sensacionalismo\" in resultado:\n",
        "         alertas_importantes.append(\"linguagem sensacionalista\")\n",
        "    if \"alerta_opiniao\" in resultado:\n",
        "         alertas_importantes.append(\"forte caráter opinativo\")\n",
        "    if \"alerta_incerteza\" in resultado:\n",
        "         alertas_importantes.append(\"uso de termos de incerteza\")\n",
        "    if \"alerta_tamanho\" in resultado:\n",
        "        alertas_importantes.append(\"texto muito curto\")\n",
        "\n",
        "    if alertas_importantes:\n",
        "         resposta.append(f\"🚨 Pontos de atenção detectados: {'; '.join(alertas_importantes)}.\")\n",
        "\n",
        "    # Conclusão geral\n",
        "    if pontuacao < 30:\n",
        "        resposta.append(\"Recomendo muita cautela e buscar outras fontes antes de confiar nesta notícia.\")\n",
        "    elif pontuacao < 50:\n",
        "        resposta.append(\"É bom verificar esta informação em outras fontes mais estabelecidas.\")\n",
        "    elif pontuacao < 70:\n",
        "         resposta.append(\"Parece razoável, mas fique atento aos pontos levantados.\")\n",
        "    else:\n",
        "        resposta.append(\"A notícia parece ter boa credibilidade com base na análise.\")\n",
        "\n",
        "    # Limitar o tamanho total da resposta para Dialogflow (geralmente tem limites)\n",
        "    return \" \".join(resposta)[:4000] # Limite generoso, ajuste se necessário\n",
        "\n",
        "def create_dialogflow_response(text_message):\n",
        "    \"\"\"Cria a estrutura JSON de resposta padrão do Dialogflow.\"\"\"\n",
        "    response_data = {\n",
        "        \"fulfillment_response\": {\n",
        "            \"messages\": [\n",
        "                {\n",
        "                    \"text\": {\n",
        "                        # O texto deve ser uma lista\n",
        "                        \"text\": [text_message]\n",
        "                    }\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    }\n",
        "    # Retorna a string JSON, status HTTP e Content-Type\n",
        "    return json.dumps(response_data), 200, {'Content-Type': 'application/json; charset=utf-8'}\n",
        "\n",
        "\n",
        "# --- Bloco para Execução Local (Testes) ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Executando Verificador de Notícias localmente...\")\n",
        "    # Exemplo de URLs para teste:\n",
        "    url_confiavel = \"https://g1.globo.com/economia/noticia/2023/10/26/bc-mantera-cortes-de-05-ponto-percentual-nas-proximas-reunioes-indicam-comunicado-e-declaracoes-de-campos-neto.ghtml\" # Exemplo G1\n",
        "    url_suspeita = \"https://www.theonion.com/study-finds-working-from-home-increases-productivity-am-1849809884\" # Exemplo The Onion (Sátira)\n",
        "    url_nao_categorizada = \"https://www.exemploqualquer.com/noticia-teste\" # Exemplo inválido ou não categorizado\n",
        "    url_erro_404 = \"https://g1.globo.com/pagina-que-nao-existe-12345\"\n",
        "    url_curta = \"https://www.tecmundo.com.br/voxel\" # Página principal, não artigo\n",
        "\n",
        "    urls_para_testar = [\n",
        "         #url_confiavel,\n",
        "         url_suspeita,\n",
        "         #url_erro_404,\n",
        "         #url_curta\n",
        "         # Adicione outras URLs que queira testar\n",
        "    ]\n",
        "\n",
        "    if not urls_para_testar:\n",
        "        try:\n",
        "             url_input = input(\"Digite a URL da notícia para verificar: \")\n",
        "             urls_para_testar.append(url_input.strip())\n",
        "        except EOFError: # Handle case where input is redirected and empty\n",
        "             print(\"Nenhuma URL fornecida para teste.\")\n",
        "             urls_para_testar = []\n",
        "\n",
        "\n",
        "    verificador_local = VerificadorNoticiasAvancado()\n",
        "\n",
        "    for url in urls_para_testar:\n",
        "        if not url: continue\n",
        "        print(f\"\\n--- Testando URL: {url} ---\")\n",
        "        try:\n",
        "            resultado = verificador_local.verificar_noticia(url)\n",
        "            # Usar a função de formatação para exibir o resultado no console\n",
        "            print(formatar_resultado_texto(resultado))\n",
        "            # Simular como seria a resposta para o Dialogflow\n",
        "            print(\"\\n--- Resposta Simulada para Dialogflow ---\")\n",
        "            print(gerar_resposta_dialogflow(resultado))\n",
        "            print(\"----------------------------------------\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro inesperado durante o teste local da URL {url}: {e}\")\n",
        "            logging.exception(\"Erro no teste local\") # Loga o traceback\n",
        "\n",
        "    print(\"\\nTestes locais concluídos.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3F6FZd2Tbvn",
        "outputId": "a32cff64-2b43-4e16-c4bc-5661012f1cf3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run verificacao_simples.py\n",
        "verificar_url(\"https://www.msn.com/pt-br/noticias/brasil/casa-branca-aponta-que-suposta-verdade-sobre-a-origem-da-covid-19-tem-rela%C3%A7%C3%A3o-com-vazamento-em-laborat%C3%B3rio-na-china/ar-AA1DbpaW?ocid=BingNewsSerp\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHTQBoceO65l",
        "outputId": "ea221ec3-8b9d-4145-c0c6-713d13ca1b95"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verificador de notícias avançado - use a função verificar_url(sua_url) para analisar uma notícia\n",
            "Verificador de Notícias Avançado inicializado!\n",
            "Analisando URL: https://www.msn.com/pt-br/noticias/brasil/casa-branca-aponta-que-suposta-verdade-sobre-a-origem-da-covid-19-tem-rela%C3%A7%C3%A3o-com-vazamento-em-laborat%C3%B3rio-na-china/ar-AA1DbpaW?ocid=BingNewsSerp\n",
            "\n",
            "==================================================\n",
            "RELATÓRIO DE VERIFICAÇÃO DE NOTÍCIA\n",
            "==================================================\n",
            "URL: https://www.msn.com/pt-br/noticias/brasil/casa-branca-aponta-que-suposta-verdade-sobre-a-origem-da-covid-19-tem-rela%C3%A7%C3%A3o-com-vazamento-em-laborat%C3%B3rio-na-china/ar-AA1DbpaW?ocid=BingNewsSerp\n",
            "Domínio: N/A\n",
            "Status da fonte: N/A\n",
            "--------------------------------------------------\n",
            "Título: N/A\n",
            "Data de publicação: N/A\n",
            "Autores: N/A\n",
            "Número de palavras: 0\n",
            "--------------------------------------------------\n",
            "MÉTRICAS:\n",
            "• Citações: 0\n",
            "• URLs referenciadas: 0\n",
            "• Índice de sensacionalismo: 0\n",
            "• Índice de incerteza: 0\n",
            "• Índice opinativo: 0\n",
            "• Pontos de exclamação: 0\n",
            "• Imagens: 0\n",
            "• Artigos similares encontrados: 0\n",
            "--------------------------------------------------\n",
            "PONTUAÇÃO DE CREDIBILIDADE: 70/100\n",
            "AVALIAÇÃO: Boa credibilidade\n",
            "Tempo de análise: 0.15 segundos\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'erro': 'Artigo não disponível para análise',\n",
              " 'pontuacao_credibilidade': 70,\n",
              " 'avaliacao': 'Boa credibilidade'}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run verificacao_simples.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPPyjXFzObfN",
        "outputId": "5ebdd32d-4516-4a67-a1d8-909017ac989a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verificador de notícias avançado - use a função verificar_url(sua_url) para analisar uma notícia\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run verificacao_avancada.py"
      ],
      "metadata": {
        "id": "c7-0hlCHN-Ia"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python \"C:\\\\Users\\\\Geraldo\\\\Documents\\\\ChatBot_NewsgamesIA\\\\verificacao_avancada.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6fxKP2lK7ZR",
        "outputId": "c9809134-9ac3-4246-b338-24270e108a1d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/C:\\\\Users\\\\Geraldo\\\\Documents\\\\ChatBot_NewsgamesIA\\\\verificacao_avancada.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python \"C:\\\\Users\\\\Geraldo\\\\Documents\\\\ChatBot_NewsgamesIA\\\\verificacao_avancada.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gKNayeLJ-_X",
        "outputId": "dcd6b91a-98c4-4198-c439-2be51c0c9845"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/C:\\\\Users\\\\Geraldo\\\\Documents\\\\ChatBot_NewsgamesIA\\\\verificacao_avancada.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "python verificacao_avancada.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "oFUJB734Hmc_",
        "outputId": "3d373546-9b02-4ae7-c090-969dcf9702f2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-19-c824c7471ad6>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-19-c824c7471ad6>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    python verificacao_avancada.py\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile verificacao_avancada.py\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import ssl\n",
        "import urllib3\n",
        "import json\n",
        "from urllib.parse import urlparse\n",
        "import hashlib\n",
        "from collections import Counter\n",
        "import time\n",
        "\n",
        "# Desabilitar avisos SSL\n",
        "urllib3.disable_warnings()\n",
        "\n",
        "class VerificadorNoticiasAvancado:\n",
        "    def __init__(self):\n",
        "        print(\"Verificador de Notícias Avançado inicializado!\")\n",
        "        # Carregar listas de fontes confiáveis e não confiáveis\n",
        "        self.fontes_confiaveis = [\n",
        "            'g1.globo.com', 'bbc.com', 'bbc.co.uk', 'reuters.com', 'apnews.com',\n",
        "            'nytimes.com', 'washingtonpost.com', 'estadao.com.br', 'folha.uol.com.br',\n",
        "            'cnn.com', 'valor.globo.com', 'info.abril.com.br', 'oglobo.globo.com',\n",
        "            'dw.com', 'npr.org', 'tecmundo.com.br', 'canaltech.com.br', 'olhardigital.com.br'\n",
        "        ]\n",
        "\n",
        "        self.fontes_suspeitas = [\n",
        "            'theonion.com', 'sensacionalista.com.br', 'breitbart.com', 'infowars.com',\n",
        "            'clickhole.com', 'nationalreport.net', 'worldnewsdailyreport.com',\n",
        "            'dailycurrant.com', 'empirenews.net'\n",
        "        ]\n",
        "\n",
        "        # Palavras para análise linguística\n",
        "        self.palavras_sensacionalistas = {\n",
        "            \"chocante\": 2, \"inacreditável\": 2, \"surpreendente\": 1, \"sensacional\": 2,\n",
        "            \"impressionante\": 1, \"alarmante\": 1.5, \"exclusivo\": 1, \"urgente\": 1.5,\n",
        "            \"shocking\": 2, \"unbelievable\": 2, \"amazing\": 1, \"surprising\": 1,\n",
        "            \"imperdível\": 1.5, \"incrível\": 1.5, \"absurdo\": 1.5, \"escandaloso\": 2,\n",
        "            \"bomba\": 1.5, \"inédito\": 1, \"revelado\": 1, \"segredo\": 1.5,\n",
        "            \"polêmico\": 1.5, \"controverso\": 1, \"escândalo\": 1.5\n",
        "        }\n",
        "\n",
        "        self.palavras_incerteza = [\n",
        "            \"talvez\", \"possivelmente\", \"provavelmente\", \"pode ser\", \"especula-se\",\n",
        "            \"acredita-se\", \"sugere\", \"alega\", \"supostamente\", \"rumores\",\n",
        "            \"maybe\", \"possibly\", \"probably\", \"might be\", \"it is speculated\",\n",
        "            \"it is believed\", \"suggests\", \"allegedly\", \"supposedly\", \"rumors\"\n",
        "        ]\n",
        "\n",
        "        self.expressoes_opinativas = [\n",
        "            \"eu acho\", \"na minha opinião\", \"parece que\", \"acredito que\",\n",
        "            \"i think\", \"in my opinion\", \"it seems that\", \"i believe\"\n",
        "        ]\n",
        "\n",
        "    def verificar_url_simples(self, url):\n",
        "        \"\"\"Executa uma verificação simples em uma URL de notícia.\"\"\"\n",
        "        try:\n",
        "            # Extrair domínio\n",
        "            parsed_url = urlparse(url)\n",
        "            dominio = parsed_url.netloc\n",
        "\n",
        "            # Tentar acessar a URL\n",
        "            response = requests.get(url, timeout=10)\n",
        "            response.raise_for_status()  # Levantar exceção para status HTTP de erro\n",
        "\n",
        "            # Verificar status HTTP\n",
        "            if response.status_code == 200:\n",
        "                print(f\"URL acessada com sucesso: {url}\")\n",
        "                print(f\"Domínio: {dominio}\")\n",
        "                print(f\"Tamanho do conteúdo: {len(response.text)} caracteres.\")\n",
        "                return {\n",
        "                    \"status\": \"sucesso\",\n",
        "                    \"dominio\": dominio,\n",
        "                    \"tamanho_conteudo\": len(response.text),\n",
        "                    \"mensagem\": \"URL acessada com sucesso\"\n",
        "                }\n",
        "            else:\n",
        "                print(f\"Erro ao acessar a URL. Código HTTP: {response.status_code}\")\n",
        "                return {\n",
        "                    \"status\": \"erro\",\n",
        "                    \"mensagem\": f\"Erro ao acessar a URL. Código HTTP: {response.status_code}\"\n",
        "                }\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Erro ao acessar a URL: {e}\")\n",
        "            return {\n",
        "                \"status\": \"erro\",\n",
        "                \"mensagem\": str(e)\n",
        "            }\n",
        "\n",
        "    # Outros métodos como \"extrair_dominio\", \"verificar_fonte\", \"baixar_artigo\", etc.\n",
        "    # permanecem iguais.\n",
        "\n",
        "    def verificar_noticia(self, url):\n",
        "        \"\"\"Função principal para verificar uma notícia.\"\"\"\n",
        "        print(f\"Analisando URL: {url}\")\n",
        "\n",
        "        # Tentar uma verificação simples primeiro\n",
        "        resultado_simples = self.verificar_url_simples(url)\n",
        "        if resultado_simples[\"status\"] == \"erro\":\n",
        "            return resultado_simples  # Retorna o erro direto se a verificação simples falhar\n",
        "\n",
        "        # Se a verificação simples for bem-sucedida, proceder para análise avançada\n",
        "        artigo = self.baixar_artigo(url)\n",
        "\n",
        "        if not artigo:\n",
        "            return {\"status\": \"erro\", \"mensagem\": \"Não foi possível baixar o artigo\"}\n",
        "\n",
        "        resultados = self.analisar_credibilidade(artigo)\n",
        "\n",
        "        # Calcular pontuação de credibilidade (0-100)\n",
        "        pontuacao = 70  # Base inicial neutro\n",
        "\n",
        "        # Ajustes baseados na fonte\n",
        "        pontuacao += resultados.get(\"pontos_fonte\", 0)\n",
        "\n",
        "        # Fatores que reduzem a pontuação\n",
        "        if \"alerta_tamanho\" in resultados:\n",
        "            pontuacao -= 15\n",
        "        if \"alerta_sensacionalismo\" in resultados:\n",
        "            if \"Alto índice\" in resultados[\"alerta_sensacionalismo\"]:\n",
        "                pontuacao -= 20\n",
        "            else:\n",
        "                pontuacao -= 10\n",
        "        if \"alerta_fontes\" in resultados:\n",
        "            pontuacao -= 10\n",
        "        if \"alerta_opiniao\" in resultados:\n",
        "            pontuacao -= 15\n",
        "        if \"alerta_incerteza\" in resultados and resultados[\"indice_incerteza\"] > 3:\n",
        "            pontuacao -= 10\n",
        "        if \"alerta_exclamacoes\" in resultados:\n",
        "            pontuacao -= 5\n",
        "        if \"alerta_imagens_descricao\" in resultados:\n",
        "            pontuacao -= 5\n",
        "\n",
        "        # Fatores que aumentam a pontuação\n",
        "        if resultados.get(\"num_citacoes\", 0) > 3:\n",
        "            pontuacao += 10\n",
        "        elif resultados.get(\"num_citacoes\", 0) > 0:\n",
        "            pontuacao += 5\n",
        "        if resultados.get(\"num_urls\", 0) > 2:\n",
        "            pontuacao += 5\n",
        "        if resultados.get(\"num_imagens\", 0) > 1:\n",
        "            pontuacao += 5\n",
        "        if resultados.get(\"artigos_similares\", 0) > 3:\n",
        "            pontuacao += 5\n",
        "\n",
        "        # Ajustar pontuação final\n",
        "        pontuacao = max(0, min(100, pontuacao))\n",
        "        resultados[\"pontuacao_credibilidade\"] = pontuacao\n",
        "\n",
        "        # Avaliação qualitativa\n",
        "        if pontuacao >= 85:\n",
        "            resultados[\"avaliacao\"] = \"Alta credibilidade\"\n",
        "        elif pontuacao >= 70:\n",
        "            resultados[\"avaliacao\"] = \"Boa credibilidade\"\n",
        "        elif pontuacao >= 50:\n",
        "            resultados[\"avaliacao\"] = \"Credibilidade moderada\"\n",
        "        elif pontuacao >= 30:\n",
        "            resultados[\"avaliacao\"] = \"Credibilidade questionável\"\n",
        "        else:\n",
        "            resultados[\"avaliacao\"] = \"Baixa credibilidade\"\n",
        "\n",
        "        return resultados"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGDnN0haHF3w",
        "outputId": "a60bbd32-71eb-486d-ec83-5d1a022d885d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing verificacao_avancada.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run verificacao_simples.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8t-JcVxBWlx",
        "outputId": "95398590-9d5d-4104-b59f-7e7d85ebc357"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verificador de notícias avançado - use a função verificar_url(sua_url) para analisar uma notícia\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEAZ2xYVBJJc",
        "outputId": "2e1105a8-00fc-4972-e7b3-5971227c364e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'drive', 'verificacao_simples.py', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XSZGdpTBFn5",
        "outputId": "9ee26a72-b08f-46a2-ca3c-10986fdd87a7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run verificacao_avancada.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "pRJzBKu684Z-",
        "outputId": "142bd58b-ba98-423e-dc7b-40de362e3244"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "File `'verificacao_avancada.py'` not found.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0mfpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_lst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_finder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    715\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/utils/path.py\u001b[0m in \u001b[0;36mget_py_filename\u001b[0;34m(name, force_win32)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'File `%r` not found.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: File `'verificacao_avancada.py'` not found.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-fe569a357e1e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'run'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'verificacao_avancada.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2416\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2417\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2418\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2419\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-52>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, parameter_s, runner, file_finder)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nt'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"^'.*'$\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m                 \u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'For Windows, use double quotes to wrap a filename: %run \"mypath\\\\myfile.py\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: File `'verificacao_avancada.py'` not found."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d5X2zS38oaF",
        "outputId": "dc530015-a722-4bdb-e1f1-73af69276fd4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run verificacao_avancada.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "ma4Ja5OrDnOo",
        "outputId": "1d877ad7-6546-44f8-a43d-5ef97776a299"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "File `'verificacao_avancada.py'` not found.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0mfpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_lst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_finder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    715\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/utils/path.py\u001b[0m in \u001b[0;36mget_py_filename\u001b[0;34m(name, force_win32)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'File `%r` not found.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: File `'verificacao_avancada.py'` not found.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-fe569a357e1e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'run'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'verificacao_avancada.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2416\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2417\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2418\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2419\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-52>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, parameter_s, runner, file_finder)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nt'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"^'.*'$\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m                 \u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'For Windows, use double quotes to wrap a filename: %run \"mypath\\\\myfile.py\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: File `'verificacao_avancada.py'` not found."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import functions_framework\n",
        "import json\n",
        "import logging\n",
        "import os # Para obter variáveis de ambiente se necessário\n",
        "\n",
        "# Importar a classe e a função de formatação do outro arquivo\n",
        "from verificacao_avancada import VerificadorNoticiasAvancado, formatar_resultado_texto\n",
        "\n",
        "# Configura o logging para Cloud Functions\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Instanciar o verificador globalmente?\n",
        "# Pró: Reutiliza o objeto e listas carregadas entre invocações (se a instância for mantida \"quente\")\n",
        "# Contra: Pode manter estado indesejado (embora esta classe pareça stateless)\n",
        "# Alternativa: Instanciar dentro da função para garantir estado limpo a cada requisição.\n",
        "# Vamos instanciar dentro da função por simplicidade e garantia de limpeza.\n",
        "# verificador_global = VerificadorNoticiasAvancado()\n",
        "\n",
        "\n",
        "@functions_framework.http\n",
        "def handle_webhook_request(request):\n",
        "    \"\"\"\n",
        "    Responde a requisições HTTP vindas do Dialogflow CX/ES.\n",
        "    Espera um JSON com a estrutura do Dialogflow e extrai um parâmetro 'url'.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        request_json = request.get_json(silent=True)\n",
        "        logging.info(f\"Recebido request do Dialogflow: {json.dumps(request_json, indent=2)}\")\n",
        "\n",
        "        # Verificar se o JSON foi recebido e tem a estrutura esperada\n",
        "        if not request_json:\n",
        "            logging.error(\"Request sem corpo JSON.\")\n",
        "            return create_dialogflow_response(\"Erro: Não recebi dados na requisição.\"), 400\n",
        "\n",
        "        # --- Extrair a URL do parâmetro do Dialogflow ---\n",
        "        # A localização exata do parâmetro pode variar (Dialogflow ES vs CX)\n",
        "        # Tentativa comum para ES/CX: sessionInfo.parameters ou queryResult.parameters\n",
        "        url_noticia = None\n",
        "        if 'sessionInfo' in request_json and 'parameters' in request_json['sessionInfo']:\n",
        "            url_noticia = request_json['sessionInfo']['parameters'].get('url') # Ajuste 'url' se o nome do parâmetro for diferente\n",
        "        elif 'queryResult' in request_json and 'parameters' in request_json['queryResult']:\n",
        "             url_noticia = request_json['queryResult']['parameters'].get('url') # Ajuste 'url' se o nome do parâmetro for diferente\n",
        "\n",
        "        if not url_noticia:\n",
        "            logging.warning(\"Parâmetro 'url' não encontrado na requisição do Dialogflow.\")\n",
        "            # Tenta pegar de uma possível mensagem de texto direto (menos comum para parâmetros)\n",
        "            if 'text' in request_json.get('message', {}):\n",
        "                 possible_url = request_json['message']['text']\n",
        "                 # Validação básica se parece uma URL\n",
        "                 if isinstance(possible_url, str) and possible_url.startswith(('http://', 'https://')):\n",
        "                      url_noticia = possible_url\n",
        "                      logging.info(f\"URL encontrada no texto da mensagem: {url_noticia}\")\n",
        "\n",
        "        # Se ainda não encontrou a URL, retorna erro amigável\n",
        "        if not url_noticia or not isinstance(url_noticia, str):\n",
        "            logging.error(\"URL da notícia não fornecida ou inválida na requisição.\")\n",
        "            return create_dialogflow_response(\"Por favor, me envie a URL da notícia que você quer verificar.\"), 400\n",
        "\n",
        "        # --- Executar a Verificação ---\n",
        "        logging.info(f\"Iniciando verificação para a URL: {url_noticia}\")\n",
        "        # Instanciar o verificador a cada requisição\n",
        "        verificador = VerificadorNoticiasAvancado()\n",
        "        resultado_analise = verificador.verificar_noticia(url_noticia)\n",
        "\n",
        "        # --- Formatar a Resposta para o Dialogflow ---\n",
        "        texto_resposta = gerar_resposta_dialogflow(resultado_analise)\n",
        "        logging.info(f\"Enviando resposta para Dialogflow: {texto_resposta[:200]}...\") # Loga início da resposta\n",
        "        return create_dialogflow_response(texto_resposta)\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.exception(\"Erro inesperado ao processar a requisição do webhook.\") # Loga o traceback completo\n",
        "        # Retorna uma mensagem de erro genérica para o usuário\n",
        "        return create_dialogflow_response(\"Desculpe, ocorreu um erro interno ao tentar verificar a notícia. Tente novamente mais tarde.\"), 500\n",
        "\n",
        "def gerar_resposta_dialogflow(resultado):\n",
        "    \"\"\"Gera uma string de resposta amigável baseada no resultado da análise.\"\"\"\n",
        "\n",
        "    if not resultado:\n",
        "         return \"Desculpe, não consegui obter um resultado para a análise.\"\n",
        "\n",
        "    status = resultado.get('status_geral', 'erro')\n",
        "\n",
        "    if status.startswith('erro'):\n",
        "        msg_erro = resultado.get('mensagem', 'Não foi possível completar a análise.')\n",
        "        if status == 'erro_ao_baixar':\n",
        "             # Erros comuns de download\n",
        "             if '404' in msg_erro:\n",
        "                  return f\"Não consegui encontrar a página da notícia (Erro 404). Verifique se a URL está correta: {resultado.get('url')}\"\n",
        "             elif 'Timeout' in msg_erro:\n",
        "                  return f\"A página demorou muito para responder. O site pode estar fora do ar ou lento. Tente novamente mais tarde. URL: {resultado.get('url')}\"\n",
        "             elif 'Connection refused' in msg_erro or 'SSL' in msg_erro:\n",
        "                  return f\"Tive problemas para conectar ao site da notícia. Pode ser um problema temporário ou de segurança do site. URL: {resultado.get('url')}\"\n",
        "             else:\n",
        "                  return f\"Não consegui acessar a notícia para analisar. Verifique a URL ou tente mais tarde. Detalhe: {msg_erro[:100]}\" # Limita tamanho do erro técnico\n",
        "        else:\n",
        "             return f\"Ocorreu um problema durante a análise. Detalhe: {msg_erro[:100]}\"\n",
        "\n",
        "    # Análise completa\n",
        "    pontuacao = resultado.get('pontuacao_credibilidade', 0)\n",
        "    avaliacao = resultado.get('avaliacao', 'Indeterminada')\n",
        "    dominio = resultado.get('dominio', 'Domínio desconhecido')\n",
        "    status_fonte = resultado.get('status_fonte', 'não categorizada')\n",
        "\n",
        "    resposta = []\n",
        "    resposta.append(f\"Análise da notícia em '{dominio}':\")\n",
        "    resposta.append(f\"Pontuação de Credibilidade: {pontuacao}/100 ({avaliacao}).\")\n",
        "\n",
        "    # Adiciona detalhes sobre a fonte\n",
        "    if status_fonte == 'confiável' or status_fonte == 'confiável (subdomínio)':\n",
        "        resposta.append(\"✅ A fonte é considerada confiável.\")\n",
        "    elif status_fonte == 'suspeita':\n",
        "        resposta.append(f\"⚠️ Atenção: Esta fonte ('{dominio}') é conhecida por notícias satíricas ou de baixa credibilidade.\")\n",
        "    else: # não categorizada ou erro\n",
        "        resposta.append(\"ℹ️ A reputação desta fonte não está na minha lista principal.\")\n",
        "\n",
        "    # Adiciona os principais alertas, se houver\n",
        "    alertas_importantes = []\n",
        "    if \"alerta_sensacionalismo\" in resultado:\n",
        "         alertas_importantes.append(\"linguagem sensacionalista\")\n",
        "    if \"alerta_opiniao\" in resultado:\n",
        "         alertas_importantes.append(\"forte caráter opinativo\")\n",
        "    if \"alerta_incerteza\" in resultado:\n",
        "         alertas_importantes.append(\"uso de termos de incerteza\")\n",
        "    if \"alerta_tamanho\" in resultado:\n",
        "        alertas_importantes.append(\"texto muito curto\")\n",
        "\n",
        "    if alertas_importantes:\n",
        "         resposta.append(f\"🚨 Pontos de atenção detectados: {'; '.join(alertas_importantes)}.\")\n",
        "\n",
        "    # Conclusão geral\n",
        "    if pontuacao < 30:\n",
        "        resposta.append(\"Recomendo muita cautela e buscar outras fontes antes de confiar nesta notícia.\")\n",
        "    elif pontuacao < 50:\n",
        "        resposta.append(\"É bom verificar esta informação em outras fontes mais estabelecidas.\")\n",
        "    elif pontuacao < 70:\n",
        "         resposta.append(\"Parece razoável, mas fique atento aos pontos levantados.\")\n",
        "    else:\n",
        "        resposta.append(\"A notícia parece ter boa credibilidade com base na análise.\")\n",
        "\n",
        "    # Limitar o tamanho total da resposta para Dialogflow (geralmente tem limites)\n",
        "    return \" \".join(resposta)[:4000] # Limite generoso, ajuste se necessário\n",
        "\n",
        "def create_dialogflow_response(text_message):\n",
        "    \"\"\"Cria a estrutura JSON de resposta padrão do Dialogflow.\"\"\"\n",
        "    response_data = {\n",
        "        \"fulfillment_response\": {\n",
        "            \"messages\": [\n",
        "                {\n",
        "                    \"text\": {\n",
        "                        # O texto deve ser uma lista\n",
        "                        \"text\": [text_message]\n",
        "                    }\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    }\n",
        "    # Retorna a string JSON, status HTTP e Content-Type\n",
        "    return json.dumps(response_data), 200, {'Content-Type': 'application/json; charset=utf-8'}\n",
        "\n",
        "\n",
        "# --- Bloco para Execução Local (Testes) ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Executando Verificador de Notícias localmente...\")\n",
        "    # Exemplo de URLs para teste:\n",
        "    url_confiavel = \"https://g1.globo.com/economia/noticia/2023/10/26/bc-mantera-cortes-de-05-ponto-percentual-nas-proximas-reunioes-indicam-comunicado-e-declaracoes-de-campos-neto.ghtml\" # Exemplo G1\n",
        "    url_suspeita = \"https://www.theonion.com/study-finds-working-from-home-increases-productivity-am-1849809884\" # Exemplo The Onion (Sátira)\n",
        "    url_nao_categorizada = \"https://www.exemploqualquer.com/noticia-teste\" # Exemplo inválido ou não categorizado\n",
        "    url_erro_404 = \"https://g1.globo.com/pagina-que-nao-existe-12345\"\n",
        "    url_curta = \"https://www.tecmundo.com.br/voxel\" # Página principal, não artigo\n",
        "\n",
        "    urls_para_testar = [\n",
        "         #url_confiavel,\n",
        "         url_suspeita,\n",
        "         #url_erro_404,\n",
        "         #url_curta\n",
        "         # Adicione outras URLs que queira testar\n",
        "    ]\n",
        "\n",
        "    if not urls_para_testar:\n",
        "        try:\n",
        "             url_input = input(\"Digite a URL da notícia para verificar: \")\n",
        "             urls_para_testar.append(url_input.strip())\n",
        "        except EOFError: # Handle case where input is redirected and empty\n",
        "             print(\"Nenhuma URL fornecida para teste.\")\n",
        "             urls_para_testar = []\n",
        "\n",
        "\n",
        "    verificador_local = VerificadorNoticiasAvancado()\n",
        "\n",
        "    for url in urls_para_testar:\n",
        "        if not url: continue\n",
        "        print(f\"\\n--- Testando URL: {url} ---\")\n",
        "        try:\n",
        "            resultado = verificador_local.verificar_noticia(url)\n",
        "            # Usar a função de formatação para exibir o resultado no console\n",
        "            print(formatar_resultado_texto(resultado))\n",
        "            # Simular como seria a resposta para o Dialogflow\n",
        "            print(\"\\n--- Resposta Simulada para Dialogflow ---\")\n",
        "            print(gerar_resposta_dialogflow(resultado))\n",
        "            print(\"----------------------------------------\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro inesperado durante o teste local da URL {url}: {e}\")\n",
        "            logging.exception(\"Erro no teste local\") # Loga o traceback\n",
        "\n",
        "    print(\"\\nTestes locais concluídos.\")"
      ],
      "metadata": {
        "id": "a1bRjIJITNGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile verificacao_simples.py\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import ssl\n",
        "import urllib3\n",
        "import json\n",
        "from urllib.parse import urlparse\n",
        "import hashlib\n",
        "from collections import Counter\n",
        "import time\n",
        "\n",
        "# Desabilitar avisos SSL\n",
        "urllib3.disable_warnings()\n",
        "\n",
        "class VerificadorNoticiasAvancado:\n",
        "    def __init__(self):\n",
        "        print(\"Verificador de Notícias Avançado inicializado!\")\n",
        "        # Carregar listas de fontes confiáveis e não confiáveis\n",
        "        self.fontes_confiaveis = [\n",
        "            'g1.globo.com', 'bbc.com', 'bbc.co.uk', 'reuters.com', 'apnews.com',\n",
        "            'nytimes.com', 'washingtonpost.com', 'estadao.com.br', 'folha.uol.com.br',\n",
        "            'cnn.com', 'valor.globo.com', 'info.abril.com.br', 'oglobo.globo.com',\n",
        "            'dw.com', 'npr.org', 'tecmundo.com.br', 'canaltech.com.br', 'olhardigital.com.br'\n",
        "        ]\n",
        "\n",
        "        self.fontes_suspeitas = [\n",
        "            'theonion.com', 'sensacionalista.com.br', 'breitbart.com', 'infowars.com',\n",
        "            'clickhole.com', 'nationalreport.net', 'worldnewsdailyreport.com',\n",
        "            'dailycurrant.com', 'empirenews.net'\n",
        "        ]\n",
        "\n",
        "        # Palavras para análise linguística\n",
        "        self.palavras_sensacionalistas = {\n",
        "            \"chocante\": 2, \"inacreditável\": 2, \"surpreendente\": 1, \"sensacional\": 2,\n",
        "            \"impressionante\": 1, \"alarmante\": 1.5, \"exclusivo\": 1, \"urgente\": 1.5,\n",
        "            \"shocking\": 2, \"unbelievable\": 2, \"amazing\": 1, \"surprising\": 1,\n",
        "            \"imperdível\": 1.5, \"incrível\": 1.5, \"absurdo\": 1.5, \"escandaloso\": 2,\n",
        "            \"bomba\": 1.5, \"inédito\": 1, \"revelado\": 1, \"segredo\": 1.5,\n",
        "            \"polêmico\": 1.5, \"controverso\": 1, \"escândalo\": 1.5\n",
        "        }\n",
        "\n",
        "        self.palavras_incerteza = [\n",
        "            \"talvez\", \"possivelmente\", \"provavelmente\", \"pode ser\", \"especula-se\",\n",
        "            \"acredita-se\", \"sugere\", \"alega\", \"supostamente\", \"rumores\",\n",
        "            \"maybe\", \"possibly\", \"probably\", \"might be\", \"it is speculated\",\n",
        "            \"it is believed\", \"suggests\", \"allegedly\", \"supposedly\", \"rumors\"\n",
        "        ]\n",
        "\n",
        "        self.expressoes_opinativas = [\n",
        "            \"eu acho\", \"na minha opinião\", \"parece que\", \"acredito que\",\n",
        "            \"i think\", \"in my opinion\", \"it seems that\", \"i believe\"\n",
        "        ]\n",
        "\n",
        "    def extrair_dominio(self, url):\n",
        "        \"\"\"Extrai o domínio base da URL.\"\"\"\n",
        "        parsed_url = urlparse(url)\n",
        "        domain = parsed_url.netloc\n",
        "        return domain\n",
        "\n",
        "    def verificar_fonte(self, url):\n",
        "        \"\"\"Verifica o domínio da URL contra as listas de fontes.\"\"\"\n",
        "        dominio = self.extrair_dominio(url)\n",
        "        resultado = {}\n",
        "\n",
        "        # Verificar se o domínio está nas listas\n",
        "        if any(dominio.endswith(fonte) for fonte in self.fontes_confiaveis):\n",
        "            resultado[\"status_fonte\"] = \"confiável\"\n",
        "            resultado[\"pontos_fonte\"] = 20\n",
        "        elif any(dominio.endswith(fonte) for fonte in self.fontes_suspeitas):\n",
        "            resultado[\"status_fonte\"] = \"suspeita\"\n",
        "            resultado[\"pontos_fonte\"] = -30\n",
        "            resultado[\"alerta_fonte\"] = \"Fonte conhecida por conteúdo satírico ou de baixa credibilidade\"\n",
        "        else:\n",
        "            resultado[\"status_fonte\"] = \"não categorizada\"\n",
        "            resultado[\"pontos_fonte\"] = 0\n",
        "\n",
        "        resultado[\"dominio\"] = dominio\n",
        "        return resultado\n",
        "\n",
        "    def baixar_artigo(self, url):\n",
        "        \"\"\"Baixa e extrai o conteúdo básico de um artigo a partir da URL\"\"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers, verify=False, timeout=10)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Tenta extrair o título\n",
        "            titulo = soup.title.text.strip() if soup.title else \"Título não encontrado\"\n",
        "\n",
        "            # Tenta extrair o conteúdo do artigo (abordagem mais abrangente)\n",
        "            content = \"\"\n",
        "\n",
        "            # Procura por conteúdo em elementos comuns\n",
        "            for tag in ['article', 'main', '.content', '.post', '.entry', '#content', '.article-body', '.news-text']:\n",
        "                if tag.startswith('.') or tag.startswith('#'):\n",
        "                    elements = soup.select(tag)\n",
        "                else:\n",
        "                    elements = soup.find_all(tag)\n",
        "\n",
        "                if elements:\n",
        "                    for element in elements:\n",
        "                        content += element.get_text(separator=' ', strip=True) + \" \"\n",
        "                    break\n",
        "\n",
        "            # Se não encontrou conteúdo nos elementos específicos, pega todos os parágrafos\n",
        "            if not content:\n",
        "                paragraphs = soup.find_all('p')\n",
        "                content = ' '.join([p.get_text(strip=True) for p in paragraphs])\n",
        "\n",
        "            # Tenta encontrar a data\n",
        "            data = None\n",
        "            # Procura por metadados comuns\n",
        "            for meta in soup.find_all('meta'):\n",
        "                if meta.get('property') in ['article:published_time', 'og:published_time', 'publication_date']:\n",
        "                    data = meta.get('content')\n",
        "                    break\n",
        "\n",
        "            # Procura por elementos que possam conter autores\n",
        "            autores = []\n",
        "            for author_tag in ['author', 'byline', '.author', '.byline', '[rel=author]']:\n",
        "                if author_tag.startswith('.') or author_tag.startswith('['):\n",
        "                    elements = soup.select(author_tag)\n",
        "                else:\n",
        "                    elements = soup.find_all(author_tag)\n",
        "\n",
        "                if elements:\n",
        "                    for element in elements:\n",
        "                        autor = element.get_text(strip=True)\n",
        "                        if autor and len(autor) < 100:  # Evitar pegar conteúdo grande por engano\n",
        "                            autores.append(autor)\n",
        "\n",
        "            # Remover duplicações nos autores\n",
        "            autores = list(dict.fromkeys(autores))\n",
        "\n",
        "            # Extrair imagens\n",
        "            imagens = []\n",
        "            for img in soup.find_all('img'):\n",
        "                src = img.get('src')\n",
        "                alt = img.get('alt', '')\n",
        "                if src and not src.startswith('data:'):\n",
        "                    # Garantir URL completa\n",
        "                    if src.startswith('/'):\n",
        "                        parsed_url = urlparse(url)\n",
        "                        base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
        "                        src = base_url + src\n",
        "                    imagens.append({\n",
        "                        'url': src,\n",
        "                        'alt': alt\n",
        "                    })\n",
        "\n",
        "            # Extrair palavras-chave\n",
        "            keywords = []\n",
        "            meta_keywords = soup.find('meta', attrs={'name': 'keywords'})\n",
        "            if meta_keywords:\n",
        "                keywords = meta_keywords.get('content', '').split(',')\n",
        "                keywords = [k.strip() for k in keywords]\n",
        "\n",
        "            return {\n",
        "                'titulo': titulo,\n",
        "                'conteudo': content,\n",
        "                'data': data,\n",
        "                'autores': autores,\n",
        "                'url': url,\n",
        "                'imagens': imagens,\n",
        "                'keywords': keywords\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao baixar artigo: {e}\")\n",
        "            return None\n",
        "\n",
        "    def analisar_linguagem(self, texto):\n",
        "        \"\"\"Analisa o tom e a linguagem usada no texto.\"\"\"\n",
        "        resultados = {}\n",
        "\n",
        "        # Análise de sensacionalismo com pesos\n",
        "        count_sensacionalismo = 0\n",
        "        palavras_encontradas = []\n",
        "\n",
        "        for palavra, peso in self.palavras_sensacionalistas.items():\n",
        "            ocorrencias = len(re.findall(r'\\b' + palavra + r'\\b', texto.lower()))\n",
        "            if ocorrencias > 0:\n",
        "                count_sensacionalismo += ocorrencias * peso\n",
        "                palavras_encontradas.append(f\"{palavra} ({ocorrencias}x)\")\n",
        "\n",
        "        resultados[\"indice_sensacionalismo\"] = count_sensacionalismo\n",
        "        resultados[\"palavras_sensacionalistas\"] = palavras_encontradas\n",
        "\n",
        "        if count_sensacionalismo > 3:\n",
        "            resultados[\"alerta_sensacionalismo\"] = \"Alto índice de linguagem sensacionalista detectado\"\n",
        "        elif count_sensacionalismo > 1.5:\n",
        "            resultados[\"alerta_sensacionalismo\"] = \"Linguagem potencialmente sensacionalista detectada\"\n",
        "\n",
        "        # Análise de incerteza\n",
        "        count_incerteza = 0\n",
        "        palavras_incerteza = []\n",
        "\n",
        "        for palavra in self.palavras_incerteza:\n",
        "            ocorrencias = len(re.findall(r'\\b' + palavra + r'\\b', texto.lower()))\n",
        "            if ocorrencias > 0:\n",
        "                count_incerteza += ocorrencias\n",
        "                palavras_incerteza.append(palavra)\n",
        "\n",
        "        resultados[\"indice_incerteza\"] = count_incerteza\n",
        "        resultados[\"palavras_incerteza\"] = palavras_incerteza\n",
        "\n",
        "        if count_incerteza > 3:\n",
        "            resultados[\"alerta_incerteza\"] = \"Alto uso de termos de incerteza detectado\"\n",
        "\n",
        "        # Análise de opinião\n",
        "        count_opiniao = 0\n",
        "        expressoes_opiniao = []\n",
        "\n",
        "        for expressao in self.expressoes_opinativas:\n",
        "            ocorrencias = len(re.findall(r'\\b' + expressao + r'\\b', texto.lower()))\n",
        "            if ocorrencias > 0:\n",
        "                count_opiniao += ocorrencias\n",
        "                expressoes_opiniao.append(expressao)\n",
        "\n",
        "        resultados[\"indice_opiniao\"] = count_opiniao\n",
        "        resultados[\"expressoes_opiniao\"] = expressoes_opiniao\n",
        "\n",
        "        if count_opiniao > 2:\n",
        "            resultados[\"alerta_opiniao\"] = \"Texto com forte caráter opinativo detectado\"\n",
        "\n",
        "        # Análise de exclamações (indicativo de sensacionalismo)\n",
        "        exclamacoes = len(re.findall(r'!', texto))\n",
        "        resultados[\"num_exclamacoes\"] = exclamacoes\n",
        "\n",
        "        if exclamacoes > 5:\n",
        "            resultados[\"alerta_exclamacoes\"] = \"Uso excessivo de pontos de exclamação\"\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def verificar_imagens(self, imagens):\n",
        "        \"\"\"Analisa as imagens associadas ao artigo.\"\"\"\n",
        "        resultados = {}\n",
        "\n",
        "        if not imagens:\n",
        "            resultados[\"alerta_imagens\"] = \"Nenhuma imagem encontrada no artigo\"\n",
        "            return resultados\n",
        "\n",
        "        resultados[\"num_imagens\"] = len(imagens)\n",
        "\n",
        "        # Verificar descrições das imagens (alt text)\n",
        "        imagens_sem_descricao = sum(1 for img in imagens if not img.get('alt'))\n",
        "        resultados[\"imagens_sem_descricao\"] = imagens_sem_descricao\n",
        "\n",
        "        if imagens_sem_descricao == len(imagens) and len(imagens) > 0:\n",
        "            resultados[\"alerta_imagens_descricao\"] = \"Nenhuma das imagens possui descrição adequada\"\n",
        "\n",
        "        # Calcular hash básico das URLs de imagens para potencial comparação futura\n",
        "        hashes_urls = [hashlib.md5(img['url'].encode()).hexdigest()[:8] for img in imagens]\n",
        "        resultados[\"hashes_imagens\"] = hashes_urls\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def buscar_noticias_similares(self, titulo, keywords):\n",
        "        \"\"\"Busca por notícias similares na web usando palavras-chave do artigo.\"\"\"\n",
        "        resultados = {}\n",
        "\n",
        "        try:\n",
        "            # Preparar termos de busca\n",
        "            termos_busca = titulo\n",
        "            if keywords:\n",
        "                termos_busca += \" \" + \" \".join(keywords[:3])  # Limitar a 3 keywords\n",
        "\n",
        "            # Simular resultados para demonstração\n",
        "            # Em uma implementação real, você usaria uma API como Google News ou similar\n",
        "            resultados[\"artigos_similares\"] = 5  # Simulando 5 artigos similares\n",
        "            resultados[\"similaridade\"] = \"média\"  # baixa, média, alta\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao buscar notícias similares: {e}\")\n",
        "            resultados[\"erro_busca\"] = str(e)\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def analisar_credibilidade(self, artigo):\n",
        "        \"\"\"Análise combinada de credibilidade baseada em características do texto e fonte.\"\"\"\n",
        "        if not artigo or not artigo.get('conteudo'):\n",
        "            return {\"erro\": \"Artigo não disponível para análise\"}\n",
        "\n",
        "        resultados = {}\n",
        "        conteudo = artigo['conteudo']\n",
        "\n",
        "        # 1. Verificação de fonte/domínio\n",
        "        info_fonte = self.verificar_fonte(artigo['url'])\n",
        "        resultados.update(info_fonte)\n",
        "\n",
        "        # 2. Copiar informações básicas\n",
        "        resultados[\"titulo\"] = artigo['titulo']\n",
        "        resultados[\"data_publicacao\"] = artigo['data']\n",
        "        resultados[\"autores\"] = artigo['autores']\n",
        "        resultados[\"url\"] = artigo['url']\n",
        "\n",
        "        # 3. Análise básica do conteúdo\n",
        "        palavras = len(conteudo.split())\n",
        "        resultados[\"num_palavras\"] = palavras\n",
        "        if palavras < 100:\n",
        "            resultados[\"alerta_tamanho\"] = \"Texto muito curto, pode não ser um artigo completo\"\n",
        "\n",
        "        # 4. Análise detalhada da linguagem\n",
        "        analise_linguagem = self.analisar_linguagem(conteudo)\n",
        "        resultados.update(analise_linguagem)\n",
        "\n",
        "        # 5. Verificação de citações\n",
        "        citacoes = re.findall(r'\"([^\"]+)\"', conteudo)\n",
        "        resultados[\"num_citacoes\"] = len(citacoes)\n",
        "\n",
        "        if len(citacoes) == 0:\n",
        "            resultados[\"alerta_fontes\"] = \"Nenhuma citação direta encontrada\"\n",
        "\n",
        "        # 6. Verificação de URLs externos (referências)\n",
        "        urls = re.findall(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', conteudo)\n",
        "        resultados[\"num_urls\"] = len(urls)\n",
        "\n",
        "        # 7. Análise de imagens\n",
        "        resultados_imagens = self.verificar_imagens(artigo.get('imagens', []))\n",
        "        resultados.update(resultados_imagens)\n",
        "\n",
        "        # 8. Verificação cruzada com outras notícias\n",
        "        resultados_similares = self.buscar_noticias_similares(\n",
        "            artigo['titulo'],\n",
        "            artigo.get('keywords', [])\n",
        "        )\n",
        "        resultados.update(resultados_similares)\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def verificar_noticia(self, url):\n",
        "        \"\"\"Função principal para verificar uma notícia.\"\"\"\n",
        "        print(f\"Analisando URL: {url}\")\n",
        "        artigo = self.baixar_artigo(url)\n",
        "\n",
        "        if not artigo:\n",
        "            return {\"status\": \"erro\", \"mensagem\": \"Não foi possível baixar o artigo\"}\n",
        "\n",
        "        resultados = self.analisar_credibilidade(artigo)\n",
        "\n",
        "        # Calcular pontuação de credibilidade (0-100)\n",
        "        pontuacao = 70  # Base inicial neutro\n",
        "\n",
        "        # Ajustes baseados na fonte\n",
        "        pontuacao += resultados.get(\"pontos_fonte\", 0)\n",
        "\n",
        "        # Fatores que reduzem a pontuação\n",
        "        if \"alerta_tamanho\" in resultados:\n",
        "            pontuacao -= 15\n",
        "        if \"alerta_sensacionalismo\" in resultados:\n",
        "            if \"Alto índice\" in resultados[\"alerta_sensacionalismo\"]:\n",
        "                pontuacao -= 20\n",
        "            else:\n",
        "                pontuacao -= 10\n",
        "        if \"alerta_fontes\" in resultados:\n",
        "            pontuacao -= 10\n",
        "        if \"alerta_opiniao\" in resultados:\n",
        "            pontuacao -= 15\n",
        "        if \"alerta_incerteza\" in resultados and resultados[\"indice_incerteza\"] > 3:\n",
        "            pontuacao -= 10\n",
        "        if \"alerta_exclamacoes\" in resultados:\n",
        "            pontuacao -= 5\n",
        "        if \"alerta_imagens_descricao\" in resultados:\n",
        "            pontuacao -= 5\n",
        "\n",
        "        # Fatores que aumentam a pontuação\n",
        "        if resultados.get(\"num_citacoes\", 0) > 3:\n",
        "            pontuacao += 10\n",
        "        elif resultados.get(\"num_citacoes\", 0) > 0:\n",
        "            pontuacao += 5\n",
        "        if resultados.get(\"num_urls\", 0) > 2:\n",
        "            pontuacao += 5\n",
        "        if resultados.get(\"num_imagens\", 0) > 1:\n",
        "            pontuacao += 5\n",
        "        if resultados.get(\"artigos_similares\", 0) > 3:\n",
        "            pontuacao += 5\n",
        "\n",
        "        # Ajustar pontuação final\n",
        "        pontuacao = max(0, min(100, pontuacao))\n",
        "        resultados[\"pontuacao_credibilidade\"] = pontuacao\n",
        "\n",
        "        # Avaliação qualitativa\n",
        "        if pontuacao >= 85:\n",
        "            resultados[\"avaliacao\"] = \"Alta credibilidade\"\n",
        "        elif pontuacao >= 70:\n",
        "            resultados[\"avaliacao\"] = \"Boa credibilidade\"\n",
        "        elif pontuacao >= 50:\n",
        "            resultados[\"avaliacao\"] = \"Credibilidade moderada\"\n",
        "        elif pontuacao >= 30:\n",
        "            resultados[\"avaliacao\"] = \"Credibilidade questionável\"\n",
        "        else:\n",
        "            resultados[\"avaliacao\"] = \"Baixa credibilidade\"\n",
        "\n",
        "        return resultados\n",
        "\n",
        "\n",
        "# Função para demonstração com formatação de saída melhorada\n",
        "def verificar_url(url):\n",
        "    verificador = VerificadorNoticiasAvancado()\n",
        "    tempo_inicio = time.time()\n",
        "    resultado = verificador.verificar_noticia(url)\n",
        "    tempo_execucao = time.time() - tempo_inicio\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"RELATÓRIO DE VERIFICAÇÃO DE NOTÍCIA\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"URL: {url}\")\n",
        "    print(f\"Domínio: {resultado.get('dominio', 'N/A')}\")\n",
        "    print(f\"Status da fonte: {resultado.get('status_fonte', 'N/A')}\")\n",
        "    print(\"-\"*50)\n",
        "\n",
        "    print(f\"Título: {resultado.get('titulo', 'N/A')}\")\n",
        "    print(f\"Data de publicação: {resultado.get('data_publicacao', 'N/A')}\")\n",
        "    print(f\"Autores: {', '.join(resultado.get('autores', ['N/A']))}\")\n",
        "    print(f\"Número de palavras: {resultado.get('num_palavras', 0)}\")\n",
        "\n",
        "    # Métricas quantitativas\n",
        "    print(\"-\"*50)\n",
        "    print(\"MÉTRICAS:\")\n",
        "    print(f\"• Citações: {resultado.get('num_citacoes', 0)}\")\n",
        "    print(f\"• URLs referenciadas: {resultado.get('num_urls', 0)}\")\n",
        "    print(f\"• Índice de sensacionalismo: {resultado.get('indice_sensacionalismo', 0)}\")\n",
        "    if resultado.get('palavras_sensacionalistas'):\n",
        "        print(f\"  - Termos: {', '.join(resultado.get('palavras_sensacionalistas', []))}\")\n",
        "    print(f\"• Índice de incerteza: {resultado.get('indice_incerteza', 0)}\")\n",
        "    if resultado.get('indice_incerteza', 0) > 0:\n",
        "        print(f\"  - Termos: {', '.join(resultado.get('palavras_incerteza', []))}\")\n",
        "    print(f\"• Índice opinativo: {resultado.get('indice_opiniao', 0)}\")\n",
        "    print(f\"• Pontos de exclamação: {resultado.get('num_exclamacoes', 0)}\")\n",
        "    print(f\"• Imagens: {resultado.get('num_imagens', 0)}\")\n",
        "    print(f\"• Artigos similares encontrados: {resultado.get('artigos_similares', 0)}\")\n",
        "\n",
        "    # Alertas\n",
        "    alertas = [k for k in resultado.keys() if k.startswith(\"alerta_\")]\n",
        "    if alertas:\n",
        "        print(\"-\"*50)\n",
        "        print(\"ALERTAS:\")\n",
        "        for alerta in alertas:\n",
        "            print(f\"⚠️ {resultado[alerta]}\")\n",
        "\n",
        "    # Pontuação final\n",
        "    print(\"-\"*50)\n",
        "    print(f\"PONTUAÇÃO DE CREDIBILIDADE: {resultado.get('pontuacao_credibilidade', 0)}/100\")\n",
        "    print(f\"AVALIAÇÃO: {resultado.get('avaliacao', 'N/A')}\")\n",
        "    print(f\"Tempo de análise: {tempo_execucao:.2f} segundos\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    return resultado\n",
        "\n",
        "\n",
        "# Execução principal\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Verificador de notícias avançado - use a função verificar_url(sua_url) para analisar uma notícia\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HxyNkum7zs4",
        "outputId": "2343d776-9888-44b7-8135-cf995fae11bd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting verificacao_simples.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run verificacao_simples.py\n",
        "verificar_url(\"https://www.oficinadanet.com.br/iphone/61164-iphone-16e-made-in-brazil-montagem-confirmada-foxconn-jundiai \")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8Ap4D-8ydBb",
        "outputId": "57eee67f-f3f6-4029-d17e-b8d3efcaeb8a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verificador de notícias lite - use a função verificar_url(sua_url) para analisar uma notícia\n",
            "Verificador de Notícias Lite inicializado!\n",
            "Analisando URL: https://www.oficinadanet.com.br/iphone/61164-iphone-16e-made-in-brazil-montagem-confirmada-foxconn-jundiai \n",
            "\n",
            "=== RESULTADO DA VERIFICAÇÃO ===\n",
            "URL: https://www.oficinadanet.com.br/iphone/61164-iphone-16e-made-in-brazil-montagem-confirmada-foxconn-jundiai \n",
            "Título: Confirmado! iPhone 16e agora é “Made in Brazil”\n",
            "Data de publicação: None\n",
            "Autores: Adalton Bonaventura, Adalton Bonaventura\n",
            "Número de palavras: 893\n",
            "Número de citações: 2\n",
            "Número de URLs referenciadas: 0\n",
            "\n",
            "Pontuação de credibilidade: 70/100\n",
            "Avaliação: Credibilidade moderada\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'titulo': 'Confirmado! iPhone 16e agora é “Made in Brazil”',\n",
              " 'data_publicacao': None,\n",
              " 'autores': ['Adalton Bonaventura', 'Adalton Bonaventura'],\n",
              " 'url': 'https://www.oficinadanet.com.br/iphone/61164-iphone-16e-made-in-brazil-montagem-confirmada-foxconn-jundiai ',\n",
              " 'num_palavras': 893,\n",
              " 'indice_sensacionalismo': 2,\n",
              " 'num_citacoes': 2,\n",
              " 'num_urls': 0,\n",
              " 'pontuacao_credibilidade': 70,\n",
              " 'avaliacao': 'Credibilidade moderada'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run verificacao_simples.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qb-M7L_rxstp",
        "outputId": "dab91926-50b8-4098-8364-8fd93f71c15f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verificador de notícias lite - use a função verificar_url(sua_url) para analisar uma notícia\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lxml[html_clean]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbxwifiBxjLi",
        "outputId": "ff8647e7-a308-4c87-9a42-b604615d09e8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lxml[html_clean] in /usr/local/lib/python3.11/dist-packages (5.3.2)\n",
            "Collecting lxml_html_clean (from lxml[html_clean])\n",
            "  Downloading lxml_html_clean-0.4.2-py3-none-any.whl.metadata (2.4 kB)\n",
            "Downloading lxml_html_clean-0.4.2-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: lxml_html_clean\n",
            "Successfully installed lxml_html_clean-0.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile verificacao_simples.py\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import ssl\n",
        "import urllib3\n",
        "\n",
        "# Desabilitar avisos SSL\n",
        "urllib3.disable_warnings()\n",
        "\n",
        "class VerificadorNoticiasLite:\n",
        "    def __init__(self):\n",
        "        print(\"Verificador de Notícias Lite inicializado!\")\n",
        "\n",
        "    def baixar_artigo(self, url):\n",
        "        \"\"\"Baixa e extrai o conteúdo básico de um artigo a partir da URL\"\"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers, verify=False, timeout=10)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Tenta extrair o título\n",
        "            titulo = soup.title.text.strip() if soup.title else \"Título não encontrado\"\n",
        "\n",
        "            # Tenta extrair o conteúdo do artigo (abordagem simples)\n",
        "            # Procura por elementos que geralmente contêm o conteúdo principal\n",
        "            content = \"\"\n",
        "\n",
        "            # Procura por conteúdo em elementos comuns\n",
        "            for tag in ['article', 'main', '.content', '.post', '.entry', '#content', '.article-body']:\n",
        "                if tag.startswith('.') or tag.startswith('#'):\n",
        "                    elements = soup.select(tag)\n",
        "                else:\n",
        "                    elements = soup.find_all(tag)\n",
        "\n",
        "                if elements:\n",
        "                    for element in elements:\n",
        "                        content += element.get_text(separator=' ', strip=True) + \" \"\n",
        "                    break\n",
        "\n",
        "            # Se não encontrou conteúdo nos elementos específicos, pega todos os parágrafos\n",
        "            if not content:\n",
        "                paragraphs = soup.find_all('p')\n",
        "                content = ' '.join([p.get_text(strip=True) for p in paragraphs])\n",
        "\n",
        "            # Tenta encontrar a data\n",
        "            data = None\n",
        "            # Procura por metadados comuns\n",
        "            for meta in soup.find_all('meta'):\n",
        "                if meta.get('property') in ['article:published_time', 'og:published_time', 'publication_date']:\n",
        "                    data = meta.get('content')\n",
        "                    break\n",
        "\n",
        "            # Procura por elementos que possam conter autores\n",
        "            autores = []\n",
        "            for author_tag in ['author', 'byline', '.author', '.byline', '[rel=author]']:\n",
        "                if author_tag.startswith('.') or author_tag.startswith('['):\n",
        "                    elements = soup.select(author_tag)\n",
        "                else:\n",
        "                    elements = soup.find_all(author_tag)\n",
        "\n",
        "                if elements:\n",
        "                    for element in elements:\n",
        "                        autor = element.get_text(strip=True)\n",
        "                        if autor and len(autor) < 100:  # Evitar pegar conteúdo grande por engano\n",
        "                            autores.append(autor)\n",
        "\n",
        "            return {\n",
        "                'titulo': titulo,\n",
        "                'conteudo': content,\n",
        "                'data': data,\n",
        "                'autores': autores,\n",
        "                'url': url\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao baixar artigo: {e}\")\n",
        "            return None\n",
        "\n",
        "    def analisar_credibilidade(self, artigo):\n",
        "        \"\"\"Análise básica de credibilidade baseada em características do texto.\"\"\"\n",
        "        if not artigo or not artigo.get('conteudo'):\n",
        "            return {\"erro\": \"Artigo não disponível para análise\"}\n",
        "\n",
        "        resultados = {}\n",
        "        conteudo = artigo['conteudo']\n",
        "\n",
        "        # Copiar informações básicas\n",
        "        resultados[\"titulo\"] = artigo['titulo']\n",
        "        resultados[\"data_publicacao\"] = artigo['data']\n",
        "        resultados[\"autores\"] = artigo['autores']\n",
        "        resultados[\"url\"] = artigo['url']\n",
        "\n",
        "        # Tamanho do texto (textos muito curtos podem ser suspeitos)\n",
        "        palavras = len(conteudo.split())\n",
        "        resultados[\"num_palavras\"] = palavras\n",
        "        if palavras < 100:\n",
        "            resultados[\"alerta_tamanho\"] = \"Texto muito curto, pode não ser um artigo completo\"\n",
        "\n",
        "        # Verificar presença de palavras sensacionalistas\n",
        "        palavras_sensacionalistas = [\"chocante\", \"inacreditável\", \"surpreendente\", \"sensacional\",\n",
        "                                    \"impressionante\", \"alarmante\", \"exclusivo\", \"urgente\",\n",
        "                                    \"shocking\", \"unbelievable\", \"amazing\", \"surprising\"]\n",
        "\n",
        "        count_sensacionalismo = 0\n",
        "        for palavra in palavras_sensacionalistas:\n",
        "            count_sensacionalismo += len(re.findall(r'\\b' + palavra + r'\\b', conteudo.lower()))\n",
        "\n",
        "        resultados[\"indice_sensacionalismo\"] = count_sensacionalismo\n",
        "        if count_sensacionalismo > 2:\n",
        "            resultados[\"alerta_sensacionalismo\"] = \"Possível linguagem sensacionalista detectada\"\n",
        "\n",
        "        # Verificar presença de fontes ou citações\n",
        "        citacoes = re.findall(r'\"([^\"]+)\"', conteudo)\n",
        "        resultados[\"num_citacoes\"] = len(citacoes)\n",
        "\n",
        "        if len(citacoes) == 0:\n",
        "            resultados[\"alerta_fontes\"] = \"Nenhuma citação direta encontrada\"\n",
        "\n",
        "        # Verificar URLs externos (referências)\n",
        "        urls = re.findall(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', conteudo)\n",
        "        resultados[\"num_urls\"] = len(urls)\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def verificar_noticia(self, url):\n",
        "        \"\"\"Função principal para verificar uma notícia.\"\"\"\n",
        "        print(f\"Analisando URL: {url}\")\n",
        "        artigo = self.baixar_artigo(url)\n",
        "\n",
        "        if not artigo:\n",
        "            return {\"status\": \"erro\", \"mensagem\": \"Não foi possível baixar o artigo\"}\n",
        "\n",
        "        resultados = self.analisar_credibilidade(artigo)\n",
        "\n",
        "        # Calcular pontuação básica de credibilidade (0-100)\n",
        "        pontuacao = 70  # Base inicial neutro\n",
        "\n",
        "        # Fatores que reduzem a pontuação\n",
        "        if \"alerta_tamanho\" in resultados:\n",
        "            pontuacao -= 15\n",
        "        if \"alerta_sensacionalismo\" in resultados:\n",
        "            pontuacao -= 15\n",
        "        if \"alerta_fontes\" in resultados:\n",
        "            pontuacao -= 10\n",
        "\n",
        "        # Fatores que aumentam a pontuação\n",
        "        if resultados.get(\"num_citacoes\", 0) > 3:\n",
        "            pontuacao += 10\n",
        "        if resultados.get(\"num_urls\", 0) > 2:\n",
        "            pontuacao += 5\n",
        "\n",
        "        # Ajustar pontuação final\n",
        "        pontuacao = max(0, min(100, pontuacao))\n",
        "        resultados[\"pontuacao_credibilidade\"] = pontuacao\n",
        "\n",
        "        # Avaliação qualitativa\n",
        "        if pontuacao >= 80:\n",
        "            resultados[\"avaliacao\"] = \"Alta credibilidade\"\n",
        "        elif pontuacao >= 60:\n",
        "            resultados[\"avaliacao\"] = \"Credibilidade moderada\"\n",
        "        elif pontuacao >= 40:\n",
        "            resultados[\"avaliacao\"] = \"Credibilidade questionável\"\n",
        "        else:\n",
        "            resultados[\"avaliacao\"] = \"Baixa credibilidade\"\n",
        "\n",
        "        return resultados\n",
        "\n",
        "\n",
        "# Função para demonstração\n",
        "def verificar_url(url):\n",
        "    verificador = VerificadorNoticiasLite()\n",
        "    resultado = verificador.verificar_noticia(url)\n",
        "\n",
        "    print(\"\\n=== RESULTADO DA VERIFICAÇÃO ===\")\n",
        "    print(f\"URL: {url}\")\n",
        "\n",
        "    if \"status\" in resultado and resultado[\"status\"] == \"erro\":\n",
        "        print(f\"ERRO: {resultado['mensagem']}\")\n",
        "        return\n",
        "\n",
        "    print(f\"Título: {resultado.get('titulo', 'N/A')}\")\n",
        "    print(f\"Data de publicação: {resultado.get('data_publicacao', 'N/A')}\")\n",
        "    print(f\"Autores: {', '.join(resultado.get('autores', ['N/A']))}\")\n",
        "    print(f\"Número de palavras: {resultado.get('num_palavras', 0)}\")\n",
        "    print(f\"Número de citações: {resultado.get('num_citacoes', 0)}\")\n",
        "    print(f\"Número de URLs referenciadas: {resultado.get('num_urls', 0)}\")\n",
        "\n",
        "    # Mostrar alertas se houver\n",
        "    alertas = [k for k in resultado.keys() if k.startswith(\"alerta_\")]\n",
        "    if alertas:\n",
        "        print(\"\\nALERTAS:\")\n",
        "        for alerta in alertas:\n",
        "            print(f\"- {resultado[alerta]}\")\n",
        "\n",
        "    print(f\"\\nPontuação de credibilidade: {resultado.get('pontuacao_credibilidade', 0)}/100\")\n",
        "    print(f\"Avaliação: {resultado.get('avaliacao', 'N/A')}\")\n",
        "\n",
        "    return resultado\n",
        "\n",
        "\n",
        "# Execução principal\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Verificador de notícias lite - use a função verificar_url(sua_url) para analisar uma notícia\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hjjVucXxSr_",
        "outputId": "493a284e-31af-4997-8715-44d3a5fb0556"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting verificacao_simples.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run verificacao_simples.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "JCTMdZx_v8ww",
        "outputId": "0a0b34dd-6b0d-46f3-d2e9-bc8261793a3f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "lxml.html.clean module is now a separate project lxml_html_clean.\nInstall lxml[html_clean] or lxml_html_clean directly.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/content/verificacao_simples.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnewspaper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArticle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangdetect\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdetect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/newspaper/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0m__copyright__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Copyright 2014, Lucas Ou-Yang'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m from .api import (build, build_article, fulltext, hot, languages,\n\u001b[0m\u001b[1;32m     11\u001b[0m                   popular_urls, Configuration as Config)\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marticle\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArticle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mArticleException\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/newspaper/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfeedparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marticle\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArticle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconfiguration\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfiguration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPOPULAR_URLS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRENDING_URL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/newspaper/article.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/newspaper/network.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconfiguration\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfiguration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmthreading\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mThreadPool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/newspaper/configuration.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mparsers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m from .text import (StopWords, StopWordsArabic, StopWordsChinese,\n\u001b[1;32m     17\u001b[0m                    StopWordsKorean, StopWordsHindi, StopWordsJapanese)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/newspaper/parsers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlxml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0metree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlxml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlxml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhtml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0munescape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/lxml/html/clean.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     ]\n\u001b[1;32m     17\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     raise ImportError(\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;34m\"lxml.html.clean module is now a separate project lxml_html_clean.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;34m\"Install lxml[html_clean] or lxml_html_clean directly.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: lxml.html.clean module is now a separate project lxml_html_clean.\nInstall lxml[html_clean] or lxml_html_clean directly."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install newspaper3k langdetect textblob nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpFsTW4fvvPu",
        "outputId": "d92981c6-fcfd-432d-d7ef-6068e660db29"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/981.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m665.6/981.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (4.13.4)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (11.1.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (6.0.2)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
            "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (5.3.2)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (2.32.3)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting tldextract>=2.0.1 (from newspaper3k)\n",
            "  Downloading tldextract-5.2.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (2.8.2)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (4.13.2)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (2025.1.31)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.18.0)\n",
            "Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.2.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Building wheels for collected packages: tinysegmenter, langdetect, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13540 sha256=cda521c32c7fac17bc3369e822c8fc7d678dc176a347603a770d114e0b36bb61\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/f8/cce3a9ae6d828bd346be695f7ff54612cd22b7cbd7208d68f3\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=ed7aecee885b682eedb4a0ffc065b9202a17b1cf5426780acc93e526a259c82c\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3341 sha256=ef8498fece772297d52cb97931cf4acd2d022833008c871c46777e21af3e72e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/d5/72/9cd9eccc819636436c6a6e59c22a0fb1ec167beef141f56491\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398380 sha256=e78130188f6f433060e0f7eb85955ae3c4092f9d492c02ad85cffbe821a71539\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/a1/46/8e68055c1713f9c4598774c15ad0541f26d5425ee7423b6493\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=484fdec3f58e4fafe496f015e05c08fd55852a72ab13e69efc635b10e668d696\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built tinysegmenter langdetect feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, langdetect, feedparser, cssselect, requests-file, feedfinder2, tldextract, newspaper3k\n",
            "Successfully installed cssselect-1.3.0 feedfinder2-0.0.4 feedparser-6.0.11 jieba3k-0.35.1 langdetect-1.0.9 newspaper3k-0.2.8 requests-file-2.1.0 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-5.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run verificacao_simples.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "8GYwn6g6vE12",
        "outputId": "77837514-8335-4819-f2c8-b7c2e45df8e0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'newspaper'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/content/verificacao_simples.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnewspaper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArticle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangdetect\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdetect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'newspaper'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile verificacao_simples.py\n",
        "\n",
        "from newspaper import Article\n",
        "from langdetect import detect\n",
        "from textblob import TextBlob\n",
        "import re\n",
        "import nltk\n",
        "import ssl\n",
        "\n",
        "# Desabilitar verificação SSL para download do NLTK (se necessário)\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "# Baixar recursos necessários do NLTK\n",
        "try:\n",
        "    nltk.data.find('punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "class VerificadorNoticias:\n",
        "    def __init__(self):\n",
        "        print(\"Verificador de Notícias inicializado!\")\n",
        "\n",
        "    def baixar_artigo(self, url):\n",
        "        \"\"\"Baixa e processa o conteúdo de um artigo a partir da URL.\"\"\"\n",
        "        try:\n",
        "            article = Article(url)\n",
        "            article.download()\n",
        "            article.parse()\n",
        "            article.nlp()  # Realiza processamento de linguagem natural\n",
        "            return article\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao baixar artigo: {e}\")\n",
        "            return None\n",
        "\n",
        "    def analisar_credibilidade(self, article):\n",
        "        \"\"\"Análise básica de credibilidade baseada em características do texto.\"\"\"\n",
        "        if not article:\n",
        "            return {\"erro\": \"Artigo não disponível para análise\"}\n",
        "\n",
        "        resultados = {}\n",
        "\n",
        "        # Verificar idioma\n",
        "        try:\n",
        "            idioma = detect(article.text)\n",
        "            resultados[\"idioma\"] = idioma\n",
        "        except:\n",
        "            resultados[\"idioma\"] = \"desconhecido\"\n",
        "\n",
        "        # Extrair informações básicas\n",
        "        resultados[\"titulo\"] = article.title\n",
        "        resultados[\"data_publicacao\"] = article.publish_date\n",
        "        resultados[\"autores\"] = article.authors\n",
        "\n",
        "        # Tamanho do texto (textos muito curtos podem ser suspeitos)\n",
        "        palavras = len(article.text.split())\n",
        "        resultados[\"num_palavras\"] = palavras\n",
        "        if palavras < 100:\n",
        "            resultados[\"alerta_tamanho\"] = \"Texto muito curto, pode não ser um artigo completo\"\n",
        "\n",
        "        # Análise de sentimento (textos muito polarizados podem indicar viés)\n",
        "        if resultados[\"idioma\"] == \"en\":  # TextBlob funciona melhor com inglês\n",
        "            blob = TextBlob(article.text)\n",
        "            polaridade = blob.sentiment.polarity\n",
        "            subjetividade = blob.sentiment.subjectivity\n",
        "            resultados[\"polaridade\"] = round(polaridade, 2)\n",
        "            resultados[\"subjetividade\"] = round(subjetividade, 2)\n",
        "\n",
        "            if abs(polaridade) > 0.6:\n",
        "                resultados[\"alerta_polaridade\"] = \"Texto com alta polaridade emocional\"\n",
        "            if subjetividade > 0.7:\n",
        "                resultados[\"alerta_subjetividade\"] = \"Texto com alto grau de subjetividade\"\n",
        "\n",
        "        # Verificar presença de palavras sensacionalistas\n",
        "        palavras_sensacionalistas = [\"chocante\", \"inacreditável\", \"surpreendente\", \"sensacional\",\n",
        "                                    \"impressionante\", \"alarmante\", \"exclusivo\", \"urgente\",\n",
        "                                    \"shocking\", \"unbelievable\", \"amazing\", \"surprising\"]\n",
        "\n",
        "        count_sensacionalismo = 0\n",
        "        for palavra in palavras_sensacionalistas:\n",
        "            count_sensacionalismo += len(re.findall(palavra, article.text.lower()))\n",
        "\n",
        "        resultados[\"indice_sensacionalismo\"] = count_sensacionalismo\n",
        "        if count_sensacionalismo > 2:\n",
        "            resultados[\"alerta_sensacionalismo\"] = \"Possível linguagem sensacionalista detectada\"\n",
        "\n",
        "        # Verificar presença de fontes ou citações - CORREÇÃO SIMPLIFICADA\n",
        "        # Use uma abordagem mais simples para encontrar citações - apenas aspas duplas\n",
        "        citacoes = re.findall(r'\"([^\"]+)\"', article.text)\n",
        "        resultados[\"num_citacoes\"] = len(citacoes)\n",
        "\n",
        "        if len(citacoes) == 0:\n",
        "            resultados[\"alerta_fontes\"] = \"Nenhuma citação direta encontrada\"\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def verificar_noticia(self, url):\n",
        "        \"\"\"Função principal para verificar uma notícia.\"\"\"\n",
        "        print(f\"Analisando URL: {url}\")\n",
        "        artigo = self.baixar_artigo(url)\n",
        "\n",
        "        if not artigo:\n",
        "            return {\"status\": \"erro\", \"mensagem\": \"Não foi possível baixar o artigo\"}\n",
        "\n",
        "        resultados = self.analisar_credibilidade(artigo)\n",
        "\n",
        "        # Calcular pontuação básica de credibilidade (0-100)\n",
        "        pontuacao = 70  # Base inicial neutro\n",
        "\n",
        "        # Fatores que reduzem a pontuação\n",
        "        if \"alerta_tamanho\" in resultados:\n",
        "            pontuacao -= 15\n",
        "        if \"alerta_polaridade\" in resultados:\n",
        "            pontuacao -= 10\n",
        "        if \"alerta_subjetividade\" in resultados:\n",
        "            pontuacao -= 10\n",
        "        if \"alerta_sensacionalismo\" in resultados:\n",
        "            pontuacao -= 15\n",
        "        if \"alerta_fontes\" in resultados:\n",
        "            pontuacao -= 10\n",
        "\n",
        "        # Ajustar pontuação final\n",
        "        pontuacao = max(0, min(100, pontuacao))\n",
        "        resultados[\"pontuacao_credibilidade\"] = pontuacao\n",
        "\n",
        "        # Avaliação qualitativa\n",
        "        if pontuacao >= 80:\n",
        "            resultados[\"avaliacao\"] = \"Alta credibilidade\"\n",
        "        elif pontuacao >= 60:\n",
        "            resultados[\"avaliacao\"] = \"Credibilidade moderada\"\n",
        "        elif pontuacao >= 40:\n",
        "            resultados[\"avaliacao\"] = \"Credibilidade questionável\"\n",
        "        else:\n",
        "            resultados[\"avaliacao\"] = \"Baixa credibilidade\"\n",
        "\n",
        "        return resultados\n",
        "\n",
        "\n",
        "# Função para demonstração\n",
        "def verificar_url(url):\n",
        "    verificador = VerificadorNoticias()\n",
        "    resultado = verificador.verificar_noticia(url)\n",
        "\n",
        "    print(\"\\n=== RESULTADO DA VERIFICAÇÃO ===\")\n",
        "    print(f\"URL: {url}\")\n",
        "\n",
        "    if \"status\" in resultado and resultado[\"status\"] == \"erro\":\n",
        "        print(f\"ERRO: {resultado['mensagem']}\")\n",
        "        return\n",
        "\n",
        "    print(f\"Título: {resultado.get('titulo', 'N/A')}\")\n",
        "    print(f\"Data de publicação: {resultado.get('data_publicacao', 'N/A')}\")\n",
        "    print(f\"Autores: {', '.join(resultado.get('autores', ['N/A']))}\")\n",
        "    print(f\"Idioma: {resultado.get('idioma', 'N/A')}\")\n",
        "    print(f\"Número de palavras: {resultado.get('num_palavras', 0)}\")\n",
        "\n",
        "    # Mostrar alertas se houver\n",
        "    alertas = [k for k in resultado.keys() if k.startswith(\"alerta_\")]\n",
        "    if alertas:\n",
        "        print(\"\\nALERTAS:\")\n",
        "        for alerta in alertas:\n",
        "            print(f\"- {resultado[alerta]}\")\n",
        "\n",
        "    print(f\"\\nPontuação de credibilidade: {resultado.get('pontuacao_credibilidade', 0)}/100\")\n",
        "    print(f\"Avaliação: {resultado.get('avaliacao', 'N/A')}\")\n",
        "\n",
        "    return resultado\n",
        "\n",
        "\n",
        "# Definir a função como global para uso após rodar o script\n",
        "def main():\n",
        "    print(\"Verificador de notícias - use a função verificar_url(sua_url) para analisar uma notícia\")\n",
        "\n",
        "# Execução principal\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wb7-FxxSu5V1",
        "outputId": "dec9b2e8-3c2f-47e5-c627-7990f04c7244"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing verificacao_simples.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run verificacao_simples.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "yt0RaupfuewG",
        "outputId": "ca643d62-8116-4b25-a7de-1d04d26231ad"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "File `'verificacao_simples.py'` not found.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0mfpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_lst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_finder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    715\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/utils/path.py\u001b[0m in \u001b[0;36mget_py_filename\u001b[0;34m(name, force_win32)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'File `%r` not found.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: File `'verificacao_simples.py'` not found.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-440e88cc130e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'run'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'verificacao_simples.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2416\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2417\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2418\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2419\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-52>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, parameter_s, runner, file_finder)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nt'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"^'.*'$\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m                 \u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'For Windows, use double quotes to wrap a filename: %run \"mypath\\\\myfile.py\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: File `'verificacao_simples.py'` not found."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run verificacao_simples.py"
      ],
      "metadata": {
        "id": "_3KAtM-q58cY",
        "outputId": "b291fe6f-f327-4549-fe55-c34cd764d5a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (verificacao_simples.py, line 90)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/content/verificacao_simples.py\"\u001b[0;36m, line \u001b[0;32m90\u001b[0m\n\u001b[0;31m    padrao_citacao = re.compile(r'\"[^\"]+\"|\\\\\"[^\\\\\"]+\\\\\"|\\\\'[^\\\\']+\\\\'')\u001b[0m\n\u001b[0m                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile verificacao_simples.py\n",
        "from newspaper import Article\n",
        "from langdetect import detect\n",
        "from textblob import TextBlob\n",
        "import re\n",
        "import nltk\n",
        "import ssl\n",
        "\n",
        "# Desabilitar verificação SSL para download do NLTK (se necessário)\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "# Baixar recursos necessários do NLTK\n",
        "try:\n",
        "    nltk.data.find('punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "class VerificadorNoticias:\n",
        "    def __init__(self):\n",
        "        print(\"Verificador de Notícias inicializado!\")\n",
        "\n",
        "    def baixar_artigo(self, url):\n",
        "        \"\"\"Baixa e processa o conteúdo de um artigo a partir da URL.\"\"\"\n",
        "        try:\n",
        "            article = Article(url)\n",
        "            article.download()\n",
        "            article.parse()\n",
        "            article.nlp()  # Realiza processamento de linguagem natural\n",
        "            return article\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao baixar artigo: {e}\")\n",
        "            return None\n",
        "\n",
        "    def analisar_credibilidade(self, article):\n",
        "        \"\"\"Análise básica de credibilidade baseada em características do texto.\"\"\"\n",
        "        if not article:\n",
        "            return {\"erro\": \"Artigo não disponível para análise\"}\n",
        "\n",
        "        resultados = {}\n",
        "\n",
        "        # Verificar idioma\n",
        "        try:\n",
        "            idioma = detect(article.text)\n",
        "            resultados[\"idioma\"] = idioma\n",
        "        except:\n",
        "            resultados[\"idioma\"] = \"desconhecido\"\n",
        "\n",
        "        # Extrair informações básicas\n",
        "        resultados[\"titulo\"] = article.title\n",
        "        resultados[\"data_publicacao\"] = article.publish_date\n",
        "        resultados[\"autores\"] = article.authors\n",
        "\n",
        "        # Tamanho do texto (textos muito curtos podem ser suspeitos)\n",
        "        palavras = len(article.text.split())\n",
        "        resultados[\"num_palavras\"] = palavras\n",
        "        if palavras < 100:\n",
        "            resultados[\"alerta_tamanho\"] = \"Texto muito curto, pode não ser um artigo completo\"\n",
        "\n",
        "        # Análise de sentimento (textos muito polarizados podem indicar viés)\n",
        "        if resultados[\"idioma\"] == \"en\":  # TextBlob funciona melhor com inglês\n",
        "            blob = TextBlob(article.text)\n",
        "            polaridade = blob.sentiment.polarity\n",
        "            subjetividade = blob.sentiment.subjectivity\n",
        "            resultados[\"polaridade\"] = round(polaridade, 2)\n",
        "            resultados[\"subjetividade\"] = round(subjetividade, 2)\n",
        "\n",
        "            if abs(polaridade) > 0.6:\n",
        "                resultados[\"alerta_polaridade\"] = \"Texto com alta polaridade emocional\"\n",
        "            if subjetividade > 0.7:\n",
        "                resultados[\"alerta_subjetividade\"] = \"Texto com alto grau de subjetividade\"\n",
        "\n",
        "        # Verificar presença de palavras sensacionalistas\n",
        "        palavras_sensacionalistas = [\"chocante\", \"inacreditável\", \"surpreendente\", \"sensacional\",\n",
        "                                    \"impressionante\", \"alarmante\", \"exclusivo\", \"urgente\",\n",
        "                                    \"shocking\", \"unbelievable\", \"amazing\", \"surprising\"]\n",
        "\n",
        "        count_sensacionalismo = 0\n",
        "        for palavra in palavras_sensacionalistas:\n",
        "            count_sensacionalismo += len(re.findall(palavra, article.text.lower()))\n",
        "\n",
        "        resultados[\"indice_sensacionalismo\"] = count_sensacionalismo\n",
        "        if count_sensacionalismo > 2:\n",
        "            resultados[\"alerta_sensacionalismo\"] = \"Possível linguagem sensacionalista detectada\"\n",
        "\n",
        "        # Verificar presença de fontes ou citações - CORREÇÃO DA EXPRESSÃO REGULAR\n",
        "        padrao_citacao = re.compile(r'\"[^\"]+\"|\\\\\"[^\\\\\"]+\\\\\"|\\\\'[^\\\\']+\\\\'')\n",
        "        citacoes = padrao_citacao.findall(article.text)\n",
        "        resultados[\"num_citacoes\"] = len(citacoes)\n",
        "\n",
        "        if len(citacoes) == 0:\n",
        "            resultados[\"alerta_fontes\"] = \"Nenhuma citação direta encontrada\"\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def verificar_noticia(self, url):\n",
        "        \"\"\"Função principal para verificar uma notícia.\"\"\"\n",
        "        print(f\"Analisando URL: {url}\")\n",
        "        artigo = self.baixar_artigo(url)\n",
        "\n",
        "        if not artigo:\n",
        "            return {\"status\": \"erro\", \"mensagem\": \"Não foi possível baixar o artigo\"}\n",
        "\n",
        "        resultados = self.analisar_credibilidade(artigo)\n",
        "\n",
        "        # Calcular pontuação básica de credibilidade (0-100)\n",
        "        pontuacao = 70  # Base inicial neutro\n",
        "\n",
        "        # Fatores que reduzem a pontuação\n",
        "        if \"alerta_tamanho\" in resultados:\n",
        "            pontuacao -= 15\n",
        "        if \"alerta_polaridade\" in resultados:\n",
        "            pontuacao -= 10\n",
        "        if \"alerta_subjetividade\" in resultados:\n",
        "            pontuacao -= 10\n",
        "        if \"alerta_sensacionalismo\" in resultados:\n",
        "            pontuacao -= 15\n",
        "        if \"alerta_fontes\" in resultados:\n",
        "            pontuacao -= 10\n",
        "\n",
        "        # Ajustar pontuação final\n",
        "        pontuacao = max(0, min(100, pontuacao))\n",
        "        resultados[\"pontuacao_credibilidade\"] = pontuacao\n",
        "\n",
        "        # Avaliação qualitativa\n",
        "        if pontuacao >= 80:\n",
        "            resultados[\"avaliacao\"] = \"Alta credibilidade\"\n",
        "        elif pontuacao >= 60:\n",
        "            resultados[\"avaliacao\"] = \"Credibilidade moderada\"\n",
        "        elif pontuacao >= 40:\n",
        "            resultados[\"avaliacao\"] = \"Credibilidade questionável\"\n",
        "        else:\n",
        "            resultados[\"avaliacao\"] = \"Baixa credibilidade\"\n",
        "\n",
        "        return resultados\n",
        "\n",
        "\n",
        "# Função para demonstração\n",
        "def verificar_url(url):\n",
        "    verificador = VerificadorNoticias()\n",
        "    resultado = verificador.verificar_noticia(url)\n",
        "\n",
        "    print(\"\\n=== RESULTADO DA VERIFICAÇÃO ===\")\n",
        "    print(f\"URL: {url}\")\n",
        "\n",
        "    if \"status\" in resultado and resultado[\"status\"] == \"erro\":\n",
        "        print(f\"ERRO: {resultado['mensagem']}\")\n",
        "        return\n",
        "\n",
        "    print(f\"Título: {resultado.get('titulo', 'N/A')}\")\n",
        "    print(f\"Data de publicação: {resultado.get('data_publicacao', 'N/A')}\")\n",
        "    print(f\"Autores: {', '.join(resultado.get('autores', ['N/A']))}\")\n",
        "    print(f\"Idioma: {resultado.get('idioma', 'N/A')}\")\n",
        "    print(f\"Número de palavras: {resultado.get('num_palavras', 0)}\")\n",
        "\n",
        "    # Mostrar alertas se houver\n",
        "    alertas = [k for k in resultado.keys() if k.startswith(\"alerta_\")]\n",
        "    if alertas:\n",
        "        print(\"\\nALERTAS:\")\n",
        "        for alerta in alertas:\n",
        "            print(f\"- {resultado[alerta]}\")\n",
        "\n",
        "    print(f\"\\nPontuação de credibilidade: {resultado.get('pontuacao_credibilidade', 0)}/100\")\n",
        "    print(f\"Avaliação: {resultado.get('avaliacao', 'N/A')}\")\n",
        "\n",
        "    return resultado\n",
        "\n",
        "\n",
        "# Para tornar a função verificar_url disponível quando o script é executado\n",
        "# Cria uma instância global para permitir uso direto após executar o script\n",
        "verificador_global = VerificadorNoticias()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Verificador de notícias - use a função verificar_url(sua_url) para analisar uma notícia\")\n",
        "    # Exemplo:\n",
        "    # verificar_url(\"https://www.exemplo.com/noticia\")\n",
        "else:\n",
        "    # Isso garante que a função esteja disponível mesmo quando importada\n",
        "    print(\"Módulo de verificação de notícias carregado.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irehuUI35bgH",
        "outputId": "340c48f8-eb09-4bd0-f1d7-8f7331a3263c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting verificacao_simples.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run verificacao_simples.py\n",
        "verificar_url(\"https://www.oficinadanet.com.br/iphone/61164-iphone-16e-made-in-brazil-montagem-confirmada-foxconn-jundiai\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "JgUq5w834XTt",
        "outputId": "a7443f6a-f904-466e-b3b2-988f2539c61b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 91) (verificacao_simples.py, line 91)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/content/verificacao_simples.py\"\u001b[0;36m, line \u001b[0;32m91\u001b[0m\n\u001b[0;31m    padrao_citacao = re.compile(r'\"[^\"]+\"|'[^']+'|\"[^\"]+\"')\u001b[0m\n\u001b[0m                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 91)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'verificar_url' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-92a8f73a1c19>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'run'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'verificacao_simples.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mverificar_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://www.oficinadanet.com.br/iphone/61164-iphone-16e-made-in-brazil-montagem-confirmada-foxconn-jundiai\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'verificar_url' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile verificacao_simples.py\n",
        "\n",
        "from newspaper import Article\n",
        "from langdetect import detect\n",
        "from textblob import TextBlob\n",
        "import re\n",
        "import nltk\n",
        "import ssl\n",
        "\n",
        "# Desabilitar verificação SSL para download do NLTK (se necessário)\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "# Baixar recursos necessários do NLTK\n",
        "try:\n",
        "    nltk.data.find('punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "class VerificadorNoticias:\n",
        "    def __init__(self):\n",
        "        print(\"Verificador de Notícias inicializado!\")\n",
        "\n",
        "    def baixar_artigo(self, url):\n",
        "        \"\"\"Baixa e processa o conteúdo de um artigo a partir da URL.\"\"\"\n",
        "        try:\n",
        "            article = Article(url)\n",
        "            article.download()\n",
        "            article.parse()\n",
        "            article.nlp()  # Realiza processamento de linguagem natural\n",
        "            return article\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao baixar artigo: {e}\")\n",
        "            return None\n",
        "\n",
        "    def analisar_credibilidade(self, article):\n",
        "        \"\"\"Análise básica de credibilidade baseada em características do texto.\"\"\"\n",
        "        if not article:\n",
        "            return {\"erro\": \"Artigo não disponível para análise\"}\n",
        "\n",
        "        resultados = {}\n",
        "\n",
        "        # Verificar idioma\n",
        "        try:\n",
        "            idioma = detect(article.text)\n",
        "            resultados[\"idioma\"] = idioma\n",
        "        except:\n",
        "            resultados[\"idioma\"] = \"desconhecido\"\n",
        "\n",
        "        # Extrair informações básicas\n",
        "        resultados[\"titulo\"] = article.title\n",
        "        resultados[\"data_publicacao\"] = article.publish_date\n",
        "        resultados[\"autores\"] = article.authors\n",
        "\n",
        "        # Tamanho do texto (textos muito curtos podem ser suspeitos)\n",
        "        palavras = len(article.text.split())\n",
        "        resultados[\"num_palavras\"] = palavras\n",
        "        if palavras < 100:\n",
        "            resultados[\"alerta_tamanho\"] = \"Texto muito curto, pode não ser um artigo completo\"\n",
        "\n",
        "        # Análise de sentimento (textos muito polarizados podem indicar viés)\n",
        "        if resultados[\"idioma\"] == \"en\":  # TextBlob funciona melhor com inglês\n",
        "            blob = TextBlob(article.text)\n",
        "            polaridade = blob.sentiment.polarity\n",
        "            subjetividade = blob.sentiment.subjectivity\n",
        "            resultados[\"polaridade\"] = round(polaridade, 2)\n",
        "            resultados[\"subjetividade\"] = round(subjetividade, 2)\n",
        "\n",
        "            if abs(polaridade) > 0.6:\n",
        "                resultados[\"alerta_polaridade\"] = \"Texto com alta polaridade emocional\"\n",
        "            if subjetividade > 0.7:\n",
        "                resultados[\"alerta_subjetividade\"] = \"Texto com alto grau de subjetividade\"\n",
        "\n",
        "        # Verificar presença de palavras sensacionalistas\n",
        "        palavras_sensacionalistas = [\"chocante\", \"inacreditável\", \"surpreendente\", \"sensacional\",\n",
        "                                    \"impressionante\", \"alarmante\", \"exclusivo\", \"urgente\",\n",
        "                                    \"shocking\", \"unbelievable\", \"amazing\", \"surprising\"]\n",
        "\n",
        "        count_sensacionalismo = 0\n",
        "        for palavra in palavras_sensacionalistas:\n",
        "            count_sensacionalismo += len(re.findall(palavra, article.text.lower()))\n",
        "\n",
        "        resultados[\"indice_sensacionalismo\"] = count_sensacionalismo\n",
        "        if count_sensacionalismo > 2:\n",
        "            resultados[\"alerta_sensacionalismo\"] = \"Possível linguagem sensacionalista detectada\"\n",
        "\n",
        "        # Verificar presença de fontes ou citações\n",
        "        padrao_citacao = re.compile(r'\"[^\"]+\"|'[^']+'|\"[^\"]+\"')\n",
        "        citacoes = padrao_citacao.findall(article.text)\n",
        "        resultados[\"num_citacoes\"] = len(citacoes)\n",
        "\n",
        "        if len(citacoes) == 0:\n",
        "            resultados[\"alerta_fontes\"] = \"Nenhuma citação direta encontrada\"\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def verificar_noticia(self, url):\n",
        "        \"\"\"Função principal para verificar uma notícia.\"\"\"\n",
        "        print(f\"Analisando URL: {url}\")\n",
        "        artigo = self.baixar_artigo(url)\n",
        "\n",
        "        if not artigo:\n",
        "            return {\"status\": \"erro\", \"mensagem\": \"Não foi possível baixar o artigo\"}\n",
        "\n",
        "        resultados = self.analisar_credibilidade(artigo)\n",
        "\n",
        "        # Calcular pontuação básica de credibilidade (0-100)\n",
        "        pontuacao = 70  # Base inicial neutro\n",
        "\n",
        "        # Fatores que reduzem a pontuação\n",
        "        if \"alerta_tamanho\" in resultados:\n",
        "            pontuacao -= 15\n",
        "        if \"alerta_polaridade\" in resultados:\n",
        "            pontuacao -= 10\n",
        "        if \"alerta_subjetividade\" in resultados:\n",
        "            pontuacao -= 10\n",
        "        if \"alerta_sensacionalismo\" in resultados:\n",
        "            pontuacao -= 15\n",
        "        if \"alerta_fontes\" in resultados:\n",
        "            pontuacao -= 10\n",
        "\n",
        "        # Ajustar pontuação final\n",
        "        pontuacao = max(0, min(100, pontuacao))\n",
        "        resultados[\"pontuacao_credibilidade\"] = pontuacao\n",
        "\n",
        "        # Avaliação qualitativa\n",
        "        if pontuacao >= 80:\n",
        "            resultados[\"avaliacao\"] = \"Alta credibilidade\"\n",
        "        elif pontuacao >= 60:\n",
        "            resultados[\"avaliacao\"] = \"Credibilidade moderada\"\n",
        "        elif pontuacao >= 40:\n",
        "            resultados[\"avaliacao\"] = \"Credibilidade questionável\"\n",
        "        else:\n",
        "            resultados[\"avaliacao\"] = \"Baixa credibilidade\"\n",
        "\n",
        "        return resultados\n",
        "\n",
        "\n",
        "# Função para demonstração\n",
        "def verificar_url(url):\n",
        "    verificador = VerificadorNoticias()\n",
        "    resultado = verificador.verificar_noticia(url)\n",
        "\n",
        "    print(\"\\n=== RESULTADO DA VERIFICAÇÃO ===\")\n",
        "    print(f\"URL: {url}\")\n",
        "\n",
        "    if \"status\" in resultado and resultado[\"status\"] == \"erro\":\n",
        "        print(f\"ERRO: {resultado['mensagem']}\")\n",
        "        return\n",
        "\n",
        "    print(f\"Título: {resultado.get('titulo', 'N/A')}\")\n",
        "    print(f\"Data de publicação: {resultado.get('data_publicacao', 'N/A')}\")\n",
        "    print(f\"Autores: {', '.join(resultado.get('autores', ['N/A']))}\")\n",
        "    print(f\"Idioma: {resultado.get('idioma', 'N/A')}\")\n",
        "    print(f\"Número de palavras: {resultado.get('num_palavras', 0)}\")\n",
        "\n",
        "    # Mostrar alertas se houver\n",
        "    alertas = [k for k in resultado.keys() if k.startswith(\"alerta_\")]\n",
        "    if alertas:\n",
        "        print(\"\\nALERTAS:\")\n",
        "        for alerta in alertas:\n",
        "            print(f\"- {resultado[alerta]}\")\n",
        "\n",
        "    print(f\"\\nPontuação de credibilidade: {resultado.get('pontuacao_credibilidade', 0)}/100\")\n",
        "    print(f\"Avaliação: {resultado.get('avaliacao', 'N/A')}\")\n",
        "\n",
        "    return resultado\n",
        "\n",
        "\n",
        "# Para testar, descomente e adicione uma URL\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Verificador de notícias - use a função verificar_url(sua_url) para analisar uma notícia\")\n",
        "    # Exemplo:\n",
        "    # verificar_url(\"https://www.exemplo.com/noticia\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvptyGaZ4E16",
        "outputId": "ff3326ab-fa01-4e62-a976-6ce4119051dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting verificacao_simples.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COUXDa0P3XfO",
        "outputId": "bf081d58-4160-4061-c6e8-cf6dab7d639a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (4.13.4)\n",
            "Collecting newspaper3k (from -r requirements.txt (line 3))\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (1.6.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (4.51.3)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (3.8.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (2.2.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (1.1.0)\n",
            "Collecting langdetect (from -r requirements.txt (line 10))\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (0.19.0)\n",
            "Collecting faiss-cpu (from -r requirements.txt (line 12))\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->-r requirements.txt (line 1)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->-r requirements.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->-r requirements.txt (line 1)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->-r requirements.txt (line 1)) (2025.1.31)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->-r requirements.txt (line 2)) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->-r requirements.txt (line 2)) (4.13.2)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k->-r requirements.txt (line 3)) (11.1.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.11/dist-packages (from newspaper3k->-r requirements.txt (line 3)) (6.0.2)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k->-r requirements.txt (line 3))\n",
            "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k->-r requirements.txt (line 3)) (5.3.2)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k->-r requirements.txt (line 3))\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting tldextract>=2.0.1 (from newspaper3k->-r requirements.txt (line 3))\n",
            "  Downloading tldextract-5.2.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting feedfinder2>=0.0.4 (from newspaper3k->-r requirements.txt (line 3))\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k->-r requirements.txt (line 3))\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from newspaper3k->-r requirements.txt (line 3)) (2.8.2)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k->-r requirements.txt (line 3))\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->-r requirements.txt (line 4)) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->-r requirements.txt (line 4)) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->-r requirements.txt (line 4)) (2024.11.6)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (1.14.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 6)) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 6)) (0.30.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 6)) (24.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 6)) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 6)) (0.5.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 7)) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 7)) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 7)) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 7)) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 7)) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 7)) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 7)) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 7)) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 7)) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 7)) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 7)) (0.15.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 7)) (2.11.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 7)) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 7)) (75.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 7)) (3.5.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 8)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 8)) (2025.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect->-r requirements.txt (line 10)) (1.17.0)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k->-r requirements.txt (line 3))\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers->-r requirements.txt (line 6)) (2025.3.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy->-r requirements.txt (line 7)) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 7)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 7)) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 7)) (0.4.0)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy->-r requirements.txt (line 7)) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy->-r requirements.txt (line 7)) (0.1.5)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k->-r requirements.txt (line 3))\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 7)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 7)) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy->-r requirements.txt (line 7)) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy->-r requirements.txt (line 7)) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy->-r requirements.txt (line 7)) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->-r requirements.txt (line 7)) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 7)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 7)) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy->-r requirements.txt (line 7)) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 7)) (0.1.2)\n",
            "Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.2.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Building wheels for collected packages: tinysegmenter, langdetect, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13540 sha256=89bd43714792922f535ccdab78f83501511e1ba60634deca710b0ce2b97986ef\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/f8/cce3a9ae6d828bd346be695f7ff54612cd22b7cbd7208d68f3\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=2516ff85c863ebf92e51cc1bac8a37d00c0c8e8c9b689795d34d4fbe8901266f\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3341 sha256=512e2e0f8075295c3cccb92bcce14d5eba8a98cab22a4ddb059afcc930775362\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/d5/72/9cd9eccc819636436c6a6e59c22a0fb1ec167beef141f56491\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398380 sha256=142ca14bbb4f4dc8521242b79113764c86155e3c6e676a4932b7649a590f1eb2\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/a1/46/8e68055c1713f9c4598774c15ad0541f26d5425ee7423b6493\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=5881692eac080bac73e20d558a4a44eba4286169c1471bfbf5282343f441466a\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built tinysegmenter langdetect feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, langdetect, feedparser, faiss-cpu, cssselect, requests-file, feedfinder2, tldextract, newspaper3k\n",
            "Successfully installed cssselect-1.3.0 faiss-cpu-1.10.0 feedfinder2-0.0.4 feedparser-6.0.11 jieba3k-0.35.1 langdetect-1.0.9 newspaper3k-0.2.8 requests-file-2.1.0 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-5.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "requests                # Para requisições web e acesso a APIs\n",
        "beautifulsoup4          # Para extração de conteúdo de páginas web\n",
        "newspaper3k             # Biblioteca para extração de artigos de notícias\n",
        "nltk                    # Processamento de linguagem natural\n",
        "scikit-learn            # Aprendizado de máquina para classificação\n",
        "transformers            # Modelos de linguagem transformers (BERT, etc.)\n",
        "spacy                   # Processamento avançado de linguagem natural\n",
        "pandas                  # Análise de dados\n",
        "python-dotenv           # Gerenciamento de chaves de API\n",
        "langdetect              # Detecção de idioma\n",
        "textblob                # Análise de sentimento\n",
        "faiss-cpu               # Busca por similaridade vetorial (opcional)\n",
        "tqdm                    # Barras de progresso para processos longos\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mudzCGyC2Odq",
        "outputId": "20efa48d-6816-419f-b04e-c6323e563fe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYHqit4l1NUr",
        "outputId": "b621c8df-91f5-4fd6-bfb8-e24c80fe87b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (3.9.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (2.18.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (4.51.3)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (3.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (2.32.3)\n",
            "Collecting python-dotenv (from -r requirements.txt (line 10))\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 3)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 3)) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 4)) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 4)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 4)) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->-r requirements.txt (line 5)) (8.1.8)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->-r requirements.txt (line 5)) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->-r requirements.txt (line 5)) (4.67.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 6)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 6)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 6)) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 6)) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 6)) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 6)) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 6)) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 6)) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 6)) (5.29.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 6)) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 6)) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 6)) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 6)) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 6)) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 6)) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 6)) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 6)) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 6)) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 6)) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 6)) (0.37.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 7)) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 7)) (0.30.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 7)) (6.0.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 7)) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 7)) (0.5.3)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask->-r requirements.txt (line 8)) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask->-r requirements.txt (line 8)) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask->-r requirements.txt (line 8)) (2.2.0)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask->-r requirements.txt (line 8)) (1.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->-r requirements.txt (line 9)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->-r requirements.txt (line 9)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->-r requirements.txt (line 9)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->-r requirements.txt (line 9)) (2025.1.31)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow->-r requirements.txt (line 6)) (0.45.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers->-r requirements.txt (line 7)) (2025.3.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->flask->-r requirements.txt (line 8)) (3.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->-r requirements.txt (line 6)) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->-r requirements.txt (line 6)) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->-r requirements.txt (line 6)) (0.15.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->-r requirements.txt (line 6)) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->-r requirements.txt (line 6)) (0.7.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow->-r requirements.txt (line 6)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow->-r requirements.txt (line 6)) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow->-r requirements.txt (line 6)) (0.1.2)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!dir requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M78RBSap02Yc",
        "outputId": "69e7c567-909b-4382-f579-fe39c02fa8c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "# Dependências básicas para chatbot\n",
        "numpy\n",
        "pandas\n",
        "scikit-learn\n",
        "nltk\n",
        "tensorflow\n",
        "transformers\n",
        "flask # Para API web se necessário\n",
        "requests\n",
        "python-dotenv # Para variáveis de ambiente"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAnFk6OXxpaP",
        "outputId": "e03fc00a-21f5-44b4-98cb-6981b898aca8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxfQ1ZWgxJyh",
        "outputId": "a075fe6d-5664-4b1f-92f0-869e9b5f4d9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run verificacao_simples.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_qeTsHnwkTc",
        "outputId": "f5533fb4-f1db-4e9f-9f64-5318e0e52c9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "verificacao_simples.py \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile verificacao_simples.py\n",
        "print(\"verificacao_simples.py \")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYEFXhSBwRyA",
        "outputId": "4c032507-a7fb-43e5-cbaa-c77fd82777f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing verificacao_simples.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd C:/Users/Geraldo/Documents/ChatBot_NewsgamesIA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNvxiULGveC_",
        "outputId": "d1a7a87d-7683-426f-8f71-b46a0a6f74f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'C:/Users/Geraldo/Documents/ChatBot_NewsgamesIA'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir(\"C:/Users/Geraldo/Documents/ChatBot_NewsgamesIA\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "TCV9T2dqvTKk",
        "outputId": "76c3ac51-5003-4362-c2b0-8e9fc909f3e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'C:/Users/Geraldo/Documents/ChatBot_NewsgamesIA'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-c5c3fa9c46ab>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"C:/Users/Geraldo/Documents/ChatBot_NewsgamesIA\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/Geraldo/Documents/ChatBot_NewsgamesIA'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.path.exists(\"C:/Users/Geraldo/Documents/ChatBot_NewsgamesIA/verificacao_simples.py\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5vq3gVcvG80",
        "outputId": "0776ac9f-9ca8-4468-f1ec-c45f871b1e3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run \"C:/Users/Geraldo/Documents/ChatBot_NewsgamesIA/verificacao_simples.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "xOQABTeFun__",
        "outputId": "ea5e89d5-c15d-48b4-b14d-60f1f209250b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "File `'C:/Users/Geraldo/Documents/ChatBot_NewsgamesIA/verificacao_simples.py'` not found.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0mfpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_lst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_finder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    715\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/utils/path.py\u001b[0m in \u001b[0;36mget_py_filename\u001b[0;34m(name, force_win32)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'File `%r` not found.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: File `'C:/Users/Geraldo/Documents/ChatBot_NewsgamesIA/verificacao_simples.py'` not found.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-e829ac6f2c19>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'run'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\"C:/Users/Geraldo/Documents/ChatBot_NewsgamesIA/verificacao_simples.py\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2416\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2417\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2418\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2419\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-52>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, parameter_s, runner, file_finder)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nt'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"^'.*'$\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m                 \u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'For Windows, use double quotes to wrap a filename: %run \"mypath\\\\myfile.py\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: File `'C:/Users/Geraldo/Documents/ChatBot_NewsgamesIA/verificacao_simples.py'` not found."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGiHr_E3t2fo",
        "outputId": "a4735d58-98ba-4bb3-bbbc-76076e79a9a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run verificacao_simples.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "xu4ZASCPtAZc",
        "outputId": "5b2819b0-984b-4295-cfcb-5c5535245652"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "File `'verificacao_simples.py'` not found.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0mfpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_lst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_finder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    715\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/utils/path.py\u001b[0m in \u001b[0;36mget_py_filename\u001b[0;34m(name, force_win32)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'File `%r` not found.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: File `'verificacao_simples.py'` not found.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-440e88cc130e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'run'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'verificacao_simples.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2416\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2417\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2418\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2419\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-52>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, parameter_s, runner, file_finder)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nt'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"^'.*'$\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m                 \u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'For Windows, use double quotes to wrap a filename: %run \"mypath\\\\myfile.py\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: File `'verificacao_simples.py'` not found."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wMeSFFNQ__g",
        "outputId": "8c24e99a-cbcd-48e0-f220-d6bd6679ad29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl.metadata (27 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==9.1.0.70 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.21.5 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl (848.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m848.7/848.7 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "Successfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.21.5 nvidia-nvtx-cu11-11.8.86 torch-2.6.0+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAn4FxHhQo71",
        "outputId": "f2d508ee-51f8-440d-fbc4-e98b271b9872"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu128\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\" (from torch) (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\"\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emhzvIc6gtOw",
        "outputId": "012e1a6f-df50-4b9e-91c9-6400b8de197e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu128\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip uninstall torch torchvision torchaudio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOaiOoQrgnUO",
        "outputId": "657501ff-55b7-4342-de4f-021f7e3ca71f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping torch as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torchvision as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torchaudio as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "TI3uvSq-gKQR",
        "outputId": "4d29b4c0-ee59-4c26-f54d-9faf197cdde7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nvcc' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-7fcf4273bd03>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnvcc\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'nvcc' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "5q8XhpZpelu5",
        "outputId": "c7ddcbe5-1371-4d9e-e9e9-90d619750007"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nvcc' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-7fbc0882d609>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnvcc\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'nvcc' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"No GPU detected by PyTorch.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3XtRnS3eBit",
        "outputId": "b0161f6b-6b00-4068-ae62-8ed71602b87c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version: 2.6.0+cu118\n",
            "CUDA Available: False\n",
            "No GPU detected by PyTorch.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip uninstall torch torchvision torchaudio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B88oSa8UdXn3",
        "outputId": "f8d9f20e-0c8c-4fd5-a1e2-9fb4eb5dd6bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.6.0+cu118\n",
            "Uninstalling torch-2.6.0+cu118:\n",
            "  Would remove:\n",
            "    /usr/local/bin/torchfrtrace\n",
            "    /usr/local/bin/torchrun\n",
            "    /usr/local/lib/python3.11/dist-packages/functorch/*\n",
            "    /usr/local/lib/python3.11/dist-packages/torch-2.6.0+cu118.dist-info/*\n",
            "    /usr/local/lib/python3.11/dist-packages/torch/*\n",
            "    /usr/local/lib/python3.11/dist-packages/torchgen/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled torch-2.6.0+cu118\n",
            "Found existing installation: torchvision 0.21.0+cu118\n",
            "Uninstalling torchvision-0.21.0+cu118:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.11/dist-packages/torchvision-0.21.0+cu118.dist-info/*\n",
            "    /usr/local/lib/python3.11/dist-packages/torchvision.libs/libcudart.60cfec8e.so.11.0\n",
            "    /usr/local/lib/python3.11/dist-packages/torchvision.libs/libjpeg.1c1c4b09.so.8\n",
            "    /usr/local/lib/python3.11/dist-packages/torchvision.libs/libnvjpeg.70530407.so.11\n",
            "    /usr/local/lib/python3.11/dist-packages/torchvision.libs/libpng16.0364a1db.so.16\n",
            "    /usr/local/lib/python3.11/dist-packages/torchvision.libs/libsharpyuv.5c41a003.so.0\n",
            "    /usr/local/lib/python3.11/dist-packages/torchvision.libs/libwebp.54a0d02a.so.7\n",
            "    /usr/local/lib/python3.11/dist-packages/torchvision.libs/libz.d13a2644.so.1\n",
            "    /usr/local/lib/python3.11/dist-packages/torchvision/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled torchvision-0.21.0+cu118\n",
            "Found existing installation: torchaudio 2.6.0+cu124\n",
            "Uninstalling torchaudio-2.6.0+cu124:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.11/dist-packages/torchaudio-2.6.0+cu124.dist-info/*\n",
            "    /usr/local/lib/python3.11/dist-packages/torchaudio/*\n",
            "    /usr/local/lib/python3.11/dist-packages/torio/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled torchaudio-2.6.0+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "kkhZodoSdMFi",
        "outputId": "acf88e64-9ce6-45b7-ceef-03edacd020ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nvcc' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-7fcf4273bd03>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnvcc\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'nvcc' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "print(f\"PyTorch CUDA Version: {torch.version.cuda}\")\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2e6A8iecLYX",
        "outputId": "01e8e6f0-2497-4ac3-ecfc-8adc8227734b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch CUDA Version: 11.8\n",
            "CUDA Available: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "print(f\"PyTorch CUDA Version: {torch.version.cuda}\")\n",
        "print(f\"torchvision CUDA Version: {torchvision._C._CUDA_VERSION}\")\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "pbh2rn2pbq9T",
        "outputId": "a60a5e1f-413c-4012-d519-a187999c9fcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch CUDA Version: 11.8\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'torchvision' has no attribute '_C'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f7f742104b78>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"PyTorch CUDA Version: {torch.version.cuda}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"torchvision CUDA Version: {torchvision._C._CUDA_VERSION}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"CUDA Available: {torch.cuda.is_available()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'torchvision' has no attribute '_C'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        },
        "id": "3gzsqLiXbRhl",
        "outputId": "f47a5517-241a-4eb3-e861-78c2469c50f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl.metadata (27 kB)\n",
            "Collecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.21.0%2Bcu118-cp311-cp311-linux_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.86)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Using cached https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl (848.7 MB)\n",
            "Downloading https://download.pytorch.org/whl/cu118/torchvision-0.21.0%2Bcu118-cp311-cp311-linux_x86_64.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch, torchvision\n",
            "Successfully installed torch-2.6.0+cu118 torchvision-0.21.0+cu118\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen"
                ]
              },
              "id": "31a4845c329e427bbd36e7e7b892f410"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip uninstall torch torchvision -y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UOmWkHsbKvr",
        "outputId": "eec19220-6217-449a-feda-e11f5ac93956"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.6.0+cu118\n",
            "Uninstalling torch-2.6.0+cu118:\n",
            "  Successfully uninstalled torch-2.6.0+cu118\n",
            "Found existing installation: torchvision 0.21.0+cu124\n",
            "Uninstalling torchvision-0.21.0+cu124:\n",
            "  Successfully uninstalled torchvision-0.21.0+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "print(f\"PyTorch CUDA Available: {torch.cuda.is_available()}\")\n",
        "print(f\"PyTorch CUDA Version: {torch.version.cuda}\")\n",
        "print(f\"torchvision Version: {torchvision.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "LN_zgZs-a8bp",
        "outputId": "2db4b049-d7bd-4197-ccc3-1fbf414162e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Detected that PyTorch and torchvision were compiled with different CUDA major versions. PyTorch has CUDA Version=11.8 and torchvision has CUDA Version=12.4. Please reinstall the torchvision that matches your PyTorch install.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-970df89640d6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"PyTorch CUDA Available: {torch.cuda.is_available()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"PyTorch CUDA Version: {torch.version.cuda}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Don't re-order these, we need to load the _C extension (done when importing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# .extensions) before entering _meta_registrations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mextension\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HAS_OPS\u001b[0m  \u001b[0;31m# usort:skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# usort:skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/extension.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \u001b[0m_check_cuda_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/extension.py\u001b[0m in \u001b[0;36m_check_cuda_version\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mt_minor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_version\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mt_major\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtv_major\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m     79\u001b[0m                 \u001b[0;34m\"Detected that PyTorch and torchvision were compiled with different CUDA major versions. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;34mf\"PyTorch has CUDA Version={t_major}.{t_minor} and torchvision has \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Detected that PyTorch and torchvision were compiled with different CUDA major versions. PyTorch has CUDA Version=11.8 and torchvision has CUDA Version=12.4. Please reinstall the torchvision that matches your PyTorch install."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7RdQCeVay6D",
        "outputId": "83f80e42-30c4-425f-c29a-208c57efd50f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.86)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run verificacao_simples.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        },
        "id": "DWcT2KjoauJX",
        "outputId": "3cb82258-ac40-47d7-fa33-7508daf8d137"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Failed to import transformers.trainer because of the following error (look up to see its traceback):\nFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\nFailed to import transformers.modeling_utils because of the following error (look up to see its traceback):\nDetected that PyTorch and torchvision were compiled with different CUDA major versions. PyTorch has CUDA Version=11.8 and torchvision has CUDA Version=12.4. Please reinstall the torchvision that matches your PyTorch install.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1966\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1967\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m )\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLOSS_MAPPING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m from .pytorch_utils import (  # noqa: F401\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/loss/loss_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloss_deformable_detr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeformableDetrForObjectDetectionLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeformableDetrForSegmentationLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloss_for_object_detection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mForObjectDetectionLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mForSegmentationLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/loss/loss_deformable_detr.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_transforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcenter_to_corners_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_scipy_available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/image_transforms.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m from .image_utils import (\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mChannelDimension\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/image_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_torchvision_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtorchvision_io\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterpolationMode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# .extensions) before entering _meta_registrations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mextension\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HAS_OPS\u001b[0m  \u001b[0;31m# usort:skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# usort:skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/extension.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \u001b[0m_check_cuda_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/extension.py\u001b[0m in \u001b[0;36m_check_cuda_version\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mt_major\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtv_major\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m     79\u001b[0m                 \u001b[0;34m\"Detected that PyTorch and torchvision were compiled with different CUDA major versions. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Detected that PyTorch and torchvision were compiled with different CUDA major versions. PyTorch has CUDA Version=11.8 and torchvision has CUDA Version=12.4. Please reinstall the torchvision that matches your PyTorch install.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1966\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1967\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/integrations/integration_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreTrainedModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTFPreTrainedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1954\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1955\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1956\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1969\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1970\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\nDetected that PyTorch and torchvision were compiled with different CUDA major versions. PyTorch has CUDA Version=11.8 and torchvision has CUDA Version=12.4. Please reinstall the torchvision that matches your PyTorch install.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1966\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1967\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# isort: off\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m from .integrations import (\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mget_reporting_integration_callbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1954\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1955\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1956\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1969\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1970\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\nFailed to import transformers.modeling_utils because of the following error (look up to see its traceback):\nDetected that PyTorch and torchvision were compiled with different CUDA major versions. PyTorch has CUDA Version=11.8 and torchvision has CUDA Version=12.4. Please reinstall the torchvision that matches your PyTorch install.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/content/verificacao_simples.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataCollatorWithPadding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 1. Carregar Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1953\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlaceholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1955\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1956\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1957\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1967\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1969\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1970\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1971\u001b[0m                 \u001b[0;34mf\" traceback):\\n{e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.trainer because of the following error (look up to see its traceback):\nFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\nFailed to import transformers.modeling_utils because of the following error (look up to see its traceback):\nDetected that PyTorch and torchvision were compiled with different CUDA major versions. PyTorch has CUDA Version=11.8 and torchvision has CUDA Version=12.4. Please reinstall the torchvision that matches your PyTorch install."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchvision --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNjcuXIjaR5r",
        "outputId": "62b3999a-5b5d-415d-c9ac-078af0f3e35b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu118)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.8.86)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run verificacao_simples.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        },
        "id": "AYazkVbkZflz",
        "outputId": "0d6b246f-8eaf-4f56-dbad-29ca38743cab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Failed to import transformers.trainer because of the following error (look up to see its traceback):\nFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\nFailed to import transformers.modeling_utils because of the following error (look up to see its traceback):\nDetected that PyTorch and torchvision were compiled with different CUDA major versions. PyTorch has CUDA Version=11.8 and torchvision has CUDA Version=12.4. Please reinstall the torchvision that matches your PyTorch install.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1966\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1967\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m )\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLOSS_MAPPING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m from .pytorch_utils import (  # noqa: F401\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/loss/loss_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloss_deformable_detr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeformableDetrForObjectDetectionLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeformableDetrForSegmentationLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloss_for_object_detection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mForObjectDetectionLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mForSegmentationLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/loss/loss_deformable_detr.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_transforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcenter_to_corners_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_scipy_available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/image_transforms.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m from .image_utils import (\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mChannelDimension\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/image_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_torchvision_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtorchvision_io\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterpolationMode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# .extensions) before entering _meta_registrations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mextension\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HAS_OPS\u001b[0m  \u001b[0;31m# usort:skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# usort:skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/extension.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \u001b[0m_check_cuda_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/extension.py\u001b[0m in \u001b[0;36m_check_cuda_version\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mt_major\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtv_major\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m     79\u001b[0m                 \u001b[0;34m\"Detected that PyTorch and torchvision were compiled with different CUDA major versions. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Detected that PyTorch and torchvision were compiled with different CUDA major versions. PyTorch has CUDA Version=11.8 and torchvision has CUDA Version=12.4. Please reinstall the torchvision that matches your PyTorch install.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1966\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1967\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/integrations/integration_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreTrainedModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTFPreTrainedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1954\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1955\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1956\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1969\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1970\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\nDetected that PyTorch and torchvision were compiled with different CUDA major versions. PyTorch has CUDA Version=11.8 and torchvision has CUDA Version=12.4. Please reinstall the torchvision that matches your PyTorch install.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1966\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1967\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# isort: off\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m from .integrations import (\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mget_reporting_integration_callbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1954\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1955\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1956\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1969\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1970\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\nFailed to import transformers.modeling_utils because of the following error (look up to see its traceback):\nDetected that PyTorch and torchvision were compiled with different CUDA major versions. PyTorch has CUDA Version=11.8 and torchvision has CUDA Version=12.4. Please reinstall the torchvision that matches your PyTorch install.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/content/verificacao_simples.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataCollatorWithPadding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 1. Carregar Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1953\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlaceholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1955\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1956\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1957\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1967\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1969\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1970\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1971\u001b[0m                 \u001b[0;34mf\" traceback):\\n{e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.trainer because of the following error (look up to see its traceback):\nFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\nFailed to import transformers.modeling_utils because of the following error (look up to see its traceback):\nDetected that PyTorch and torchvision were compiled with different CUDA major versions. PyTorch has CUDA Version=11.8 and torchvision has CUDA Version=12.4. Please reinstall the torchvision that matches your PyTorch install."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Detected: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"No GPU Detected.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYBGIG2sYGdi",
        "outputId": "e425f817-bfe7-490a-bd77-83e06968b33c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No GPU Detected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNav4qBK_ZIj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45a5d21f-5134-498b-9281-e8f1dc1e1e14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU-7Z2JUZM3W"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1prxGsDFa7El",
        "outputId": "31b93efa-d737-47cd-9404-fc16f7d2fab2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.5.0\n"
          ]
        }
      ],
      "source": [
        "import datasets\n",
        "print(datasets.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLUL2yfD3ncO"
      },
      "outputs": [],
      "source": [
        "%run verificacao_simples.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lM60nv9Y4bpe",
        "outputId": "47304a10-5c85-4449-f320-311e3ac3f29e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting verificacao_simples.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile verificacao_simples.py\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
        "from datasets import load_dataset\n",
        "\n",
        "# 1. Carregar Dataset\n",
        "dataset = load_dataset(\"liar\")  # Exemplo: Dataset LIAR (classificação de declarações)\n",
        "\n",
        "# 2. Pré-Processamento\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    # Use a coluna correta do dataset: 'statement'\n",
        "    return tokenizer(examples[\"statement\"], truncation=True, padding=\"max_length\")\n",
        "\n",
        "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Verificar o número de classes no dataset\n",
        "labels = set(dataset[\"train\"][\"label\"])\n",
        "num_classes = len(labels)\n",
        "print(f\"Número de classes no dataset: {num_classes}\")\n",
        "\n",
        "# 3. Carregar Modelo Pré-Treinado\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=num_classes)\n",
        "\n",
        "# Criar um Data Collator para lidar com padding dinâmico\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# 4. Definir Configurações de Treino\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",  # Diretório para salvar os resultados\n",
        "    logging_steps=500,  # Fazer logs a cada 500 steps\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",  # Diretório para salvar os logs\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=encoded_dataset[\"train\"],\n",
        "    eval_dataset=encoded_dataset[\"validation\"],\n",
        "    data_collator=data_collator,  # Substituindo o tokenizador\n",
        ")\n",
        "\n",
        "# 5. Treinar o Modelo\n",
        "trainer.train()\n",
        "\n",
        "# 6. Função de Verificação\n",
        "def verificar_noticia(noticia):\n",
        "    inputs = tokenizer(noticia, return_tensors=\"pt\", truncation=True, padding=\"max_length\")\n",
        "    outputs = model(**inputs)\n",
        "    predicted_class = outputs.logits.argmax().item()\n",
        "    return f\"Classificação: {predicted_class}\"\n",
        "\n",
        "# Teste com uma notícia\n",
        "noticia_teste = \"O presidente disse que a inflação caiu.\"\n",
        "resultado = verificar_noticia(noticia_teste)\n",
        "print(f\"Resultado: {resultado}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhkljy2e4rWQ"
      },
      "outputs": [],
      "source": [
        "with open(\"verificacao_simples.py\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
        "from datasets import load_dataset\n",
        "\n",
        "# 1. Carregar Dataset\n",
        "dataset = load_dataset(\"liar\")  # Exemplo: Dataset LIAR (classificação de declarações)\n",
        "\n",
        "# 2. Pré-Processamento\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    # Use a coluna correta do dataset: 'statement'\n",
        "    return tokenizer(examples[\"statement\"], truncation=True, padding=\"max_length\")\n",
        "\n",
        "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Verificar o número de classes no dataset\n",
        "labels = set(dataset[\"train\"][\"label\"])\n",
        "num_classes = len(labels)\n",
        "print(f\"Número de classes no dataset: {num_classes}\")\n",
        "\n",
        "# 3. Carregar Modelo Pré-Treinado\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=num_classes)\n",
        "\n",
        "# Criar um Data Collator para lidar com padding dinâmico\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# 4. Definir Configurações de Treino\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",  # Diretório para salvar os resultados\n",
        "    logging_steps=500,  # Fazer logs a cada 500 steps\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",  # Diretório para salvar os logs\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=encoded_dataset[\"train\"],\n",
        "    eval_dataset=encoded_dataset[\"validation\"],\n",
        "    data_collator=data_collator,  # Substituindo o tokenizador\n",
        ")\n",
        "\n",
        "# 5. Treinar o Modelo\n",
        "trainer.train()\n",
        "\n",
        "# 6. Função de Verificação\n",
        "def verificar_noticia(noticia):\n",
        "    inputs = tokenizer(noticia, return_tensors=\"pt\", truncation=True, padding=\"max_length\")\n",
        "    outputs = model(**inputs)\n",
        "    predicted_class = outputs.logits.argmax().item()\n",
        "    return f\"Classificação: {predicted_class}\"\n",
        "\n",
        "# Teste com uma notícia\n",
        "noticia_teste = \"O presidente disse que a inflação caiu.\"\n",
        "resultado = verificar_noticia(noticia_teste)\n",
        "print(f\"Resultado: {resultado}\")\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "d3ZxrJ2u5fJq",
        "outputId": "dd7cbded-d533-4e50-f65d-961d0166a3fb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "File `'/C:/Users/Geraldo/Documents/ChatBot_NewsgamesIA/verificacao_simples.py'` not found.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0mfpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_lst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_finder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    715\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/utils/path.py\u001b[0m in \u001b[0;36mget_py_filename\u001b[0;34m(name, force_win32)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'File `%r` not found.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: File `'/C:/Users/Geraldo/Documents/ChatBot_NewsgamesIA/verificacao_simples.py'` not found.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-3b2bd4bf86d6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'run'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/C:/Users/Geraldo/Documents/ChatBot_NewsgamesIA/verificacao_simples.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2416\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2417\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2418\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2419\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-52>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, parameter_s, runner, file_finder)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nt'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"^'.*'$\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m                 \u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'For Windows, use double quotes to wrap a filename: %run \"mypath\\\\myfile.py\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: File `'/C:/Users/Geraldo/Documents/ChatBot_NewsgamesIA/verificacao_simples.py'` not found."
          ]
        }
      ],
      "source": [
        "%run /C:/Users/Geraldo/Documents/ChatBot_NewsgamesIA/verificacao_simples.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmqGFizE6lJt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpbaLlaga2ZH"
      },
      "outputs": [],
      "source": [
        "pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OUWPI5iZOmg"
      },
      "outputs": [],
      "source": [
        "%%writefile verificacao_simples.py\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
        "from datasets import load_dataset\n",
        "\n",
        "# 1. Carregar Dataset\n",
        "dataset = load_dataset(\"liar\")  # Exemplo: Dataset LIAR (classificação de declarações)\n",
        "\n",
        "# 2. Pré-Processamento\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    # Use a coluna correta do dataset: 'statement'\n",
        "    return tokenizer(examples[\"statement\"], truncation=True, padding=\"max_length\")\n",
        "\n",
        "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Verificar o número de classes no dataset\n",
        "labels = set(dataset[\"train\"][\"label\"])\n",
        "num_classes = len(labels)\n",
        "print(f\"Número de classes no dataset: {num_classes}\")\n",
        "\n",
        "# 3. Carregar Modelo Pré-Treinado\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=num_classes)\n",
        "\n",
        "# Criar um Data Collator para lidar com padding dinâmico\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# 4. Definir Configurações de Treino\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",  # Diretório para salvar os resultados\n",
        "    logging_steps=500,  # Fazer logs a cada 500 steps\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",  # Diretório para salvar os logs\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=encoded_dataset[\"train\"],\n",
        "    eval_dataset=encoded_dataset[\"validation\"],\n",
        "    data_collator=data_collator,  # Substituindo o tokenizador\n",
        ")\n",
        "\n",
        "# 5. Treinar o Modelo\n",
        "trainer.train()\n",
        "\n",
        "# 6. Função de Verificação\n",
        "def verificar_noticia(noticia):\n",
        "    inputs = tokenizer(noticia, return_tensors=\"pt\", truncation=True, padding=\"max_length\")\n",
        "    outputs = model(**inputs)\n",
        "    predicted_class = outputs.logits.argmax().item()\n",
        "    return f\"Classificação: {predicted_class}\"\n",
        "\n",
        "# Teste com uma notícia\n",
        "noticia_teste = \"O presidente disse que a inflação caiu.\"\n",
        "resultado = verificar_noticia(noticia_teste)\n",
        "print(f\"Resultado: {resultado}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S57hnN0dpYGV"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade datasets torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9itd1vcqYWs"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade datasets torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UM_5Vr7YqiWR"
      },
      "outputs": [],
      "source": [
        "%run verificacao_simples.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcSi_DomrAjX"
      },
      "outputs": [],
      "source": [
        "%%writefile verificacao_simples.py\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "\n",
        "# 1. Carregar Dataset\n",
        "dataset = load_dataset(\"liar\")  # Exemplo: Dataset LIAR (classificação de declarações)\n",
        "\n",
        "# 2. Pré-Processamento\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    # Use a coluna correta do dataset: 'statement'\n",
        "    return tokenizer(examples[\"statement\"], truncation=True, padding=\"max_length\")\n",
        "\n",
        "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# 3. Carregar Modelo Pré-Treinado\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
        "\n",
        "# 4. Definir Configurações de Treino\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",  # Diretório para salvar os resultados\n",
        "    logging_steps=500,  # Fazer logs a cada 500 steps\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",  # Diretório para salvar os logs\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=encoded_dataset[\"train\"],\n",
        "    eval_dataset=encoded_dataset[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# 5. Treinar o Modelo\n",
        "trainer.train()\n",
        "\n",
        "# 6. Função de Verificação\n",
        "def verificar_noticia(noticia):\n",
        "    inputs = tokenizer(noticia, return_tensors=\"pt\", truncation=True, padding=\"max_length\")\n",
        "    outputs = model(**inputs)\n",
        "    predicted_class = outputs.logits.argmax().item()\n",
        "    return \"Verdadeira\" if predicted_class == 1 else \"Falsa\"\n",
        "\n",
        "# Teste com uma notícia\n",
        "noticia_teste = \"O presidente disse que a inflação caiu.\"\n",
        "resultado = verificar_noticia(noticia_teste)\n",
        "print(f\"Resultado: {resultado}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjpf5tnPrIAw"
      },
      "outputs": [],
      "source": [
        "%run verificacao_simples.py"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "11Z1qfhN4IALXOiUFEXK5DGZBgHI4lJcJ",
      "authorship_tag": "ABX9TyPrrSpTOHvuwtCvMHqhSE61",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}